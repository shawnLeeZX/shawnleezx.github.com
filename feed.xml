<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shawn</title>
    <description>&lt;h1&gt;Children of The Sun&lt;/h1&gt; </description>
    <link>http://shawnLeeZX.github.io/</link>
    <atom:link href="http://shawnLeeZX.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 08 Mar 2019 16:40:42 +0800</pubDate>
    <lastBuildDate>Fri, 08 Mar 2019 16:40:42 +0800</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>Intuition That Motivates Inverse Transform Sampling</title>
        <description>&lt;p&gt;This post notes down the observation that motives inverse transform sampling
(ITS) to sample from any probability distributions.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;Given a random variable (r.v.) $X$, and a pseudo-random number generator that
can emit uniform random numbers, how to derive a method to sample from any
distribution? The answer has been rather mystic for me at the first time that I
learned about the inverse transform sampling in a distance time ago. I
revisited the issue today, and decide to outline the intuition behind it.&lt;/p&gt;

&lt;p&gt;It is not easy to characterize randomness in a computer that is built on
deterministic Turing model. So the randomness comes from pseudonumber
generator. I did not know the machinery to generate pseudo-random numbers, but
I guess obtaining uniform number are hand-crafted procedure that are manually
designed. So it is probably hard to devise one sampler for uniform
distribution. The difficulty may be similar imagined in the physical world,
where sampling are performed physically relying on the principle of symmetry,
e.g., throwing a coin; we cannot do sampling if we do not have the physical
process that has such type of randomness.&lt;/p&gt;

&lt;p&gt;Thus, how can be convert the manpower involved in the pseudo-random generator
to generate other probability distributions?&lt;/p&gt;

&lt;p&gt;To motivate ITS, we need to go back to the measure-theoretical definition of
probability, which I will skip. With necessary background in advanced
probability theory, we can have the following derivation.&lt;/p&gt;

&lt;p&gt;Suppose we have a transform $T$, such that $\nu(T(X)) = \mu (T^{-1}(X))$. Given
a set $E \in \mathcal{E}$, where $\mathcal{E}$ is the measurable space of $X$,
we have the measure of a set $F \in \mathcal{F}$ as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nu(T(E)) = \mu (T^{-1}(F))&lt;/script&gt;

&lt;p&gt;This is the intuition underlying the seeming magic underlying ITS, which is
normally not discussed, as in the Wikipedia
&lt;a href=&quot;https://en.wikipedia.org/wiki/Inverse_transform_sampling&quot;&gt;entry&lt;/a&gt;. That is, we
are manipulating the probability measure to a form that we can compute.
Then, if we can obtain a sample $y$ according to $\nu$, $T^{-1}(y)$ is the
sample we would get if we obtain a sample according to $\mu$.&lt;/p&gt;

&lt;p&gt;However, if the above statement were true, $T$ should be monotonous, so measure
of different values does not overlap. Otherwise, the probability to sample $y$
is the probability of all elements of $X$ that are mapped to $y$,
i.e., $\forall x, T(x) = y$. In the one-dimensional case, monotony can
concentrate the measure of an interval, e.g., $[a, b] \subset \mathbb{R}$, to
an interval $[c, d] \subset \mathbb{R}$; that is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nu ([c, d]) = \int_{a}^{b} \mu(x) dx, c = T(a), d = T(b)&lt;/script&gt;

&lt;p&gt;Lastly, we need to manipulate $F$ so that $T(F)$ is computable, where comes the
Cumulative Distribution Function (CDF). Let $\mu$ be the CDF of $\mu$. Observe
that the meaning of $\nu[0, c]$ are literally, &lt;em&gt;the probability $T(x)$ lies in
$[0, c]$ is $c$&lt;/em&gt;; that is the definition of uniform distribution.&lt;/p&gt;

&lt;p&gt;That is the derivation of Inverse Transform Sampling.&lt;/p&gt;
</description>
        <pubDate>Thu, 21 Feb 2019 20:37:39 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2019/02/21/intuition-that-motivates-inverse-transform-sampling/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2019/02/21/intuition-that-motivates-inverse-transform-sampling/</guid>
        
        
        <category>Math</category>
        
        <category>Statistics</category>
        
      </item>
    
      <item>
        <title>Scientific Humanism</title>
        <description>&lt;p&gt;While reading the Wikipedia page of &lt;a href=&quot;https://en.wikipedia.org/wiki/E._O._Wilson#Scientific_humanism&quot;&gt;Edward O. Wilson&lt;/a&gt;,
 I re-discovered the &lt;a href=&quot;http://americanhumanist.org/wp-content/uploads/2018/08/HumanismandItsAspirations_jefferson1.pdf&quot;&gt;Humanism Manifesto III&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Previously, I have known the manifesto for several years. Back then at the
 first time I read the manifesto, it sounded exactly the philosophy that I am looking
 for. But, the action of &lt;a href=&quot;https://humanists.international/&quot;&gt;Humanist International&lt;/a&gt; (HI),
 the umbrella organization to advocate scientific humanism does not feel right
 for me. The impression I was left was that HI reactively fights against
 religion, against non-freedom. One does not beat darkness to bring light; one
 light a fire. The fight over freedom, over religion has long been won since
 the capitalism revolution against the theological authoritarianism. The victory
 was only partly political, which feels like is the focus of HI. The other
 indispensable part is social-economical. The old feudal agricultural
 social-economical structure was replaced by a commercial cooperative
 social-economical structure; and that is the foundation of the political
 change. However, the problem of our age is exactly the pathological
 development of barbarism capitalism, which was what socialism revolutionized
 against in the past, the root of the social tumult in the past
 century, and the rising of populism recently. That is to say, I feel that HI
 is a legacy fighting a ghost under the banner of scientific humanism. That’s
 why I gradually lost my attention on it previously.&lt;/p&gt;

&lt;p&gt;But then, I did not know Edward O. Wilson, and his &lt;em&gt;Consilient of
 Knowledge&lt;/em&gt;. I did not know that the first manifesto was drafted under the
 leadership of John Dewey. That is to say, I wrongly mistook HI for Humanist
 Manifesto.&lt;/p&gt;

&lt;p&gt;Now I started to think that the manifesto &lt;strong&gt;is&lt;/strong&gt; the philosophy of our time in
 its infancy. I am investigating it from time to time. The investigation mostly
 focuses on a new social-economical structure could be built on such a
 philosophy, and what kind of a new institution that would be devised from the
 new social-economical structure; and also the alignment of force that under
 such a philosophy — if there exists a community like the Invisible College
 in the Age of Reason, the Vienna Circle in the pre-WWII age, or Homebrew
 Computer Club in the aftermath of Counter Culture.&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Feb 2019 11:24:48 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2019/02/20/scientific-humanism/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2019/02/20/scientific-humanism/</guid>
        
        
        <category>Philosophy</category>
        
        <category>Update</category>
        
      </item>
    
      <item>
        <title>机械柏拉图学院</title>
        <description>&lt;p&gt;9月17日下午，我们来聊一聊关于世界、关于生活的根本问题。&lt;/p&gt;

&lt;!-- &lt;img class=&quot;center&quot; src=&quot;/images/mechanic_plato_academy.jpg&quot; width=&quot;100%&quot; height=&quot;100%&quot;  &gt; --&gt;

&lt;h2 id=&quot;section&quot;&gt;活动基调&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;古者富贵而名摩灭，不可胜记，唯俶傥非常之人称焉。盖西伯拘而演《周易》；仲尼厄而
作《春秋》；屈原放逐，乃赋《离骚》；左丘失明，厥有《国语》；孙子膑脚，《兵法》
修列；不韦迁蜀，世传《吕览》；韩非囚秦，《说难》、《孤愤》。《诗》三百篇，大氐
贤圣发愤之所为作也。此人皆意有所郁结，不得通其道，故述往事，思来者。 — 《报
任安书》 司马迁&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- more --&gt;

&lt;p&gt;自文明以降，先贤著书立说，多因天下失道，以“述往事，思来者”。中华方迈出两千年东
方之“黑暗时代”，然其文艺复兴又被百年动乱所打断。中国知识分子对自身文化的梳理远
未完成。希腊系文明经城邦时代开科学民主之先河、历黑暗时代神权威权统治之中世纪、
自佛罗伦萨文艺复兴、以经典自由主义造建崭新社会形态、经社会自由主义改良，在二十
世纪末，法兰西斯·福山以《The End of History &amp;amp; Last Man》一书乐观但“天真”地表达
人类体制的探索已经结束，人类社会迎来“世界的终结”。然零八年金融危机后，世界表面
的安宁被打破，英国脱欧，民粹主义盛行，川普总统，世界公民普适之日遥遥无期；中华
民间力量方才萌芽便已蒙刹；以自由之名诞生的工具精神（tools of personal
liberation）、黑客精神亦由屠龙少年被驯化为泯然众人；被称为人类最后的发明的人工
智能很可能将带来第四次工业革命，颠覆现有的社会下层基础。&lt;/p&gt;

&lt;p&gt;变革之时代多需哲学之变革。孔孟之道诞生于礼崩乐坏，周法不行之时；自由主义诞生于
文艺复兴，神权不再之时；存在主义诞生于世界大战，仿临近纪末日之时。无论是可生活
（inhabitable)之哲学，或可统治之哲学（政治哲学），现今社会至今我并未发现已成形
之答案。故吾辈所可做之事，乃继往圣之学，并“相信智能是有能力去想像未来的，基于值
得保留的那部分现在，从而作出对未来的一种投射，并为实现这样的一种未来而发明所需
要的工具，这是我们的救赎（John Dewey）。”&lt;/p&gt;

&lt;p&gt;以上是此活动之基调，望招纳志同道合之人，究往事，思来者。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;活动媒介&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;华夏的古老经常使我们忘记，中国其实多么年轻。&lt;/p&gt;

  &lt;p&gt;用梁任公的术语说，“少年中国”仍然是“新民”。&lt;/p&gt;

  &lt;p&gt;她步入这个拥挤的世界，已经不能再指望海阔天空的处女地。&lt;/p&gt;

  &lt;p&gt;青春往往是痛苦而危险的，因为世界的表象和世界的律法形成了一对险恶的漩涡。&lt;/p&gt;

  &lt;p&gt;在她最需要智慧的时刻，通常没有她所需要的语言。&lt;/p&gt;

  &lt;p&gt;世界没有任何现成的语言能够适应她的需要。&lt;/p&gt;

  &lt;p&gt;她只能一面发现世界，一面发明自己的语言。&lt;/p&gt;

  &lt;p&gt;当她找到适当的语言，就会发现自己适当的位置。&lt;/p&gt;

  &lt;p&gt;这时，诸神的天空不再变幻莫测。&lt;/p&gt;

  &lt;p&gt;在此之前，世界仍然不会给她家园的感觉。&lt;/p&gt;

  &lt;p&gt;— 《从华夏到中国》 刘仲敬&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;虽谦虚被中华民族视为美德，整个民族时至今日仍充满了当年“天朝上国”的傲慢，长文论
述可参考殷海光《中国文化的展望》，而殊不知其所不知。此多非国人之过错，由于教育、
宣传、信息传播之原因，大量国人仍处在“青春期”。所以交流介质多数时间希望以&lt;em&gt;英文&lt;/em&gt;进行，以期招纳业已简单理解西方文明之人士，互通有无。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;活动内容&lt;/h2&gt;

&lt;p&gt;本次活动为投石问路，拟以《Dubliner》中较为简单的短篇故事为载体，寻世界新民。
《Dubliner》为James Joyce的短篇小说集，描述了爱尔兰在欧洲资产阶级革命之后巨大的
社会变革下都柏林中产阶级在寻找自己的位置、生活和认同感的故事。&lt;/p&gt;

&lt;p&gt;因为阅读的是短篇小说，所以不会对参与过多的时间负担，我们将花两三个小时来阅读、
讨论Dubliner中的短篇小说，发现书中人物与我们所处时空的相似。&lt;em&gt;交流主要以英语进行。&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;活动报名&lt;/h2&gt;

&lt;p&gt;一次活动以十人为上限。报名采取先到先得，但后到者可通过陈述：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;参与动机；&lt;/li&gt;
  &lt;li&gt;学思历程（可选）；&lt;/li&gt;
  &lt;li&gt;阅读史（可选）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;来获取资格。&lt;/p&gt;

&lt;p&gt;注意：获取资格的优先级以陈述的深度最高，报名时间次之。因此，先报名者同样会由于
后来者的深度陈述而遗憾落选。所以推荐所有报名者书写陈述^_^。&lt;/p&gt;

&lt;p&gt;确认参与将会发予精美电子版《Dubliner》。报名结果将会通过邮件送达，所以一定要在
报名处填常用邮箱哟。&lt;/p&gt;

&lt;p&gt;报名截止时间为&lt;strong&gt;9月14日，周四晚十二点&lt;/strong&gt;，报名确认将在第二天周五晚十点前发出。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;活动注意事项&lt;/h2&gt;

&lt;p&gt;报名确认不能参加请尽快通知组织方，从而能给让出名额给其他人；无故缺席将会被记入
黑名单。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;报名链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.huodongxing.com/event/2403775592100?td=3182774891054&quot;&gt;点这里&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Sep 2017 20:57:08 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2017/09/06/ji-xie-plato-academy/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2017/09/06/ji-xie-plato-academy/</guid>
        
        
        <category>Life</category>
        
      </item>
    
      <item>
        <title>Notes on I-theory</title>
        <description>&lt;!-- more --&gt;

&lt;ul&gt;
  &lt;li&gt;It is a Hebbian style unsupervised learning algorithm that computes invariant
yet selective signature of a particular image.&lt;/li&gt;
  &lt;li&gt;Originally, the semantics meaning of the signature is the cumulative
distribution function (CDF) of a proxy distribution of the distribution of
the orbit ${ gI : g \in G }$ of $I$, where $I$ is the canonical image
template, and $G$ is transformation that may apply on $I$.&lt;/li&gt;
  &lt;li&gt;The proxy probability distribution is a one dimension probability
distribution induced by projecting each $gI$ to $R$ using a (possibly random)
template $t$ (taking inner product between $I$ and $t$).&lt;/li&gt;
  &lt;li&gt;The CDF is computed by using a Heaviside step function $H$.&lt;/li&gt;
  &lt;li&gt;During computation of the CDF, uniform (haar) measure is used for $G$:
different $gI$ may be projected to the same value in $R$, thus the CDF at $x
\in R$ is a integral (summation) weighted by measure on group.&lt;/li&gt;
  &lt;li&gt;In practice, histogram is used and each neuron may be responsible to
represent a particular bin of the histogram (see Unsupervised learning of
invariant representations).&lt;/li&gt;
  &lt;li&gt;Given the presence of the integral, the signature can also be computed using
moments, where the $H$ can be replaced with $f(x) = x^r$, since moments can
characterize distributions in certain extent.&lt;/li&gt;
  &lt;li&gt;Since the selectivity theorem requires the infinite number of templates,
i-theory defines Kolmogorov-Smirnov (KS) metric on the proxy distributions
(other metrics are possible), and uses it to prove a error bound when only a
finite number of templates are available.&lt;/li&gt;
  &lt;li&gt;Without using all the orbits (partially observable group), covariant
(equivariant) can be characterized in i-theory.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Critiques.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Essentially this is a memory based system, where $gI$ is assigned uniform
measure, and the representation learned is generic (not tying with any target
function). All transforms have to be physical transform, since it needs to be
trained with training data in the form ``movies’’.&lt;/li&gt;
  &lt;li&gt;Focus is on the properties of the signature, instead of the the space the
signature lives in, which could have rich structure. Topological structure,
metric structure of the group is not used. Probability inference does not
play a role here. Structure of the probability distribution is not used.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 20 May 2017 17:09:01 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2017/05/20/note-on-i-theory/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2017/05/20/note-on-i-theory/</guid>
        
        
        <category>NN</category>
        
        <category>Math</category>
        
        <category>Research</category>
        
      </item>
    
      <item>
        <title>A Little More Details About RNN Gradient Calculation</title>
        <description>&lt;p&gt;This post is about how to use chain rule to calculate back propagation through
time.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;When I was deriving the gradient of hidden-to-hidden matrix in Recurrent Neural
Network, I found product rule and chain rule are essentially the same thing,
and note the interesting discovery down here (it just shows I do not know enough
about vector calculus … ).&lt;/p&gt;

&lt;p&gt;Hidden layer of a vanilla RNN can be expressed as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_t = \sigma (W_{xh}x + W_{hh}h_{t-1} + b_h)&lt;/script&gt;

&lt;h3 id=&quot;gradient&quot;&gt;Gradient&lt;/h3&gt;

&lt;p&gt;If we want to calculate $\nabla_{W_{hh}}h_{t}$, we need to do back propagation
through time. To compute $\frac{\partial h_t}{\partial W_{hh}}$, notice that
$W_{hh}$ and $h_{t-1}$ both contain $W_{hh}$, so it involves chain rule.&lt;/p&gt;

&lt;p&gt;Use chain rule, we will have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{W_{hh}}h_{t} = \sum\limits_{1\leq k \leq t}(\frac{\partial
h_{t}}{\partial h_{k}}\frac{\partial^{+}h_{k}}{\partial W_{hh}})&lt;/script&gt;

&lt;p&gt;where $\partial^+$ means immediate partial derivative of $h_k$ respect to $W_{hh}$.&lt;/p&gt;

&lt;p&gt;When I unroll stuff, and it seems we are using product rule $W_{hh}h_{t-1}$
instead of chain rule. So it turns out product rule is a special form of chain
rule. Let’s see how.&lt;/p&gt;

&lt;h3 id=&quot;chain-rule&quot;&gt;Chain rule&lt;/h3&gt;

&lt;p&gt;First define chain rule. If $R^{n} \xrightarrow{f} R^{m}$ is
differentiable at $x_0$ and $R^{m} \xrightarrow{g} R^{p}$ is differentiable at
$f(x_0)$, then $g \circ f$ is differentiable at $x_0$, and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(g \circ f)'(x_0) = g'(f(x_0))f'(x_0)&lt;/script&gt;

&lt;h3 id=&quot;an-example-of-chain-rule&quot;&gt;An example of chain rule&lt;/h3&gt;

&lt;p&gt;Suppose &lt;script type=&quot;math/tex&quot;&gt;w = g(u, v),  = f(t)=
 \begin{bmatrix} f_1(t)\\ f_2(t) \end{bmatrix}&lt;/script&gt;,
 then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{dw}{dw} = \frac{d g \circ f}{dt} = \begin{bmatrix} \frac{\partial
w}{\partial u} &amp; \frac{\partial w}{\partial v} \end{bmatrix} \begin{bmatrix} \frac{\partial f_1(t)}{\partial t}\\ \frac{\partial f_2{t}}{\partial t} \end{bmatrix}
= \frac{\partial w}{\partial u}\frac{\partial f_1(t)}{\partial t} + \frac{\partial w}{\partial v}\frac{\partial f_2(t)}{\partial t} %]]&gt;&lt;/script&gt;

&lt;p&gt;If $g(u, v) = uv$, then we get product rule. If $g(u, v) = u/v$, we get
division rule.&lt;/p&gt;

&lt;h3 id=&quot;application-of-chain-rule&quot;&gt;Application of chain rule&lt;/h3&gt;

&lt;p&gt;With chain rule, we have $h_t = h_t(I(W_{hh}), h_{t-1}(W_{hh}))$, which is
exactly the above example. Unroll $\frac{\partial h_t}{\partial W_{hh}}$ with
the rule, we get the gradient at the beginning.&lt;/p&gt;

&lt;h3 id=&quot;more-info&quot;&gt;More info&lt;/h3&gt;

&lt;p&gt;Refer to &lt;a href=&quot;http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/&quot;&gt;here&lt;/a&gt;
and &lt;em&gt;Razvan et al. On the difficulty of training recurrent neural networks&lt;/em&gt;.&lt;/p&gt;
</description>
        <pubDate>Thu, 16 Mar 2017 17:35:48 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2017/03/16/rnn-gradient-calculation-with-a-little-more-details/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2017/03/16/rnn-gradient-calculation-with-a-little-more-details/</guid>
        
        
        <category>RNN</category>
        
        <category>Math</category>
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>Determinant, Differential Form, and Wedge Product</title>
        <description>&lt;p&gt;This is a note to push a thorough understanding of determinant in stack for
future reference.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;determinant&quot;&gt;Determinant&lt;/h2&gt;

&lt;p&gt;I have seen many ways to derive determinant. Notably, two of them make the most
sense.&lt;/p&gt;

&lt;p&gt;In &lt;em&gt;Linear Algebra Done Right&lt;/em&gt;, determinant is defined as the product of
eigenvalues, which is based on the geometrical intuition of determinant — how
the volume will change if multiplied by this matrix. It arrives at its
recursive computational definition by reasoning from matrices that are almost
diagonal. The intuition underlying is each matrix can be viewed as a summation
of permutations of a series of diagonal matrices.&lt;/p&gt;

&lt;p&gt;In &lt;em&gt;Vector Calculus, Linear Algebra, and Differential Forms — A Unified
Approach&lt;/em&gt;, the author starts from the 2D and 3D cases, and induces the general
form of determinant by generalizing on certain important properties. It also
starts from the geometric perspective, though not from eigenvalues.&lt;/p&gt;

&lt;p&gt;Essentially, the derivation of determinant above can be seen as a generalized of
computational formula of 3D volume in a vector space by observing its key
properties. It can be seen as a primary mathematical object that represents
the volume of objects in the space.&lt;/p&gt;

&lt;h2 id=&quot;differential-form&quot;&gt;Differential Form&lt;/h2&gt;

&lt;p&gt;Differential form is built on on determinant. In the derivation of determinant
previously (in the previous books), we do not stress what the vector is. In
differential form, the vector is operator that operates on vectors (called
co-vector in differential geometry). In this case, k-form is defined as&lt;/p&gt;

&lt;p&gt;A k-form is $dx_{1}, dx_{2} \ldots dx_{k}$ is the linear function over
k-tuples of vectors $(v_1, v_2 \ldots, v_k)$ equal to the determinant of the
matrix whose general term is $dx_i(v_j)$; i.e., the element in the $i$th row,
$j$th column is the $i$th-component of the $j$th vector, $v_{ji}$.&lt;/p&gt;

&lt;p&gt;where $dx_{i}, i = 1 \ldots k$ is the co-vector that operates on vector $v_j$
, which extracts (whose output is) the $i$th coefficients of $v_j$.&lt;/p&gt;

&lt;h2 id=&quot;wedge-product&quot;&gt;Wedge Product&lt;/h2&gt;

&lt;p&gt;Wedge product is the formalized binary operation that preserves the properties
of determinant (making sure the results still compute volume) when multiplying
two differential form together.&lt;/p&gt;

&lt;p&gt;In this sense, the derivation of wedge product is also the derivation that why
we need to multiply the determinant of Jacobi matrix when doing variables
substitution for multi-variable integral.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.sjsu.edu/faculty/watkins/difforms0.htm&quot;&gt;The Nature of Differential Forms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;books-to-check-out&quot;&gt;Books to check out&lt;/h2&gt;

&lt;h3 id=&quot;from-reference&quot;&gt;From reference&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;M. Schreiber, Differential Forms: A Heuristic Introduction, Springer-Verlag, 1977.&lt;/li&gt;
  &lt;li&gt;Richard E. Williamson, Richard H. Crowell and Hale F. Trotter, Calculus of Vector Functions, Prentice-Hall, 1968.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;from-stored-books&quot;&gt;From stored books&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Differential forms_ a complement to vector calculus-Academic Press (1997)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;from-amazon&quot;&gt;From Amazon&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Calculus-Vector-Functions-Richard-Williamson/dp/013112367X/ref=sr_1_1?ie=UTF8&amp;amp;qid=1489484473&amp;amp;sr=8-1&amp;amp;keywords=Calculus+of+Vector+Functions&quot;&gt;Calculus of Vector Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Vector-Calculus-Jerrold-E-Marsden/dp/1429215089/ref=pd_sbs_14_6?_encoding=UTF8&amp;amp;pd_rd_i=1429215089&amp;amp;pd_rd_r=YRGM5SAK2HV3REFAHS15&amp;amp;pd_rd_w=8kuQu&amp;amp;pd_rd_wg=P0BDX&amp;amp;psc=1&amp;amp;refRID=YRGM5SAK2HV3REFAHS15&quot;&gt;Vector Calculus&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Vector-Calculus-Miroslav-Lovric/dp/0471725692/ref=sr_1_6?ie=UTF8&amp;amp;qid=1489484473&amp;amp;sr=8-6&amp;amp;keywords=Calculus+of+Vector+Functions&quot;&gt;Vector Calculus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 14 Mar 2017 17:58:28 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2017/03/14/determinant/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2017/03/14/determinant/</guid>
        
        
        <category>Math</category>
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>龙的牙医——这就是活着啊</title>
        <description>&lt;!-- more --&gt;

&lt;p&gt;&lt;img class=&quot;center&quot; src=&quot;/images/dragons_dentists/1.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;
&lt;img class=&quot;center&quot; src=&quot;/images/dragons_dentists/2.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;
&lt;img class=&quot;center&quot; src=&quot;/images/dragons_dentists/3.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;
&lt;img class=&quot;center&quot; src=&quot;/images/dragons_dentists/4.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Mar 2017 15:19:41 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2017/03/14/zhe-jiu-shi-huo-zhe-a/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2017/03/14/zhe-jiu-shi-huo-zhe-a/</guid>
        
        
        <category>Life</category>
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>Calculating Receptive Field of CNN</title>
        <description>&lt;!-- more --&gt;

&lt;p&gt;The receptive field (RF) $l_k$ of layer $k$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l_k = l_{k-1} + ((f_k - 1) * \prod_{i=1}^{k-1}s_i)&lt;/script&gt;

&lt;p&gt;where $l_{k-1}$ is the receptive field of layer $k-1$, $f_k$ is the filter size
(height or width, but assuming they are the same here), and $s_i$ is the stride
of layer $i$.&lt;/p&gt;

&lt;p&gt;The formula above calculates receptive field from bottom up (from layer
1). Intuitively, RF in layer $k$ covers $(f_k - 1) * s_{k-1}$ more pixels
relative with layer $k-1$. However, the increment needs to be translated to the
first layer, so the increments is a factorial — a stride in layer $k-1$ is
exponentially more strides in the lower layers.&lt;/p&gt;
</description>
        <pubDate>Sat, 11 Feb 2017 22:20:05 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2017/02/11/calculating-receptive-field-of-cnn/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2017/02/11/calculating-receptive-field-of-cnn/</guid>
        
        
        <category>CNN</category>
        
      </item>
    
      <item>
        <title>A Command Line Music Player in One Line of Code</title>
        <description>&lt;p&gt;I manually maintain the songs and music I listen by folder, and most of the
time, the only function of a music player useful is to import music in a folder
and listen to them. However, for unknown reason, rhythmbox (default music player
of Ubuntu) consumes a large number of memory and CPU times. So I consider
finding a way to do play music through command line, which lets me get rid
clicking, and the other trouble.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mpg123&lt;/code&gt; is an open-source mp3 file players, and it supports playing by file
list. To play all the mp3 files under current folder&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;find . -name &lt;span class=&quot;s2&quot;&gt;&quot;*.mp3&quot;&lt;/span&gt; | mpg123 -@ -
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;which pipes the files found by &lt;code class=&quot;highlighter-rouge&quot;&gt;find&lt;/code&gt; to mpg123, where &lt;code class=&quot;highlighter-rouge&quot;&gt;-@&lt;/code&gt; is short for
&lt;code class=&quot;highlighter-rouge&quot;&gt;--list&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;-&lt;/code&gt; is short for stand input.&lt;/p&gt;

&lt;p&gt;To pause the playing, type &lt;code class=&quot;highlighter-rouge&quot;&gt;C-Z&lt;/code&gt; which stops the program. Type &lt;code class=&quot;highlighter-rouge&quot;&gt;fg&lt;/code&gt; to bring it
to the foreground and play again.&lt;/p&gt;

&lt;p&gt;To create a shuffled play list, just pipe through &lt;code class=&quot;highlighter-rouge&quot;&gt;shuf&lt;/code&gt; before &lt;code class=&quot;highlighter-rouge&quot;&gt;mpg123&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;find . -name &lt;span class=&quot;s2&quot;&gt;&quot;*.mp3&quot;&lt;/span&gt; | shuf | mpg123 -@ -
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Enjoy :).&lt;/p&gt;
</description>
        <pubDate>Thu, 26 Jan 2017 16:22:37 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2017/01/26/a-command-line-music-player-in-one-line-of-code/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2017/01/26/a-command-line-music-player-in-one-line-of-code/</guid>
        
        
        <category>Linux</category>
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>相由心生</title>
        <description>&lt;p&gt;身是菩提樹，&lt;/p&gt;

&lt;p&gt;心如明鏡台，&lt;/p&gt;

&lt;p&gt;時時勤拂拭，&lt;/p&gt;

&lt;p&gt;勿使惹塵埃。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;菩提本無樹，&lt;/p&gt;

&lt;p&gt;明鏡亦非台，&lt;/p&gt;

&lt;p&gt;本來無一物，&lt;/p&gt;

&lt;p&gt;何處惹塵埃？&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;section&quot;&gt;故事&lt;/h2&gt;

&lt;p&gt;在南北朝的時候，佛教禪宗傳到了第五祖弘忍大師，弘忍大師當時在湖北的黃梅開壇講學，手下有弟子五百餘人，其中翹楚者當屬大弟子神秀大師。&lt;/p&gt;

&lt;p&gt;神秀也是大家公認的禪宗衣缽的繼承人。&lt;/p&gt;

&lt;p&gt;弘忍漸漸的老去，於是他要在弟子中尋找一個繼承人，所以他就對徒弟們說，大家都做一首畿子（有禪意的詩），看誰做得好就傳衣缽給誰。&lt;/p&gt;

&lt;p&gt;這時神秀很想繼承衣缽，但又怕因為出於繼承衣缽的目的而去做這個畿子，違法了佛家的無為而作意境。
所以他就在半夜起來，在院牆上寫了一首畿子身是菩提樹，心為明鏡台。時時勤拂拭，勿使惹塵埃。
這首畿子的意思是，要時時刻刻的去照顧自己的心靈和心境，通過不斷的修行來抗拒外面的誘惑，和種種邪魔。&lt;/p&gt;

&lt;p&gt;是一種入世的心態，強調修行的作用。
而這種理解與禪宗大乘教派的頓悟是不太吻合的，所以當第二天早上大家看到這個畿子的時候，都說好，而且都猜到是神秀作的而很佩服的時候，弘忍看到了以後沒有做任何的評價。
因為他知道神秀還沒有頓悟。&lt;/p&gt;

&lt;p&gt;而這時，當廟裡的和尚們都在談論這首畿子的時候，被廚房裡的一個火頭僧—慧能禪師聽到了。
慧能當時就叫別人帶他去看這個畿子，這裡需要說明的一點是，慧能是個文盲，他不識字。
他聽別人說了這個畿子，當時就說這個人還沒有領悟到真諦啊。
於是他自己又做了一個畿子，央求別人寫在了神秀的畿子的旁邊，菩提本無樹，明鏡亦非台，本來無一物，何處惹塵埃。
有這首畿子可以看出慧能是個有大智慧的人（後世有人說他是十世比丘轉世），他這個畿子很契合禪宗的頓悟的理念。
是一種出世的態度，主要意思是，世上本來就是空的，看世間萬物無不是一個空字，心本來就是空的話，就無所謂抗拒外面的誘惑，任何事物從心而過，不留痕跡。
這是禪宗的一種很高的境界，領略到這層境界的人，就是所謂的開悟了。&lt;/p&gt;

&lt;p&gt;弘忍看到這個畿子以後，問身邊的人是誰寫的，邊上的人說是慧能寫的，於是他叫來了慧能，當著他和其他僧人的面說：寫得亂七八糟，胡言亂語，並親自擦掉了這個畿子。
然後在慧能的頭上打了三下就走了。這時只有慧能理解了五祖的意思，於是他在晚上三更的時候去了弘忍的禪房，在那裡弘忍向他講解了《金剛經》這部佛教最重要的經典之一，並傳了衣缽給他。
然後為了防止神秀的人傷害慧能，讓慧能連夜逃走。於是慧能連夜遠走南方，隱居10年之後在莆田少林寺創立了禪宗的南宗。
而神秀在第二天知道了這件事以後，曾派人去追慧能，但沒有追到。&lt;/p&gt;

&lt;p&gt;後來神秀成為梁朝的護國法師，創立了禪宗的北宗。&lt;/p&gt;
</description>
        <pubDate>Sun, 01 Jan 2017 11:48:59 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2017/01/01/puti/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2017/01/01/puti/</guid>
        
        
        <category>Life</category>
        
      </item>
    
  </channel>
</rss>
