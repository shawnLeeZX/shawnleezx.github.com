<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shawn</title>
    <description>&lt;h1&gt;World Hacker's Notebook&lt;/h1&gt;</description>
    <link>http://shawnLeeZX.github.io/</link>
    <atom:link href="http://shawnLeeZX.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 14 Mar 2017 15:20:49 +0800</pubDate>
    <lastBuildDate>Tue, 14 Mar 2017 15:20:49 +0800</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>龙的牙医——这就是活着啊</title>
        <description>
</description>
        <pubDate>Tue, 14 Mar 2017 15:19:41 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2017/03/14/zhe-jiu-shi-huo-zhe-a/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2017/03/14/zhe-jiu-shi-huo-zhe-a/</guid>
        
        
      </item>
    
      <item>
        <title>Calculating Receptive Field of CNN</title>
        <description>&lt;!-- more --&gt;

&lt;p&gt;The receptive field (RF) $l_k$ of layer $k$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l_k = l_{k-1} + ((f_k - 1) * \prod_{i=1}^{k-1}s_i)&lt;/script&gt;

&lt;p&gt;where $l_{k-1}$ is the receptive field of layer $k-1$, $f_k$ is the filter size
(height or width, but assuming they are the same here), and $s_i$ is the stride
of layer $i$.&lt;/p&gt;

&lt;p&gt;The formula above calculates receptive field from bottom up (from layer
1). Intuitively, RF in layer $k$ covers $(f_k - 1) * s_{k-1}$ more pixels
relative with layer $k-1$. However, the increment needs to be translated to the
first layer, so the increments is a factorial — a stride in layer $k-1$ is
exponentially more strides in the lower layers.&lt;/p&gt;
</description>
        <pubDate>Sat, 11 Feb 2017 22:20:05 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2017/02/11/calculating-receptive-field-of-cnn/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2017/02/11/calculating-receptive-field-of-cnn/</guid>
        
        
        <category>CNN</category>
        
      </item>
    
      <item>
        <title>A Command Line Music Player in One Line of Code</title>
        <description>&lt;p&gt;I manually maintain the songs and music I listen by folder, and most of the
time, the only function of a music player useful is to import music in a folder
and listen to them. However, for unknown reason, rhythmbox (default music player
of Ubuntu) consumes a large number of memory and CPU times. So I consider
finding a way to do play music through command line, which lets me get rid
clicking, and the other trouble.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mpg123&lt;/code&gt; is an open-source mp3 file players, and it supports playing by file
list. To play all the mp3 files under current folder&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;find . -name &lt;span class=&quot;s2&quot;&gt;&quot;*.mp3&quot;&lt;/span&gt; | mpg123 -@ -
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;which pipes the files found by &lt;code class=&quot;highlighter-rouge&quot;&gt;find&lt;/code&gt; to mpg123, where &lt;code class=&quot;highlighter-rouge&quot;&gt;-@&lt;/code&gt; is short for
&lt;code class=&quot;highlighter-rouge&quot;&gt;--list&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;-&lt;/code&gt; is short for stand input.&lt;/p&gt;

&lt;p&gt;To pause the playing, type &lt;code class=&quot;highlighter-rouge&quot;&gt;C-Z&lt;/code&gt; which stops the program. Type &lt;code class=&quot;highlighter-rouge&quot;&gt;fg&lt;/code&gt; to bring it
to the foreground and play again.&lt;/p&gt;

&lt;p&gt;To create a shuffled play list, just pipe through &lt;code class=&quot;highlighter-rouge&quot;&gt;shuf&lt;/code&gt; before &lt;code class=&quot;highlighter-rouge&quot;&gt;mpg123&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;find . -name &lt;span class=&quot;s2&quot;&gt;&quot;*.mp3&quot;&lt;/span&gt; | shuf | mpg123 -@ -
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Enjoy :).&lt;/p&gt;
</description>
        <pubDate>Thu, 26 Jan 2017 16:22:37 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2017/01/26/a-command-line-music-player-in-one-line-of-code/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2017/01/26/a-command-line-music-player-in-one-line-of-code/</guid>
        
        
        <category>Linux</category>
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>相由心生</title>
        <description>&lt;p&gt;身是菩提樹，&lt;/p&gt;

&lt;p&gt;心如明鏡台，&lt;/p&gt;

&lt;p&gt;時時勤拂拭，&lt;/p&gt;

&lt;p&gt;勿使惹塵埃。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;菩提本無樹，&lt;/p&gt;

&lt;p&gt;明鏡亦非台，&lt;/p&gt;

&lt;p&gt;本來無一物，&lt;/p&gt;

&lt;p&gt;何處惹塵埃？&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;section&quot;&gt;故事&lt;/h2&gt;

&lt;p&gt;在南北朝的時候，佛教禪宗傳到了第五祖弘忍大師，弘忍大師當時在湖北的黃梅開壇講學，手下有弟子五百餘人，其中翹楚者當屬大弟子神秀大師。&lt;/p&gt;

&lt;p&gt;神秀也是大家公認的禪宗衣缽的繼承人。&lt;/p&gt;

&lt;p&gt;弘忍漸漸的老去，於是他要在弟子中尋找一個繼承人，所以他就對徒弟們說，大家都做一首畿子（有禪意的詩），看誰做得好就傳衣缽給誰。&lt;/p&gt;

&lt;p&gt;這時神秀很想繼承衣缽，但又怕因為出於繼承衣缽的目的而去做這個畿子，違法了佛家的無為而作意境。
所以他就在半夜起來，在院牆上寫了一首畿子身是菩提樹，心為明鏡台。時時勤拂拭，勿使惹塵埃。
這首畿子的意思是，要時時刻刻的去照顧自己的心靈和心境，通過不斷的修行來抗拒外面的誘惑，和種種邪魔。&lt;/p&gt;

&lt;p&gt;是一種入世的心態，強調修行的作用。
而這種理解與禪宗大乘教派的頓悟是不太吻合的，所以當第二天早上大家看到這個畿子的時候，都說好，而且都猜到是神秀作的而很佩服的時候，弘忍看到了以後沒有做任何的評價。
因為他知道神秀還沒有頓悟。&lt;/p&gt;

&lt;p&gt;而這時，當廟裡的和尚們都在談論這首畿子的時候，被廚房裡的一個火頭僧—慧能禪師聽到了。
慧能當時就叫別人帶他去看這個畿子，這裡需要說明的一點是，慧能是個文盲，他不識字。
他聽別人說了這個畿子，當時就說這個人還沒有領悟到真諦啊。
於是他自己又做了一個畿子，央求別人寫在了神秀的畿子的旁邊，菩提本無樹，明鏡亦非台，本來無一物，何處惹塵埃。
有這首畿子可以看出慧能是個有大智慧的人（後世有人說他是十世比丘轉世），他這個畿子很契合禪宗的頓悟的理念。
是一種出世的態度，主要意思是，世上本來就是空的，看世間萬物無不是一個空字，心本來就是空的話，就無所謂抗拒外面的誘惑，任何事物從心而過，不留痕跡。
這是禪宗的一種很高的境界，領略到這層境界的人，就是所謂的開悟了。&lt;/p&gt;

&lt;p&gt;弘忍看到這個畿子以後，問身邊的人是誰寫的，邊上的人說是慧能寫的，於是他叫來了慧能，當著他和其他僧人的面說：寫得亂七八糟，胡言亂語，並親自擦掉了這個畿子。
然後在慧能的頭上打了三下就走了。這時只有慧能理解了五祖的意思，於是他在晚上三更的時候去了弘忍的禪房，在那裡弘忍向他講解了《金剛經》這部佛教最重要的經典之一，並傳了衣缽給他。
然後為了防止神秀的人傷害慧能，讓慧能連夜逃走。於是慧能連夜遠走南方，隱居10年之後在莆田少林寺創立了禪宗的南宗。
而神秀在第二天知道了這件事以後，曾派人去追慧能，但沒有追到。&lt;/p&gt;

&lt;p&gt;後來神秀成為梁朝的護國法師，創立了禪宗的北宗。&lt;/p&gt;
</description>
        <pubDate>Sun, 01 Jan 2017 11:48:59 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2017/01/01/puti/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2017/01/01/puti/</guid>
        
        
        <category>Life</category>
        
      </item>
    
      <item>
        <title>The Brave World</title>
        <description>&lt;p&gt;我想，从今而始，新世界到来了。&lt;/p&gt;
</description>
        <pubDate>Sat, 05 Nov 2016 10:11:56 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2016/11/05/the-brave-world/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2016/11/05/the-brave-world/</guid>
        
        
        <category>Life</category>
        
      </item>
    
      <item>
        <title>Excerpts of The Glory and The Dream, Narrative History of America</title>
        <description>&lt;p&gt;When I was reading this book, I did not take notes or do underlining because it
is from the library. I have much that I want to put it down. The stories of
Mrs. Roosevelt, the union movement, World War II, Korean War, the raise of
radio, the pocket radio in 60s, counter-culture in 60s, racial movement, Cuba
revolution etc. However, I only manage to do this now, about two months
later. The thread in my mind has faded way. I do not think I could skim the
book again to get those things. In the following is the ending of the book.
The end of the ending is particularly interesting. Have faith for the future.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;ending&quot;&gt;Ending&lt;/h2&gt;

&lt;p&gt;There is a school of historians which holds that great events may tell us less
about the past than the trivia accumulated by ordinary people -the letters,
pressed flowers, prom programs, cherished toys and the like saved by those who
loved them and could not bear to throw them away.  From time to time
construction workers will stumble across such caches, sometimes entombed in old
mansions.  Occasionally they may find something almost as elaborate as the
Westinghouse time capsule which was buried at the New York World’s Fair
of 1939.  Such discoveries always excite curiosity, and the older ones stir
speculation over what this or that article meant to people at the time it was
put away.  With the growing mobility of Americans the accumulation of such
troves is rarer, but if members of the swing generation had once put away,
perhaps, in a storeroom the size of Fibber McGee’s fabled closet- it might
provide insight into what they had been like, what they had endured, what their
dreams had been, and which had been realized and which dashed.&lt;/p&gt;

&lt;p&gt;Envisaging such a cupboard, we see in front on the top shelf a steel tennis
racket, several dieting books, a wide necktie, and a pantsuit broad in the
beam.  Just behind them are a “Welcome Home POWs” bumper sticker, one for MIAs
(“Only Hanoi Knows” ), and a peace decal; then a brass-colored PT boat tie
clip, and cassette recordings of Camelot, Arlo Guthrie’s Alice s Restaurant,
and Carol Channing’s Hello, Dolly.  Behind them, well hidden in a corner
beneath a pile of tie-dyed jeans, are well- thumbed copies of Fanny Hill and
The Autobiography of a Flea.&lt;/p&gt;

&lt;p&gt;Various items of clothing occupy much of the space on the second shelf: a
sheath dress, a gray flannel suit, a man’s narrow-brimmed felt hat, several
incredibly narrow neckties, a child’s coonskin cap, and a straw boater with the
legend i like ike on the hatband.  Concealed beneath them is an obsolete item
of female apparel: a diaphragm in a white plastic case.  Beyond is a curious
little silver lapel pin.  It resembles the bottom of a man’s shoe with a hole
in the sole.  Nearby are a My Fair Lady album, a record of Edith Piaf singing
“II Pleut,” a Winky Clink kit, a Mouseketeer cap, and a collapsed Bayby-Tenda.
Copies of Fireman Small and Peyton Place lie on top of miscellaneous papers: a
pamphlet on how to stop smoking, a Fish House Punch recipe, an Around the World
in Eighty Days program, a batch of bills from a diaper service, and an envelope
containing plans for a home bomb shelter (never opened).&lt;/p&gt;

&lt;p&gt;Near the front of the third shelf is a Dior New Look skirt, an Eisenhower
jacket which appears to have been worn by a slender man, early nylons, a
freshman beanie, a copy of Tropic of Cancer, and under it a packet of three
Trojans.  (They sold for a dollar.) Various certificates: military discharge,
marriage license, college diplomas.  A ruptured duck pin.&lt;/p&gt;

&lt;p&gt;An Army divisional patch.  Rationing stamps.  Navy dog tags, long tarnished.  A
packet of V-letters.  A Nazi helmet; a samurai sword.  A Kate Smith Columbia
record: “God Bless America.”  A rhinestone V-for-Victory pin.  The bottom shelf
is rather junky.  A pair of Thorn Mean saddle shoes, very dirty, stand on top
of an equally soiled reversible raincoat, beneath which is a sport coat with a
belted back.  A dead corsage is pressed between two 78 rpm records” Deep
Purple” and “Stardust.”  Beside them lie campaign pins reading “We Want
Willkie” and “FDR.”  A third pin is shaped like a sunflower.  Then: a shabby
Phiico radio in the form of an arch, a tattered copy of Gone With the Wind, a
copy of Ulysses in which only the last forty pages seem to have been read, Boy
and Girl Scout handbooks, and several square Big Little Books.  There is a
dusty Lionel train transformer, a jump rope, several marbles and one steelie, a
splintered hockey stick, a well-oiled first baseman’s mitt, a Shirley Temple
doll, a sheaf of bubble gum cards, a G-man cap gun.  Two Post Toasties box
tops.  A box of cherry bombs.  A Bolo ball attached by elastic to a paddle.  A
pair of brown corduroy knickers.  A hair ribbon.  An old stand-up telephone.&lt;/p&gt;

&lt;p&gt;Lastly, on the floor of the closet, are a batch of snapshots taken with a box
Brownie.  There are automobiles in them: a Model A Ford with the windshield
down in some, a Chevy sporting a sassy rumble seat in others, and in the older
ones, brown with age, a Model T. People are posing by the running boards.  It
is summer, yet the adults look very formal.  The men are wearing stiff collars,
the women vast hats and shapeless cotton dresses.  &lt;strong&gt;But it is the children who
seem oddest.  Like their parents they are quaintly dressed.  There is something
else, though.  It takes a moment to realize why they look so peculiar.  Then
you see it.  There is an intensity in their expressions.  They are leaning
slightly forward, as though trying to see into the future.  And they are
smiling.&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 02 Nov 2016 21:19:47 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2016/11/02/excerpts-of-the-glory-and-the-dream-narrative-history-of-america/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2016/11/02/excerpts-of-the-glory-and-the-dream-narrative-history-of-america/</guid>
        
        
        <category>History</category>
        
        <category>Excerpts</category>
        
      </item>
    
      <item>
        <title>Key points of Understanding Deep Conv Net</title>
        <description>&lt;p&gt;This note tries to summarize the keypoints of the paper by Mallat:
&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/26953183&quot;&gt;Understanding deep convolutional networks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Key concepts: contraction, linearization, fibre, parallel transport,
multi-scale support vector&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;Mechanically, Convolutional Neural Network could be formalized using cascading
the following of operators that compose a function $f$, whose value is in $R$
for regression problems, and is the index of the sample’s class for
classification problems.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_j = \rho W_j x_{j-1}&lt;/script&gt;

&lt;p&gt;where $j$ indicates the depth of a network.&lt;/p&gt;

&lt;p&gt;Essentially, neural network is a way to combat the curse of dimensionality. In
the paper, Mallat formalizes this process using linearization of hierarchical
symmetry, space contraction and sparse separation. In short, it is done by
defining a new variable $\Phi(x)$, where $\Phi$ is a &lt;em&gt;contractive&lt;/em&gt; operator
that reduces the range of variations of $x$, while still &lt;em&gt;separating&lt;/em&gt; different
values of $f: \Phi(x) \not= \Phi(x’)$ if $f(x) \not= f(x’)$. $\Phi$ should have
the properties to &lt;strong&gt;linearize hierarchical symmetries, does space contraction and
achieve sparse separation&lt;/strong&gt; to keep classification margin (value margin for
regression). $\Phi$ is called the new representation of $x$.&lt;/p&gt;

&lt;h2 id=&quot;linearization-of-symmetries&quot;&gt;Linearization of symmetries&lt;/h2&gt;

&lt;p&gt;Neural network finds equivalence classes $\Phi(x)$ that linearize
$f$. $\Phi(x)$ is built by collapsing complex symmetry groups.&lt;/p&gt;

&lt;h3 id=&quot;linearization&quot;&gt;Linearization&lt;/h3&gt;

&lt;p&gt;By definition, linearization means to find a representation $\Phi$ that $f$ on
$x \in \Omega$, where the domain $\Omega$ is a high dimensional open set, which
could be $L^2(R^n)$ or $R^d$, could be approximated by a linear combination&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\bar{f} = \sum_i\Phi_i(x)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The idea of linearization is to find important yet highly complex basis vector
that linearly contributes to $f$, so $f$ is a linear projection of
&lt;script type=&quot;math/tex&quot;&gt;\Phi_i(x)&lt;/script&gt;. An example would be the word vector, like &lt;code class=&quot;highlighter-rouge&quot;&gt;actor - man + woman =
actress&lt;/code&gt;. Similar phenomenon is observed in images as well.&lt;/p&gt;

&lt;p&gt;On the other hand, it also means $\Phi(x)$ absorbs in the variability that is
not related to $f$. An example could be translation variability, which is
absorbed by subsampling in NN.&lt;/p&gt;

&lt;p&gt;It is difficulty for me to understand what does Mallat mean by&lt;/p&gt;

&lt;p&gt;``We can then optimize a low-dimensional linear projection along directions where
$f$ is constant.’’&lt;/p&gt;

&lt;p&gt;The above is the meaning I have guessed. In summary, if $\Phi$ is fixed, the
optimization works on the linear projection’s weight. ``$f$ is constant’’ means
after the change of variable, the variability in the original space $\Omega$
has been absorbed, so a variability in the direction that does not change
$\Phi$ does not change $f$.&lt;/p&gt;

&lt;h3 id=&quot;space-collapse&quot;&gt;Space collapse&lt;/h3&gt;

&lt;p&gt;The key is to find $\Phi$ that linearizes $f$, which is to find $\Phi$ that
collapses the directions that $f$ remains constant. Mallat formalizes this
``collapse’’ in term of group theory, or more particularly the group of
transformations.&lt;/p&gt;

&lt;p&gt;The word collapse is my own creation, which tries to summarize the key point of
building invariant representation $\Phi$ utilizing groups of
symmetry. It corresponds to the absorbing of variability in previous
section. &lt;script type=&quot;math/tex&quot;&gt;\Phi_i(x)&lt;/script&gt; represents an equivalence class (could be the orbit of
the group, more on this later) of $x$. Collapse is the process of combining
those classes together to form an equivalence class.&lt;/p&gt;

&lt;p&gt;Formally, if $f$ is invariant to an operator $g$ that preserves the value of
$f$, which is to say $f(gx) = f(x)$, we can safely incorporate $g$ in $\Phi(x)$
(putting $gx$ in an equivalence class), so any transformation $g$ on $x$, aka
$gx$, does not change $\Phi(x)$, consequently it does not change $f$. Acting as
an equivalence class, they contribute to the linearization of $f$.&lt;/p&gt;

&lt;p&gt;An example could be the translation group as an example, $f(gx) = f(x - u)$,
where $u \in \Omega$, representing the displacement of the translation. $f$’s
value does not change by translation, so by averaging (subsampling) spatially,
a direction that $f$ remains constant is collapsed out. This direction is part
of a equivalence class formed by collapsing out the translation group.&lt;/p&gt;

&lt;p&gt;By pinpointing those equivalence classes $\Phi(x)$, linearization is achieved
by doing linearly projection on them.&lt;/p&gt;

&lt;h2 id=&quot;more-details&quot;&gt;More details&lt;/h2&gt;

&lt;p&gt;The high level plan stops here. Those above two are the main components making
up the framework of NN. Now we fill in the details, aka sparse separation,
space contraction, separation of scales and complex interaction among
multi-scale.&lt;/p&gt;

&lt;h3 id=&quot;the-functional-form-of-phi&quot;&gt;The functional form of $\Phi$&lt;/h3&gt;

&lt;p&gt;$\Phi$ is handcrafted basis when the time of dictionary learning or deep
learning has not come. It is a vector that captures important (in any criteria
matters for the problem at hand) features.&lt;/p&gt;

&lt;h3 id=&quot;contraction-for-sparsity&quot;&gt;Contraction for sparsity&lt;/h3&gt;

&lt;p&gt;There are some general criteria for basis $\Phi$, one of which may require them
be sparse. A sparse signal brings to better linear separation. Or in other
word, factors of variation are better disentangled.&lt;/p&gt;

&lt;p&gt;For fixed basis vectors, such as wavelets in scattering network they are
defined to separate scale and frequency. Thus for signals that only contains a
few of those, the wavelet coefficients would be rather sparse.&lt;/p&gt;

&lt;p&gt;For an adaptive basis vector, to promote sparsity, Mallat puts forward the
concept of space contraction, which is achieved through the non-linear
activation function, a modulus or a ReLU. It explicitly contract the volume of
the space to achieve a sparse representation.&lt;/p&gt;

&lt;p&gt;In the case of fixed basis like wavelet, if the representation is already
sparse, non-linear activation won’t reduce the volume too much since most of
the dimensions are already zero.&lt;/p&gt;

&lt;h3 id=&quot;diffeomorphism-calls-for-multi-scale-separation&quot;&gt;Diffeomorphism calls for multi-scale separation&lt;/h3&gt;

&lt;p&gt;Diffeomorphism can also be modeled a group symmetry. Diffeomorphism could be
factorized as a translation and a scaling effects. So to linearize
diffeomorphism, a multi-scale basis to separate the scaling effects are needed.&lt;/p&gt;

&lt;h3 id=&quot;multi-scale-interaction&quot;&gt;Multi-scale interaction&lt;/h3&gt;

&lt;p&gt;It is not enough to only separate signal using basis vectors, but also to model
their interactions. This justifies a deep architecture.&lt;/p&gt;

&lt;h2 id=&quot;combining-all-the-above-together&quot;&gt;Combining all the above together&lt;/h2&gt;

&lt;p&gt;Now we could map each symbol in the following to their functional roles.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_j = \rho W_j x_{j-1}&lt;/script&gt;

&lt;p&gt;Each row of $W$ is the basis vector; $\rho$ is the contraction operator to
promote sparsity; &lt;script type=&quot;math/tex&quot;&gt;x_j = ... x_{j-1}&lt;/script&gt; is the recursive form that takes cares
multi-scale separation and interaction.&lt;/p&gt;

&lt;p&gt;At the same time, &lt;script type=&quot;math/tex&quot;&gt;W_{j-1}x_{j-1}&lt;/script&gt; linearizes &lt;script type=&quot;math/tex&quot;&gt;W_j x_j&lt;/script&gt;. If we adding
pooling in, it corresponds to collapsing the direction where $f$ remains
constant.&lt;/p&gt;

&lt;h2 id=&quot;recast-in-term-of-differential-geometry&quot;&gt;Recast in term of differential geometry&lt;/h2&gt;

&lt;p&gt;Mallat use the formalism in differential geometry to succinctly define the
hierarchically built $\Phi$.&lt;/p&gt;

&lt;p&gt;According to Mallat, the power of Neural Network is it is able learn such a
representation that linearize hierarchical symmetry groups, which is formalized
using fibres of hierarchical symmetry groups. Nonlinear contraction reduces the
variability of each fibre, so removing the variability that is not directly
related to currently sample being processed. Fibres of current layer are
transported to fibres of next layers, which are fibres of large
groups. Cascadingly applying the parallel transport on a sample, we obtain
multi-scale support vectors that are sparse.&lt;/p&gt;

&lt;p&gt;However, it is hard to map these math names to the operational name of conv
net. Here is my best guess up to now.&lt;/p&gt;

&lt;p&gt;Each channel of a conv filter is a part of the orbits.
The equivalence class is the cross-channel sum of orbits of filters. The index
$P_j$ is the indices of orbits, which could be understood as the spatial
indices (orbits of translation group) and channel indices (orbits of those
groups, such as rotation groups). Since we have done a summation, how each 2D
filter corresponds to each part of the orbit does not matter, thus we get an
equivalence class.&lt;/p&gt;

&lt;p&gt;Each equivalence class is called a fibre, which is just the fancy in
differential geometry to call an operator. In this case, the fibre is just the
cascadingly built operator $ρW_j$ mentioned at the beginning section. $ρW_j$
computes an approximate mapping from fibre of layer $j-1$ to $j$, which is
called a parallel transport in $P_j$.
A parallel transport is defined by a group $G_j$ of symmetries acting on the
index set $P_j$ of a layer $x_j$.
It is called an approximate mapping because fibre is supposed to be an
equivalence class of orbits of $x_{j-1}$, and the transport be mapping from
those equivalence class. however $pW_j$ starts from $x_{j-1}$, not
equivalence classes of orbits of $x_{j-1}$. So it is assumed to be approximated
by applying $g$ to $x_j$, we get $\bar{g}$ on $x_{j-1}$. Again implicitly
$x_{j}$ will be unrolled to the equivalence class of orbit of this layer,
which is a larger groups which is semidirect products of groups of layer $j-1$,
and groups (corresponding to channels in layer $j$) of layer $j$. The
approximation only makes sense by corresponding each channel of a filter to be
a part of a orbit of $g$, as in the previous paragraph. The channel of filters
are the groups of layer $j-1$, while the number of filters (output channel
number in most convnet implementations) is the groups of the layer
$j$. Assuming each filter channel corresponds to a part of the
orbit, $g.x_{j} = g.[\rho W_j x_{j-1}] \approx \rho W_j[\bar{g}.x_{j-1}]$
in the paper makes sense, since the summation cross channel would absorb $g$ in
$W_{j}$.&lt;/p&gt;

&lt;p&gt;So the convolution is called convolutions along the fibres. Implicitly, $\rho
W_j$ expands $x_{j-1}$ to its orbits, then computes equivalence classes of
orbits of $x_{j}$ by cross-channelly adding the result of applying filters on
those orbits. If in this sense, $x_{j-1}$ is indeed an equivalence class of
orbits. I guess the approximation means that this is just an assumption, since
no constrains are enforced to make sure channels of a filter are orbits of
$x_{j-1}$.&lt;/p&gt;

&lt;p&gt;In this model, network filters are guiding nonlinear contractions, to reduce the
data variability in directions of local symmetries. The classification margin
can be controlled by sparse separations along network fibres&lt;/p&gt;

&lt;p&gt;Each fibre is complex basis vector. Sparsity ensures separateness. Mallat
introduces the concept of multi-scale support vector, explained in the
following. To avoid further contracting their distance, they can be separated
along different fibres indexed by $b$. The separation is achieved by filters
$w_{j,h.b}$, which transform $x_{j−1}$ and $x’_{j−1}$ into $x_{j}(g, h,
b)$and $x’_{j}(g, h, b)$ having sparse supports on different fibres $b$. The
next contraction $ρW_{j+1}$ reduces distances along fibres indexed by $(g, h)
\in G_j$,but not across $b \in B_j$,which preserves distances.&lt;/p&gt;
</description>
        <pubDate>Mon, 29 Aug 2016 10:41:00 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2016/08/29/key-points-of-understanding-deep-conv-net/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2016/08/29/key-points-of-understanding-deep-conv-net/</guid>
        
        
        <category>CNN</category>
        
      </item>
    
      <item>
        <title>An idea to gather people's stories</title>
        <description>&lt;p&gt;In post-religion world, the God has stepped down, even humanism. People start to
look for icons that is personal yet inspirational, who could set an example for
their life. The idea of this project is to collect the people who may a
tech-utopian, aka the people who believe by making progress in technology, and
ease the dissemination of knowledge and information, eventually, the power could
be brought to the people.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Mark_Shuttleworth&quot;&gt;Mark_Shuttleworth&lt;/a&gt;, the funder
of Ubuntu.&lt;/p&gt;

</description>
        <pubDate>Sat, 27 Aug 2016 09:35:00 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2016/08/27/a-idea-to-gather-peoples-stories/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2016/08/27/a-idea-to-gather-peoples-stories/</guid>
        
        
      </item>
    
      <item>
        <title>[Repost] Explosion of the Space Shuttle Challenger Address to the Nation</title>
        <description>&lt;p&gt;Accidentially heared the &lt;a href=&quot;https://youtu.be/SBOVkptjJhE?t=3959&quot;&gt;speech&lt;/a&gt; given by
Reagan to the nation after Challenger crashed.&lt;/p&gt;

&lt;p&gt;Below is an excerpt. Full transcript is after that. The
&lt;a href=&quot;http://history.nasa.gov/reagan12886.html&quot;&gt;transcript&lt;/a&gt; is from NASA website.&lt;/p&gt;

&lt;h2 id=&quot;excerpts&quot;&gt;Excerpts&lt;/h2&gt;

&lt;p&gt;We’ve grown used to wonders in this century. It’s hard to dazzle us. But for 25
years the United States space program has been doing just that. We’ve grown
used to the idea of space, and perhaps we forget that we’ve only just
begun. We’re still pioneers. They, the members of the Challenger crew, were
pioneers.&lt;/p&gt;

&lt;p&gt;I know it is hard to understand, but sometimes painful things like this
happen. It’s all part of the process of exploration and discovery. It’s all
part of taking a chance and expanding man’s horizons. &lt;strong&gt;The future doesn’t belong
to the fainthearted; it belongs to the brave&lt;/strong&gt;. The Challenger crew was pulling
us into the future, and we’ll continue to follow them.&lt;/p&gt;

&lt;p&gt;There’s a coincidence today. On this day 390 years ago, the great explorer Sir
Francis Drake died aboard ship off the coast of Panama. &lt;strong&gt;In his lifetime the
great frontiers were the oceans, and an historian later said, “He lived by the
sea, died on it, and was buried in it.”&lt;/strong&gt; Well, today we can say of the
Challenger crew: Their dedication was, like Drake’s, complete.&lt;/p&gt;

&lt;p&gt;The crew of the space shuttle Challenger honored us by the manner in which they
lived their lives. &lt;strong&gt;We will never forget them, nor the last time we saw them,
this morning, as they prepared for their journey and waved goodbye and “slipped
the surly bonds of earth” to “touch the face of God.”&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;full-speech&quot;&gt;Full Speech&lt;/h2&gt;

&lt;!-- more --&gt;

&lt;p&gt;Ladies and gentlemen, I’d planned to speak to you tonight to report on the
state of the Union, but the events of earlier today have led me to change those
plans. Today is a day for mourning and remembering.&lt;/p&gt;

&lt;p&gt;Nancy and I are pained to the core by the tragedy of the shuttle Challenger. We
know we share this pain with all of the people of our country. This is truly a
national loss.&lt;/p&gt;

&lt;p&gt;Nineteen years ago, almost to the day, we lost three astronauts in a terrible
accident on the ground. But we’ve never lost an astronaut in flight; we’ve
never had a tragedy like this. And perhaps we’ve forgotten the courage it took
for the crew of the shuttle; but they, the Challenger Seven, were aware of the
dangers, but overcame them and did their jobs brilliantly. We mourn seven
heroes: Michael Smith, Dick Scobee, Judith Resnik, Ronald McNair, Ellison
Onizuka, Gregory Jarvis, and Christa McAuliffe. We mourn their loss as a nation
together.&lt;/p&gt;

&lt;p&gt;For the families of the seven, we cannot bear, as you do, the full impact of
this tragedy. But we feel the loss, and we’re thinking about you so very
much. Your loved ones were daring and brave, and they had that special grace,
that special spirit that says, “Give me a challenge and I’ll meet it with joy.”
They had a hunger to explore the universe and discover its truths. They wished
to serve, and they did. They served all of us.&lt;/p&gt;

&lt;p&gt;We’ve grown used to wonders in this century. It’s hard to dazzle us. But for 25
years the United States space program has been doing just that. We’ve grown
used to the idea of space, and perhaps we forget that we’ve only just
begun. We’re still pioneers. They, the members of the Challenger crew, were
pioneers.&lt;/p&gt;

&lt;p&gt;And I want to say something to the schoolchildren of America who were watching
the live coverage of the shuttle’s takeoff. I know it is hard to understand,
but sometimes painful things like this happen. It’s all part of the process of
exploration and discovery. It’s all part of taking a chance and expanding man’s
horizons. The future doesn’t belong to the fainthearted; it belongs to the
brave. The Challenger crew was pulling us into the future, and we’ll continue
to follow them.&lt;/p&gt;

&lt;p&gt;I’ve always had great faith in and respect for our space program, and what
happened today does nothing to diminish it. We don’t hide our space program. We
don’t keep secrets and cover things up. We do it all up front and in
public. That’s the way freedom is, and we wouldn’t change it for a minute.&lt;/p&gt;

&lt;p&gt;We’ll continue our quest in space. There will be more shuttle flights and more
shuttle crews and, yes, more volunteers, more civilians, more teachers in
space. Nothing ends here; our hopes and our journeys continue.&lt;/p&gt;

&lt;p&gt;I want to add that I wish I could talk to every man and woman who works for
NASA or who worked on this mission and tell them: “Your dedication and
professionalism have moved and impressed us for decades. And we know of your
anguish. We share it.”&lt;/p&gt;

&lt;p&gt;There’s a coincidence today. On this day 390 years ago, the great explorer Sir
Francis Drake died aboard ship off the coast of Panama. In his lifetime the
great frontiers were the oceans, and an historian later said, “He lived by the
sea, died on it, and was buried in it.” Well, today we can say of the
Challenger crew: Their dedication was, like Drake’s, complete.&lt;/p&gt;

&lt;p&gt;The crew of the space shuttle Challenger honored us by the manner in which they
lived their lives. We will never forget them, nor the last time we saw them,
this morning, as they prepared for their journey and waved goodbye and “slipped
the surly bonds of earth” to “touch the face of God.”&lt;/p&gt;
</description>
        <pubDate>Sat, 30 Jul 2016 08:42:00 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2016/07/30/repost-explosion-of-the-space-shuttle-challenger-address-to-the-nation/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2016/07/30/repost-explosion-of-the-space-shuttle-challenger-address-to-the-nation/</guid>
        
        
        <category>Repost</category>
        
        <category>Life</category>
        
      </item>
    
      <item>
        <title>Note On Representation of Operator</title>
        <description>&lt;p&gt;A note to remind me of some intuition ruminated. Do not have the time to write
down detailed proof.&lt;/p&gt;

&lt;p&gt;It is about spectral theorem of matrix, adjoint operator, SVD and tensor.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;spectral-theorem&quot;&gt;Spectral Theorem&lt;/h2&gt;

&lt;p&gt;The representation theory of linear functional is essentially one dimension
version of group representation theory. The $w$ in the inner product $\langle
x, w \rangle$ representation of functional $f(x)$ is essentially the normal of
the hyperplane, which is the functional’s representation in the dual space. The
process of applying $f$ on $x$ is a process of projection.&lt;/p&gt;

&lt;p&gt;For a general operator $T: X \rightarrow Y$, whose representation is denoted
$A$, if the bases of the $X, Y$ are orthogonal, the adjoint operator $T^{*}$ of
$T$’s representation is $A^{T}$(on the condition that bases of $X, Y$ is chosen
to be orthogonal). The process of applying $T$ on $x$ is a process of first
doing multiple projection, then sum those projection together. The fact that
adjoint operator has the spectral theorem is just because in certain bases,
when we are applying the multiple projection, we are not projecting at all, but
just scale along the hyperplane. The process of changing bases, applying
scaling, then changing bases back written in form of matrix multiplication is
$M^{-1}\Lambda M$, which is the spectral decomposition of representation of $T$.&lt;/p&gt;

&lt;h2 id=&quot;adjoint-operator-is-just-the-transpose&quot;&gt;Adjoint Operator is Just the Transpose&lt;/h2&gt;

&lt;p&gt;The projection perspective also explains why $T^{*} = A^{T}$.&lt;/p&gt;

&lt;p&gt;Let $Ax = y$. Each $x_i$ contributes to $y_i$ linearly. So just like a linear
equation has a unique solution as long as $A$ is of full rank, there must be a
one-to-one relationship between $T$ and $T^{*}$, which is formalized using
representation theory of functional again.&lt;/p&gt;

&lt;h2 id=&quot;svd&quot;&gt;SVD&lt;/h2&gt;

&lt;p&gt;SVD just factorizes the multi-projection into a rotation matrix and a scaling
matrix.&lt;/p&gt;

&lt;h2 id=&quot;tensor&quot;&gt;Tensor&lt;/h2&gt;

&lt;p&gt;Tensor is a generalized operator that handles multi-linearity.&lt;/p&gt;
</description>
        <pubDate>Sun, 01 May 2016 17:02:00 +0800</pubDate>
        <link>http://shawnLeeZX.github.io/blog/2016/05/01/note-on-representation-of-operator/</link>
        <guid isPermaLink="true">http://shawnLeeZX.github.io/blog/2016/05/01/note-on-representation-of-operator/</guid>
        
        
        <category>Math</category>
        
        <category>Note</category>
        
      </item>
    
  </channel>
</rss>
