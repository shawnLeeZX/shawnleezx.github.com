<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta content="Shawn" property="og:site_name">
  <link href="/favicon.png" rel="icon">

  
  <meta content="Research Catalog" property="og:title">
  

  
  <meta content="article" property="og:type">
  

  
  <meta content="<h1>Children of The Sun</h1> " property="og:description">
  

  
  <meta content="http://shawnLeeZX.github.io/research_catalog" property="og:url">
  

  

  <meta property="og:image" content="">

  

  

  <title>Research Catalog - Shawn</title>
  <meta name="description" content="<h1>Children of The Sun</h1> ">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/css/bootstrap.min.css" integrity="sha384-GJzZqFGwb1QTTN6wy59ffF1BuGJpLSa9DkKMp0DgiMDm4iYMj70gZWKYbI706tWS" crossorigin="anonymous">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.slim.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="/css/main.css">

  <!-- Typesetting math using MathJax -->
  <!-- mathjax config similar to math.stackexchange -->
  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     jax: ["input/TeX", "output/HTML-CSS"],
     tex2jax: {
       inlineMath: [ ['$', '$'] ],
       displayMath: [ ['$$', '$$']],
       processEscapes: true,
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     },
     messageStyle: "none",
     "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
   });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-44220731-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>

  <link href="/css/fonts/orbitron.css" rel="stylesheet" type="text/css">
  <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
  <link rel="canonical" href="http://shawnLeeZX.github.io/research_catalog">
  <link rel="alternate" type="application/rss+xml" title="Shawn" href="http://shawnLeeZX.github.io/feed.xml">
</head>

  <body>
    <section>
  <nav class="navbar navbar-default navbar-expand-lg fixed-top">
    <div class="container">

      <!-- Brand and toggle get grouped for better mobile display -->
      <a class="navbar-brand" href="/">Shawn</a>
      <button type="button" class="navbar-toggle btn" data-toggle="collapse" data-target="#navBar" aria-expanded="false">
        <i class="fa fa-bars"></i>
      </button>

      <div class="collapse navbar-collapse" id="navBar">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item"><a class="nav-link" href="/research_catalog">Research</a></li>
          <li class="nav-item"><a class="nav-link" href="/philosophy">Philosophy</a></li>
          <li class="nav-item"><a class="nav-link" href="/blog">Notes</a></li>
        </ul>
      </div><!-- /.navbar-collapse -->

    </div>
  </nav>
</section>

    <section>
      <div class="jumbotron">
  <div class="container">
    <h1 class="page-title" itemprop="name headline">Research Catalog</h1>
    
  </div>
</div>

<div class="container content">
  <p><br />
I envision artificial intelligence (AI) will be a core component that starts a
new era, not in term of the Age of Reason vs the Age of Romantics, but in term
of the Renaissance vs the Middle Age. For the reasoning, read the essays in the
<em>philosophy panel</em> in the <a href="/">landing page</a>. Help is wanted, refer to <em>Call for
help</em> section in the <a href="https://arxiv.org/abs/1811.12783">manuscript</a>.</p>

<p>In current roadmap, the primary focus is to develop the theory of neural
networks — an introduction is present specifically for it
<a href="/theory_intro">here</a>. I also do some works out of hacking interests, and works
that support myself financially. The works are cataloged in this page.</p>

<h2 id="table-of-contents">Table of Contents</h2>

<ul id="markdown-toc">
  <li><a href="#table-of-contents" id="markdown-toc-table-of-contents">Table of Contents</a></li>
  <li><a href="#theory" id="markdown-toc-theory">Theory</a></li>
  <li><a href="#hacking" id="markdown-toc-hacking">Hacking</a></li>
  <li><a href="#miscellaneous" id="markdown-toc-miscellaneous">Miscellaneous</a></li>
</ul>

<h2 id="theory">Theory</h2>

<p><em>To get an impression of the big picture of the theory without getting too
technically involved, you can look at the slide <a href="/slides/nn_complexity">here</a>.</em></p>

<p><strong>Measure, Manifold, Learning, and Optimization: A Theory Of Neural Networks</strong>:
The theory gives S-System, <strong>a measure-theoretical definition of NNs</strong>; endows
a stochastic manifold structure on the intermediate feature space of NNs
through information geometry; proposes a learning framework that unifies both
supervised learning and unsupervised learning in the same objective function;
and proves <strong>under practical conditions</strong>, for <strong>large size nonlinear deep
NNs</strong> with a class of losses, including the hinge loss, <strong>all local minima are
global minima</strong> with zero loss errors. It also completes the analogy between
NNs and Renormalization Group. <a href="https://arxiv.org/abs/1811.12783">Preprint</a>.</p>

<p><em>The manuscript is taken as hard-to-understand for its first readers. I am in
the process of writing introductions to the work. I have finished an
easier-to-read manuscript on its optimization part</em>:</p>

<p><strong>The Benign Loss Landscape of Neural Networks</strong>. We study the loss landscape
of NNs, and prove under a set of conditions, for <em>large-size nonlinear
deep</em> NNs with a class of loss functions $\mathcal{L}_0$, including the
hinge loss, <em>all local minima are global minima with zero loss errors.</em> For
the class $\mathcal{L}_0$ of loss functions, the proof enables a complete
characterization of the stationary points of the NNs’ loss landscape:
eigenvalues of the Hessian of the NNs are distributed {\it symmetrically
w.r.t. zero}, and concentrate increasingly towards zero as the risk decreases.
We also show the loss landscape of a practical NN throughout training conforms
to the one characterized. <a href="/images/optnn.pdf">Preprint</a>.</p>

<p><em>Essays written to introduce the theory can be found</em> <a href="/theory_intro">here</a>.</p>

<h2 id="hacking">Hacking</h2>

<p><strong>Akid, A Library for Neural Network Research and Production From A Dataism
Approach</strong>: akid is a python neural network package that uses dataism
abstraction on backends (Tensorflow, or PyTorch are supported) for research in
NNs. It supports Acyclic Directed Computational Graph, Multi-GPU Computing,
Visualization (computation graph, weight filters, feature maps, and training
dynamics statistics), Meta-Syntax to generate network structure and more.  To
learn more, see <a href="/akid">here</a>.</p>

<h2 id="miscellaneous">Miscellaneous</h2>

<p><strong>Orthogonal Neural Networks:</strong> I am also involved with designing
algorithms to improve neural networks (NNs). The new algorithm
constrains the weight matrices of NNs to be orthogonal. It is
theoretically inspired by <strong>a novel generalization error bound</strong>
that goes beyond Lipschitz constant based bounds through a new
covering scheme. It characterizes the error induced by erroneous
expansion of intra-class variations, and erroneous contraction of
inter-class difference. The analysis relies on the proof that NNs
are local $\delta$-isometry by breaking the instance space down by
their hyperplane arrangement, and bounds the singular values
according to Cauchy interlacing law. Competitive results were
achieved by the proposed methods. <a href="https://arxiv.org/abs/1905.05929/">Preprint</a>.</p>

<p><strong>Regularization by Adversarial Robustness Makes Diffident Neural Networks</strong>:
The problem of adversarial examples has shown that modern Neural Network (NN)
models could be rather fragile. Among the most promising techniques to solve
the problem, one is to require the model to be <em>$\epsilon$-adversarially robust</em>
(AR); that is, to require the model not to change predicted labels when any
given input examples are perturbed within a certain range. However, it is
widely observed that such methods would lead to standard performance
degradation, i.e., the degradation on natural examples.  In this work, we study
the degradation through the regularization of AR on NNs. We
find that AR is achieved by regularizing/biasing NNs
towards less confident solutions by making the changes in the feature space
(induced by changes in the instance space) of most layers smoother uniformly in
all directions; so to a certain extent, it prevents sudden change in prediction
w.r.t.  perturbations. However, the end result of such smoothing concentrates
samples around decision boundaries and leads to worse standard performance.
 The results suggest we might consider ways that build AR into NNs a gentler
way to avoid the problematic regularization.
<a href="/images/ar.pdf">Preprint</a></p>

</div>

    </section>

    

    <nav class="navbar navbar-default navbar-fixed-bottom">
<div class="container footer-content">
    <!-- Nothing is there. -->
</div>
</nav>
  </body>

</html>
