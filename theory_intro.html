<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta content="Shawn" property="og:site_name">
  <link href="/favicon.png" rel="icon">

  
  <meta content="Deep Neural Network Theory Introduction" property="og:title">
  

  
  <meta content="article" property="og:type">
  

  
  <meta content="<h1>Children of The Sun</h1> " property="og:description">
  

  
  <meta content="http://shawnLeeZX.github.io/theory_intro" property="og:url">
  

  

  <meta property="og:image" content="">

  

  

  <title>Deep Neural Network Theory Introduction - Shawn</title>
  <meta name="description" content="<h1>Children of The Sun</h1> ">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/css/bootstrap.min.css" integrity="sha384-GJzZqFGwb1QTTN6wy59ffF1BuGJpLSa9DkKMp0DgiMDm4iYMj70gZWKYbI706tWS" crossorigin="anonymous">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.slim.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="/css/main.css">

  <!-- Typesetting math using MathJax -->
  <!-- mathjax config similar to math.stackexchange -->
  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     jax: ["input/TeX", "output/HTML-CSS"],
     tex2jax: {
       inlineMath: [ ['$', '$'] ],
       displayMath: [ ['$$', '$$']],
       processEscapes: true,
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     },
     messageStyle: "none",
     "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
   });
  </script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-44220731-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>

  <link href="/css/fonts/orbitron.css" rel="stylesheet" type="text/css">
  <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
  <link rel="canonical" href="http://shawnLeeZX.github.io/theory_intro">
  <link rel="alternate" type="application/rss+xml" title="Shawn" href="http://shawnLeeZX.github.io/feed.xml">
</head>

  <body>
    <section>
  <nav class="navbar navbar-default navbar-expand-lg fixed-top">
    <div class="container">

      <!-- Brand and toggle get grouped for better mobile display -->
      <a class="navbar-brand" href="/">Shawn</a>
      <button type="button" class="navbar-toggle btn" data-toggle="collapse" data-target="#navBar" aria-expanded="false">
        <i class="fa fa-bars"></i>
      </button>

      <div class="collapse navbar-collapse" id="navBar">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item"><a class="nav-link" href="/research_catalog">Research</a></li>
          <li class="nav-item"><a class="nav-link" href="/#philosophy">Philosophy</a></li>
          <li class="nav-item"><a class="nav-link" href="/employment">Updates</a></li>
          <li class="nav-item"><a class="nav-link" href="/blog">Notes</a></li>
        </ul>
      </div><!-- /.navbar-collapse -->

    </div>
  </nav>
</section>

    <section>
      <div class="jumbotron">
  <div class="container">
    <h1 class="page-title" itemprop="name headline">Deep Neural Network Theory Introduction</h1>
    
  </div>
</div>

<div class="container content">
  <ul id="markdown-toc">
  <li><a href="#overview-of-theory-of-dnns" id="markdown-toc-overview-of-theory-of-dnns">Overview of Theory of DNNs</a></li>
  <li><a href="#the-optimization-power-of-neural-networks" id="markdown-toc-the-optimization-power-of-neural-networks">The Optimization Power of Neural Networks</a></li>
</ul>

<h2 id="overview-of-theory-of-dnns">Overview of Theory of DNNs</h2>

<p>A short introduction to the theory is present in this section â€” it is rather
technical.  The theory is divided into the following <em>four</em> parts:</p>

<ul>
  <li>
    <p><strong>S-System</strong>, a formal measure-theoretical framework that builds a
hierarchical hypothesis space, of which NNs are special cases. It is
motivated by the fact that nature is a complex system built
hierarchically, and a mechanism is needed for any agents living in
it to recognize and predict hierarchical events happening.</p>
  </li>
  <li>
    <p>The <strong>geometry</strong> of S-System. The objects in the hypothesis space
are probability measures, thus have an information-geometry
structure. It characterizes the phenomenon that NNs compose and
recompose manifolds that have increasingly high level semantic
meaning.</p>
  </li>
  <li>
    <p>The <strong>learning framework</strong> of S-System. It describes the objective
functions to identify an element of the hypothesis space by learning
the parameters. It gives principled derivation of back propagation,
and unifies supervised learning and unsupervised learning in NNs.</p>
  </li>
  <li>
    <p>The <strong>optimization</strong> landscape of S-System. It identifies principles
and conditions that make the non-convex optimization of the learning
problem of S-System benign; that is, all local minima are global
minima.</p>
  </li>
</ul>

<p>The theory is a principle to assign probabilities to events, e.g., predicting the
event that today would be a rainy or sunny day, given past experience. To
recall what a principle to assign probabilities is, we recall the principle of
symmetry, and the law of large number (LLN). The example of the principle of
symmetry is ubiquitous in elementary probability theory, i.e., the equal
probability assigned to the event that front, or back side is obtained when
flipping a coin. So are the law of large number, which assigns normal
probability distribution to that of the mean of a large number of random
variables.  However, the way that these principles assign probabilities grounds
on a certain ensemble of repeated trials: the equal probability assigned to
coin flipping is mostly grounded on the observation on thousands of repeated
trials; and the LLN grounds on averaging repeated trials (with, or without the
i.i.d. assumption). However, how can we assign probabilities to the events that
are not as simple as coin flipping, and is not the average of ensemble, like
the one of weather prediction?</p>

<div id="cloud">
<figure>
    <img src="/images/cirrus6_small.jpg" height="200" alt="Cirrus" />
    <img src="/images/cumulonimbus3_small.jpg" height="200" alt="Cumulonimbus" />
     <figcaption>On the left is Cirrus, the type of cloud that normally
wouldn't rain; on the right is the Cumulonimbus, the type of cloud that
normally would lead to
thunderstorms.</figcaption>
</figure>
</div>

<p>Given daily life experience, Cirrus (c.f. left picture) normally
would not rain, but Cumulonimbus (c.f. right picture) would lead
to thunderstorms. The prediction from daily experience comes from
repeatedly observing the cloud shapes and the weather afterwards. The
physics behind the cloud formation and raining is a complex interaction
in the complex system of moisture, dust, gravity, temperature and wind,
of which the outcome is highly chaotic thus uncertain. The past
experience allows us to divide the cloud shapes into different groups by
their salient features that would have a high, or low chance to induce
different types of weather. THUS, in summary, we have a mechanism to
approximate the probability of outcomes of a complex physical system by
observing certain features of the system. That is, to estimate the
plausibility of the raining event based on the events that a certain
group/type of cloud shape occurs.</p>

<p>The theory is a theory of the above mechanism. S-System is the mechanism
to construct feature events, e.g., the shapes of the cloud, that are
divided into groups/types and mirror the hierarchical relationship in
the physical complex system in a self-organized way, e.g., the
interaction between water moleculars and dust particles. The geometry
part characterizes the information geometry structure in the feature
space. The relationship between it and S-System is like the one between
Hilbert Space and functions: it provides a manifold structure. The
learning framework characterizes how the past experience helps learn the
grouping of events: the objective function that divides events into
different cloud types/groups, and discovers feature events in a
self-organized way. The optimization part investigates how such a
grouping is implementable given objective functions in the learning
framework through optimization. They overall make up a theory of NNs
(and S-System): the mechanism to approximate/assign the probability
of/to outcomes of events to estimate the plausibility of events, e.g.,
the occurrence of rain or not.</p>

<p>Keep reading at <a href="https://arxiv.org/abs/1811.12783">here</a> if the following
embedded pdf does not work. If one finds the manuscript rather hard to read, an
option is to skip to the <a href="#the-optimization-power-of-neural-networks">next
section</a> in this page. It
introduces the optimization part of it, which I have written with relevant
introductions. I planned to write introductions to the rest of the theory,
however, the progress is measured in months. Very likely, I need to wait for
the publication of the optimization part, before I have the time to work on the
rest.</p>

<embed src="/images/nntheory.pdf" type="application/pdf" width="100%" height="600px" />

<h2 id="the-optimization-power-of-neural-networks">The Optimization Power of Neural Networks</h2>

<p>The hierarchical organization of NNs to recognize and represent the
hierarchical physical world has made itself a complex system. Contrary to
existing shallow models, or simple models, it is exactly the complexity built
in hierarchy that makes NNs the most powerful inference model. In a complex
system, when the collective behavior of events is stable, in the sense that it
emerges on the overall interaction of lower scale events, yet does not depends
on any small subsets of them, it is termed an emergent behavior. For instance,
the magnetic force of a magnet is a macroscopic phenomenon, yet it may emerge
from the alignment of spin magnetic moment of many elementary particles. A
misalignment/disorder in a fraction of the particles that does not exceed a
critical points to cause phase transition wonâ€™t destroy the magnetic field of
the magnet, only weakening it. More examples and more thorough and in depth
discussion about emergence could be found in Simon (1962), Nicolis and Nicolis
(2012) and Kadanoff (2000). Contrary to the high constrained interaction in the
spinning particles, the neuron population in a NN is highly flexible and
adaptable. A large collection of cooperative yet autonomous neurons gives NNs
the ability to partition events into arbitrary groups and infer the
plausibility of any groups of events, which is the emergent behavior emerging
from the disorder in the NN complex system. More technically, formulated as an
optimization problem to minimize the error between the empirical probability
distribution and its parametric representation, though being non-convex, NNs
can reach zero error as long as NNs maintain its diversity while increasing its
neuron population.</p>

<p>The cooperative interaction among neurons are formalized as a set of
boundedness and diversity assumptions, explained in the illustrations below.</p>

<figure style="float: right;
         color: white;
         background-color:  rgba(30, 255, 3, 0.50);
         border: #1eff03 solid;" class="illustration white-bg">
  <img src="/images/Hessian_illustration.svg" alt="Cumulonimbus" />
    <figcaption style="text-align: left;">Illustration of the conditions
      characterized by assumptions in the manuscript in the NN
      context. Informally, it characterizes the strength of the activation path
      drawn in colors that connects layer p âˆ’ 1 and layer q, and the
      correlation between different paths. For each path, in a 10-layer NN with
      100 neurons in each layer, it can correlate with $10^4$ other paths among
      all $10^8$ paths in the NN. The activation paths are the entries of the
      Hessian of a NN.</figcaption>
</figure>

<p>Each activation path probably codes certain hierarchical pattern, e.g, a
pattern of a person. It is reasonable to suggest that the capacity is quite
significant. It is interesting to formulate testable quantities to see how much
correlation exists in real world NNs. It also excitingly connects to the
sparsity of neuron activation, and the long held hypothesis that information is
coded in the neuron connection patterns in biological NNs.</p>

<p>Back to a larger context, NN is a fabulous mechanism that can indefinitely
increase the number of parameters, thus its learning capacity, in a meaningful
way, i.e., creating higher level features yet maintaining the diversity of the
features created. Such mechanism does not normally hold in other systems or
algorithms. Taking linear NNs for example, though with the potential to
infinitely increase its parameters, matrices that multiply together still have
a high correlation structure within, thus cannot create a population of
diverse neurons that are of low correlation with a majority of the other
neurons. <strong>Accompanying the theorem proved in the manuscript, which states
empirical risk $R_m(T)$ can be optimized to zero, we can see that the
 assumptions actually characterize sufficient preconditions to the optimization
 power of NNs.</strong></p>

<p>To learn more, keep reading the manuscript embedded below. If the browser does
not support embedded pdf, one can read it <a href="/images/optnn.pdf">here</a>.</p>

<embed src="/images/optnn.pdf" type="application/pdf" width="100%" height="600px" />


</div>

    </section>

    

    <nav class="navbar navbar-default navbar-fixed-bottom">
<div class="container footer-content">
    <!-- Nothing is there. -->
</div>
</nav>
  </body>

</html>
