<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta content="Shawn" property="og:site_name">
  <link href="/favicon.png" rel="icon">

  
  <meta content="NN Theory Introduction" property="og:title">
  

  
  <meta content="article" property="og:type">
  

  
  <meta content="<h1>Children of The Sun</h1>" property="og:description">
  

  
  <meta content="http://shawnLeeZX.github.io/theory_intro" property="og:url">
  

  

  <meta property="og:image" content="">

  

  

  <title>NN Theory Introduction - Shawn</title>
  <meta name="description" content="<h1>Children of The Sun</h1>">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.2.2/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="/css/main.css">
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>

  <!-- Typesetting math using MathJax -->
  <!-- mathjax config similar to math.stackexchange -->
  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     jax: ["input/TeX", "output/HTML-CSS"],
     tex2jax: {
       inlineMath: [ ['$', '$'] ],
       displayMath: [ ['$$', '$$']],
       processEscapes: true,
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     },
     messageStyle: "none",
     "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
   });
  </script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-44220731-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>

  <link href='https://fonts.googleapis.com/css?family=PT+Sans' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Fira+Mono' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Source+Code+Pro:400,200,300,500,600,700,900' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Gentium+Basic:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Alegreya:400,400italic,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Lora:400,400italic,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Fira+Sans:400,300,500,700' rel='stylesheet' type='text/css'>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.2.0/jquery.min.js"></script>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.1/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-hQpvDQiCJaD2H465dQfA717v7lu5qHWtDbWNPvaTJ0ID5xnPUlVXnKzq7b8YUkbN" crossorigin="anonymous">
  <link rel="canonical" href="http://shawnLeeZX.github.io/theory_intro">
  <link rel="alternate" type="application/rss+xml" title="Shawn" href="http://shawnLeeZX.github.io/feed.xml">
</head>

  <body>
    <section>
<nav class="navbar navbar-default navbar-fixed-top">

  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Shawn</a>
    </div>


    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav navbar-right">
        <li><a href="/research_catalog">Research</a></li>
        <li><a href="/images/cv.pdf">Resume</a></li>
        <li><a href="/about">About</a></li>
        <li><a href="/blog">Blog</a></li>
        <li><a href="/blog/archives">Archive</a></li>
      </ul>

    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>
</section>

    <section>
      <div class="jumbotron">
  <div class="container">
    <h1 class="page-title" itemprop="name headline">NN Theory Introduction</h1>
    
  </div>
</div>

<div class="container">
  <ul id="markdown-toc">
  <li><a href="#theory-of-dnns" id="markdown-toc-theory-of-dnns">Theory of DNNs</a>    <ul>
      <li><a href="#the-optimization-power-of-neural-networks" id="markdown-toc-the-optimization-power-of-neural-networks">The Optimization Power of Neural Networks</a></li>
    </ul>
  </li>
</ul>

<h2 id="theory-of-dnns">Theory of DNNs</h2>

<p>In the past six years, I have worked out a theory of NNs. A short introduction
to theory is present in this section — it is rather technical.  The theory is
divided into the following <em>four</em> parts:</p>

<ul>
  <li>
    <p><strong>S-System</strong>, a formal measure-theoretical framework that builds a
hierarchical hypothesis space, of which NNs are special cases. It is
motivated by the fact that nature is a complex system built
hierarchically, and a mechanism is needed for any agents living in
it to recognize and predict hierarchical events happening.</p>
  </li>
  <li>
    <p>The <strong>geometry</strong> of S-System. The objects in the hypothesis space
are probability measures, thus have an information-geometry
structure. It characterizes the phenomenon that NNs compose and
recompose manifolds that have increasingly high level semantic
meaning.</p>
  </li>
  <li>
    <p>The <strong>learning framework</strong> of S-System. It describes the objective
functions to identify an element of the hypothesis space by learning
the parameters. It gives principled derivation of back propagation,
and unifies supervised learning and unsupervised learning in NNs.</p>
  </li>
  <li>
    <p>The <strong>optimization</strong> landscape of S-System. It identifies principles
and conditions that make the non-convex optimization of the learning
problem of S-System benign; that is, all local minima are global
minima.</p>
  </li>
</ul>

<p>The theory is a principle to assign probabilities to events, e.g., predicting the
event that today would be a rainy or sunny day, given past experience. To
recall what a principle to assign probabilities is, we recall the principle of
symmetry, and the law of large number (LLN). The example of the principle of
symmetry is ubiquitous in elementary probability theory, i.e., the equal
probability assigned to the event that front, or back side is obtained when
flipping a coin. So are the law of large number, which assigns normal
probability distribution to that of the mean of a large number of random
variables.  However, the way that these principles assign probabilities grounds
on a certain ensemble of repeated trials: the equal probability assigned to
coin flipping is mostly grounded on the observation on thousands of repeated
trials; and the LLN grounds on averaging repeated trials (with, or without the
i.i.d. assumption). However, how can we assign probabilities to the events that
are not as simple as coin flipping, and is not the average of ensemble, like
the one of weather prediction?</p>

<div id="cloud">
<figure>
    <img src="/images/cirrus6_small.jpg" height="200" alt="Cirrus" />
    <img src="/images/cumulonimbus3_small.jpg" height="200" alt="Cumulonimbus" />
     <figcaption>On the left is Cirrus, the type of cloud that normally
wouldn't rain; on the right is the Cumulonimbus, the type of cloud that
normally would lead to
thunderstorms.</figcaption>
</figure>
</div>

<p>Given daily life experience, Cirrus (c.f. left picture) normally
would not rain, but Cumulonimbus (c.f. right picture) would lead
to thunderstorms. The prediction from daily experience comes from
repeatedly observing the cloud shapes and the weather afterwards. The
physics behind the cloud formation and raining is a complex interaction
in the complex system of moisture, dust, gravity, temperature and wind,
of which the outcome is highly chaotic thus uncertain. The past
experience allows us to divide the cloud shapes into different groups by
their salient features that would have a high, or low chance to induce
different types of weather. THUS, in summary, we have a mechanism to
approximate the probability of outcomes of a complex physical system by
observing certain features of the system. That is, to estimate the
plausibility of the raining event based on the events that a certain
group/type of cloud shape occurs.</p>

<p>The theory is a theory of the above mechanism. S-System is the mechanism
to construct feature events, e.g., the shapes of the cloud, that are
divided into groups/types and mirror the hierarchical relationship in
the physical complex system in a self-organized way, e.g., the
interaction between water moleculars and dust particles. The geometry
part characterizes the information geometry structure in the feature
space. The relationship between it and S-System is like the one between
Hilbert Space and functions: it provides a manifold structure. The
learning framework characterizes how the past experience helps learn the
grouping of events: the objective function that divides events into
different cloud types/groups, and discovers feature events in a
self-organized way. The optimization part investigates how such a
grouping is implementable given objective functions in the learning
framework through optimization. They overall make up a theory of NNs
(and S-System): the mechanism to approximate/assign the probability
of/to outcomes of events to estimate the plausibility of events, e.g.,
the occurrence of rain or not.</p>

<p>Keep reading at <a href="https://arxiv.org/abs/1811.12783">here</a>.</p>

<embed src="/images/optnn.pdf" type="application/pdf" width="100%" height="600px" />

<h3 id="the-optimization-power-of-neural-networks">The Optimization Power of Neural Networks</h3>

<p>The hierarchical organization of NNs to recognize and represent the
hierarchical physical world has made itself a complex system. Contrary to
existing shallow models, or simple models, it is exactly the complexity built
in hierarchy that makes NNs the most powerful inference model. In a complex
system, when the collective behavior of events is stable, in the sense that it
emerges on the overall interaction of lower scale events, yet does not depends
on any small subsets of them, it is termed an emergent behavior. For instance,
the magnetic force of a magnet is a macroscopic phenomenon, yet it may emerge
from the alignment of spin magnetic moment of many elementary particles. A
misalignment/disorder in a fraction of the particles that does not exceed a
critical points to cause phase transition won’t destroy the magnetic field of
the magnet, only weakening it. More examples and more thorough and in depth
discussion about emergence could be found in Simon (1962) Nicolis and Nicolis
(2012) Kadanoff (2000). Contrary to the high constrained interaction in the
spinning particles, the neuron population in a NN is highly flexible and
adaptable. A large collection of cooperative yet autonomous neurons, formalized
as assumptions 6.1 6.2, gives NNs the ability to partition events into
arbitrary groups and infer the plausibility of any groups of events (proved in
theorem 6.1), which is the emergent behavior emerging from the disorder in the
NN complex system. More technically, formulated as an optimization problem to
minimize the error between the empirical probability distribution and its
parametric representation, though being non-convex, NNs can reach zero error as
long as NNs maintain its diversity while increasing its neuron population.</p>

<p>The interaction among neurons are formalized as a set of boundedness and
diversity assumptions, explained in the illustrations below.</p>

<embed src="/images/hessian.pdf" type="application/pdf" width="100%" height="600px" />

<div class="white-bg floated">
<figure class="illustration">
    <img src="/images/Hessian_illustration.svg" height="600" alt="Cumulonimbus" />
     <figcaption>On the left is Cirrus, the type of cloud that normally
wouldn't rain; on the right is the Cumulonimbus, the type of cloud that
normally would lead to thunderstorms.</figcaption>
</figure>
</div>

<p>Each activation path
probably codes certain hierarchical pattern, e.g, a pattern of a person. It is
reasonable to suggest that the capacity is quite significant. It is interesting
to formulate testable quantities to see how much correlation exists in real
world NNs. It also excitingly connects to the sparsity of neuron activation,
and the long held hypothesis that information is coded in the neuron connection
patterns in biological NNs.</p>

<p>Back to a larger context, NN is a fabulous mechanism that can indefinitely
increase the number of parameters, thus its learning capacity, in a meaningful
way, i.e., creating higher level features yet maintaining the diversity of the
features created. Such mechanism does not normally hold in other systems or
algorithms. Taking linear NNs for example, though with the potential to
infinitely increase its parameters, matrices that multiply together still have
a high correlation structure within, thus cannot create a population of
diverse neurons that are of low correlation with a majority of the other
neurons. {\bf Accompanying \cref{thm:landscape}, which states $R_m(T)$ can be
optimized to zero, we can see that assumptions \ref{a:boundedness}
\ref{a:diversity} actually characterizes sufficient preconditions to the
optimization power of NNs.}</p>

</div>

    </section>

    

    <!-- <nav class="navbar navbar-default navbar-fixed-bottom"> -->
<!-- <div class="container footer-content"> -->
    <!-- Nothing is there. -->
<!-- </div> -->
<!-- </nav> -->
  </body>

</html>
