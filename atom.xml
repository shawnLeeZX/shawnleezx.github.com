<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[SHAWN LEE]]></title>
  <link href="http://shawnLeeZX.github.io/atom.xml" rel="self"/>
  <link href="http://shawnLeeZX.github.io/"/>
  <updated>2016-08-30T12:51:12+08:00</updated>
  <id>http://shawnLeeZX.github.io/</id>
  <author>
    <name><![CDATA[Shawn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Key points of Understanding Deep Conv Net]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2016/08/29/key-points-of-understanding-deep-conv-net/"/>
    <updated>2016-08-29T10:41:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2016/08/29/key-points-of-understanding-deep-conv-net</id>
    <content type="html"><![CDATA[<p>This note tries to summarize the keypoints of the paper by Mallat:
<a href="http://www.ncbi.nlm.nih.gov/pubmed/26953183">Understanding deep convolutional networks</a>.</p>

<p>Key concepts: contraction, linearization, fibre, parallel transport,
multi-scale support vector</p>

<!-- more -->

<p>Mechanically, Convolutional Neural Network could be formalized using cascading
the following of operators that compose a function $f$, whose value is in $R$
for regression problems, and is the index of the sample’s class for
classification problems.</p>

<script type="math/tex; mode=display">
x_j = \rho W_j x_{j-1}
</script>

<p>where $j$ indicates the depth of a network.</p>

<p>Essentially, neural network is a way to combat the curse of dimensionality. In
the paper, Mallat formalizes this process using linearization of hierarchical
symmetry, space contraction and sparse separation. In short, it is done by
defining a new variable $\Phi(x)$, where $\Phi$ is a <em>contractive</em> operator
that reduces the range of variations of $x$, while still <em>separating</em> different
values of $f: \Phi(x) \not= \Phi(x’)$ if $f(x) \not= f(x’)$. $\Phi$ should have
the properties to <strong>linearize hierarchical symmetries, does space contraction and
achieve sparse separation</strong> to keep classification margin (value margin for
regression). $\Phi$ is called the new representation of $x$.</p>

<h2 id="linearization-of-symmetries">Linearization of symmetries</h2>

<p>Neural network finds equivalence classes $\Phi(x)$ that linearize
$f$. $\Phi(x)$ is built by collapsing complex symmetry groups.</p>

<h3 id="linearization">Linearization</h3>

<p>By definition, linearization means to find a representation $\Phi$ that $f$ on
$x \in \Omega$, where the domain $\Omega$ is a high dimensional open set, which
could be $L^2(R^n)$ or $R^d$, could be approximated by a linear combination</p>

<p><script type="math/tex">\bar{f} = \sum_i\Phi_i(x)</script>.</p>

<p>The idea of linearization is to find important yet highly complex basis vector
that linearly contributes to $f$, so $f$ is a linear projection of
<script type="math/tex">\Phi_i(x)</script>. An example would be the word vector, like <code>actor - man + woman =
actress</code>. Similar phenomenon is observed in images as well.</p>

<p>On the other hand, it also means $\Phi(x)$ absorbs in the variability that is
not related to $f$. An example could be translation variability, which is
absorbed by subsampling in NN.</p>

<p>It is difficulty for me to understand what does Mallat mean by</p>

<p>&#8220;We can then optimize a low-dimensional linear projection along directions where
$f$ is constant.’’</p>

<p>The above is the meaning I have guessed. In summary, if $\Phi$ is fixed, the
optimization works on the linear projection’s weight. &#8220;$f$ is constant’’ means
after the change of variable, the variability in the original space $\Omega$
has been absorbed, so a variability in the direction that does not change
$\Phi$ does not change $f$.</p>

<h3 id="space-collapse">Space collapse</h3>

<p>The key is to find $\Phi$ that linearizes $f$, which is to find $\Phi$ that
collapses the directions that $f$ remains constant. Mallat formalizes this
&#8220;collapse’’ in term of group theory, or more particularly the group of
transformations.</p>

<p>The word collapse is my own creation, which tries to summarize the key point of
building invariant representation $\Phi$ utilizing groups of
symmetry. It corresponds to the absorbing of variability in previous
section. <script type="math/tex">\Phi_i(x)</script> represents an equivalence class (could be the orbit of
the group, more on this later) of $x$. Collapse is the process of combining
those classes together to form an equivalence class.</p>

<p>Formally, if $f$ is invariant to an operator $g$ that preserves the value of
$f$, which is to say $f(gx) = f(x)$, we can safely incorporate $g$ in $\Phi(x)$
(putting $gx$ in an equivalence class), so any transformation $g$ on $x$, aka
$gx$, does not change $\Phi(x)$, consequently it does not change $f$. Acting as
an equivalence class, they contribute to the linearization of $f$.</p>

<p>An example could be the translation group as an example, $f(gx) = f(x - u)$,
where $u \in \Omega$, representing the displacement of the translation. $f$’s
value does not change by translation, so by averaging (subsampling) spatially,
a direction that $f$ remains constant is collapsed out. This direction is part
of a equivalence class formed by collapsing out the translation group.</p>

<p>By pinpointing those equivalence classes $\Phi(x)$, linearization is achieved
by doing linearly projection on them.</p>

<h2 id="more-details">More details</h2>

<p>The high level plan stops here. Those above two are the main components making
up the framework of NN. Now we fill in the details, aka sparse separation,
space contraction, separation of scales and complex interaction among
multi-scale.</p>

<h3 id="the-functional-form-of-phi">The functional form of $\Phi$</h3>

<p>$\Phi$ is handcrafted basis when the time of dictionary learning or deep
learning has not come. It is a vector that captures important (in any criteria
matters for the problem at hand) features.</p>

<h3 id="contraction-for-sparsity">Contraction for sparsity</h3>

<p>There are some general criteria for basis $\Phi$, one of which may require them
be sparse. A sparse signal brings to better linear separation. Or in other
word, factors of variation are better disentangled.</p>

<p>For fixed basis vectors, such as wavelets in scattering network they are
defined to separate scale and frequency. Thus for signals that only contains a
few of those, the wavelet coefficients would be rather sparse.</p>

<p>For an adaptive basis vector, to promote sparsity, Mallat puts forward the
concept of space contraction, which is achieved through the non-linear
activation function, a modulus or a ReLU. It explicitly contract the volume of
the space to achieve a sparse representation.</p>

<p>In the case of fixed basis like wavelet, if the representation is already
sparse, non-linear activation won’t reduce the volume too much since most of
the dimensions are already zero.</p>

<h3 id="diffeomorphism-calls-for-multi-scale-separation">Diffeomorphism calls for multi-scale separation</h3>

<p>Diffeomorphism can also be modeled a group symmetry. Diffeomorphism could be
factorized as a translation and a scaling effects. So to linearize
diffeomorphism, a multi-scale basis to separate the scaling effects are needed.</p>

<h3 id="multi-scale-interaction">Multi-scale interaction</h3>

<p>It is not enough to only separate signal using basis vectors, but also to model
their interactions. This justifies a deep architecture.</p>

<h2 id="combining-all-the-above-together">Combining all the above together</h2>

<p>Now we could map each symbol in the following to their functional roles.</p>

<script type="math/tex; mode=display">
x_j = \rho W_j x_{j-1}
</script>

<p>Each row of $W$ is the basis vector; $\rho$ is the contraction operator to
promote sparsity; <script type="math/tex">x_j = ... x_{j-1}</script> is the recursive form that takes cares
multi-scale separation and interaction.</p>

<p>At the same time, <script type="math/tex">W_{j-1}x_{j-1}</script> linearizes <script type="math/tex">W_j x_j</script>. If we adding
pooling in, it corresponds to collapsing the direction where $f$ remains
constant.</p>

<h2 id="recast-in-term-of-differential-geometry">Recast in term of differential geometry</h2>

<p>Mallat use the formalism in differential geometry to succinctly define the
hierarchically built $\Phi$.</p>

<p>According to Mallat, the power of Neural Network is it is able learn such a
representation that linearize hierarchical symmetry groups, which is formalized
using fibres of hierarchical symmetry groups. Nonlinear contraction reduces the
variability of each fibre, so removing the variability that is not directly
related to currently sample being processed. Fibres of current layer are
transported to fibres of next layers, which are fibres of large
groups. Cascadingly applying the parallel transport on a sample, we obtain
multi-scale support vectors that are sparse.</p>

<p>However, it is hard to map these math names to the operational name of conv
net. Here is my best guess up to now.</p>

<p>Each channel of a conv filter is a part of the orbits.
The equivalence class is the cross-channel sum of orbits of filters. The index
$P_j$ is the indices of orbits, which could be understood as the spatial
indices (orbits of translation group) and channel indices (orbits of those
groups, such as rotation groups). Since we have done a summation, how each 2D
filter corresponds to each part of the orbit does not matter, thus we get an
equivalence class.</p>

<p>Each equivalence class is called a fibre, which is just the fancy in
differential geometry to call an operator. In this case, the fibre is just the
cascadingly built operator $ρW_j$ mentioned at the beginning section. $ρW_j$
computes an approximate mapping from fibre of layer $j-1$ to $j$, which is
called a parallel transport in $P_j$.
A parallel transport is defined by a group $G_j$ of symmetries acting on the
index set $P_j$ of a layer $x_j$.
It is called an approximate mapping because fibre is supposed to be an
equivalence class of orbits of $x_{j-1}$, and the transport be mapping from
those equivalence class. however $pW_j$ starts from $x_{j-1}$, not
equivalence classes of orbits of $x_{j-1}$. So it is assumed to be approximated
by applying $g$ to $x_j$, we get $\bar{g}$ on $x_{j-1}$. Again implicitly
$x_{j}$ will be unrolled to the equivalence class of orbit of this layer,
which is a larger groups which is semidirect products of groups of layer $j-1$,
and groups (corresponding to channels in layer $j$) of layer $j$. The
approximation only makes sense by corresponding each channel of a filter to be
a part of a orbit of $g$, as in the previous paragraph. The channel of filters
are the groups of layer $j-1$, while the number of filters (output channel
number in most convnet implementations) is the groups of the layer
$j$. Assuming each filter channel corresponds to a part of the
orbit, $g.x_{j} = g.[\rho W_j x_{j-1}] \approx \rho W_j[\bar{g}.x_{j-1}]$
in the paper makes sense, since the summation cross channel would absorb $g$ in
$W_{j}$.</p>

<p>So the convolution is called convolutions along the fibres. Implicitly, $\rho
W_j$ expands $x_{j-1}$ to its orbits, then computes equivalence classes of
orbits of $x_{j}$ by cross-channelly adding the result of applying filters on
those orbits. If in this sense, $x_{j-1}$ is indeed an equivalence class of
orbits. I guess the approximation means that this is just an assumption, since
no constrains are enforced to make sure channels of a filter are orbits of
$x_{j-1}$.</p>

<p>In this model, network filters are guiding nonlinear contractions, to reduce the
data variability in directions of local symmetries. The classification margin
can be controlled by sparse separations along network fibres</p>

<p>Each fibre is complex basis vector. Sparsity ensures separateness. Mallat
introduces the concept of multi-scale support vector, explained in the
following. To avoid further contracting their distance, they can be separated
along different fibres indexed by $b$. The separation is achieved by filters
$w_{j,h.b}$, which transform $x_{j−1}$ and $x’_{j−1}$ into $x_{j}(g, h,
b)$and $x’_{j}(g, h, b)$ having sparse supports on different fibres $b$. The
next contraction $ρW_{j+1}$ reduces distances along fibres indexed by $(g, h)
\in G_j$,but not across $b \in B_j$,which preserves distances.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Repost] Explosion of the Space Shuttle Challenger Address to the Nation]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2016/07/30/repost-explosion-of-the-space-shuttle-challenger-address-to-the-nation/"/>
    <updated>2016-07-30T08:42:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2016/07/30/repost-explosion-of-the-space-shuttle-challenger-address-to-the-nation</id>
    <content type="html"><![CDATA[<p>Accidentially heared the <a href="https://youtu.be/SBOVkptjJhE?t=3959">speech</a> given by
Reagan to the nation after Challenger crashed.</p>

<p>Below is an excerpt. Full transcript is after that. The
<a href="http://history.nasa.gov/reagan12886.html">transcript</a> is from NASA website.</p>

<h2 id="excerpts">Excerpts</h2>

<p>We’ve grown used to wonders in this century. It’s hard to dazzle us. But for 25
years the United States space program has been doing just that. We’ve grown
used to the idea of space, and perhaps we forget that we’ve only just
begun. We’re still pioneers. They, the members of the Challenger crew, were
pioneers.</p>

<p>I know it is hard to understand, but sometimes painful things like this
happen. It’s all part of the process of exploration and discovery. It’s all
part of taking a chance and expanding man’s horizons. <strong>The future doesn’t belong
to the fainthearted; it belongs to the brave</strong>. The Challenger crew was pulling
us into the future, and we’ll continue to follow them.</p>

<p>There’s a coincidence today. On this day 390 years ago, the great explorer Sir
Francis Drake died aboard ship off the coast of Panama. <strong>In his lifetime the
great frontiers were the oceans, and an historian later said, “He lived by the
sea, died on it, and was buried in it.”</strong> Well, today we can say of the
Challenger crew: Their dedication was, like Drake’s, complete.</p>

<p>The crew of the space shuttle Challenger honored us by the manner in which they
lived their lives. <strong>We will never forget them, nor the last time we saw them,
this morning, as they prepared for their journey and waved goodbye and “slipped
the surly bonds of earth” to “touch the face of God.”</strong></p>

<h2 id="full-speech">Full Speech</h2>

<!-- more -->

<p>Ladies and gentlemen, I’d planned to speak to you tonight to report on the
state of the Union, but the events of earlier today have led me to change those
plans. Today is a day for mourning and remembering.</p>

<p>Nancy and I are pained to the core by the tragedy of the shuttle Challenger. We
know we share this pain with all of the people of our country. This is truly a
national loss.</p>

<p>Nineteen years ago, almost to the day, we lost three astronauts in a terrible
accident on the ground. But we’ve never lost an astronaut in flight; we’ve
never had a tragedy like this. And perhaps we’ve forgotten the courage it took
for the crew of the shuttle; but they, the Challenger Seven, were aware of the
dangers, but overcame them and did their jobs brilliantly. We mourn seven
heroes: Michael Smith, Dick Scobee, Judith Resnik, Ronald McNair, Ellison
Onizuka, Gregory Jarvis, and Christa McAuliffe. We mourn their loss as a nation
together.</p>

<p>For the families of the seven, we cannot bear, as you do, the full impact of
this tragedy. But we feel the loss, and we’re thinking about you so very
much. Your loved ones were daring and brave, and they had that special grace,
that special spirit that says, “Give me a challenge and I’ll meet it with joy.”
They had a hunger to explore the universe and discover its truths. They wished
to serve, and they did. They served all of us.</p>

<p>We’ve grown used to wonders in this century. It’s hard to dazzle us. But for 25
years the United States space program has been doing just that. We’ve grown
used to the idea of space, and perhaps we forget that we’ve only just
begun. We’re still pioneers. They, the members of the Challenger crew, were
pioneers.</p>

<p>And I want to say something to the schoolchildren of America who were watching
the live coverage of the shuttle’s takeoff. I know it is hard to understand,
but sometimes painful things like this happen. It’s all part of the process of
exploration and discovery. It’s all part of taking a chance and expanding man’s
horizons. The future doesn’t belong to the fainthearted; it belongs to the
brave. The Challenger crew was pulling us into the future, and we’ll continue
to follow them.</p>

<p>I’ve always had great faith in and respect for our space program, and what
happened today does nothing to diminish it. We don’t hide our space program. We
don’t keep secrets and cover things up. We do it all up front and in
public. That’s the way freedom is, and we wouldn’t change it for a minute.</p>

<p>We’ll continue our quest in space. There will be more shuttle flights and more
shuttle crews and, yes, more volunteers, more civilians, more teachers in
space. Nothing ends here; our hopes and our journeys continue.</p>

<p>I want to add that I wish I could talk to every man and woman who works for
NASA or who worked on this mission and tell them: “Your dedication and
professionalism have moved and impressed us for decades. And we know of your
anguish. We share it.”</p>

<p>There’s a coincidence today. On this day 390 years ago, the great explorer Sir
Francis Drake died aboard ship off the coast of Panama. In his lifetime the
great frontiers were the oceans, and an historian later said, “He lived by the
sea, died on it, and was buried in it.” Well, today we can say of the
Challenger crew: Their dedication was, like Drake’s, complete.</p>

<p>The crew of the space shuttle Challenger honored us by the manner in which they
lived their lives. We will never forget them, nor the last time we saw them,
this morning, as they prepared for their journey and waved goodbye and “slipped
the surly bonds of earth” to “touch the face of God.”</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Note On Representation of Operator]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2016/05/01/note-on-representation-of-operator/"/>
    <updated>2016-05-01T17:02:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2016/05/01/note-on-representation-of-operator</id>
    <content type="html"><![CDATA[<p>A note to remind me of some intuition ruminated. Do not have the time to write
down detailed proof.</p>

<p>It is about spectral theorem of matrix, adjoint operator, SVD and tensor.</p>

<!-- more -->

<h2 id="spectral-theorem">Spectral Theorem</h2>

<p>The representation theory of linear functional is essentially one dimension
version of group representation theory. The $w$ in the inner product $\langle
x, w \rangle$ representation of functional $f(x)$ is essentially the normal of
the hyperplane, which is the functional’s representation in the dual space. The
process of applying $f$ on $x$ is a process of projection.</p>

<p>For a general operator $T: X \rightarrow Y$, whose representation is denoted
$A$, if the bases of the $X, Y$ are orthogonal, the adjoint operator $T^{*}$ of
$T$’s representation is $A^{T}$(on the condition that bases of $X, Y$ is chosen
to be orthogonal). The process of applying $T$ on $x$ is a process of first
doing multiple projection, then sum those projection together. The fact that
adjoint operator has the spectral theorem is just because in certain bases,
when we are applying the multiple projection, we are not projecting at all, but
just scale along the hyperplane. The process of changing bases, applying
scaling, then changing bases back written in form of matrix multiplication is
$M^{-1}\Lambda M$, which is the spectral decomposition of representation of $T$.</p>

<h2 id="adjoint-operator-is-just-the-transpose">Adjoint Operator is Just the Transpose</h2>

<p>The projection perspective also explains why $T^{*} = A^{T}$.</p>

<p>Let $Ax = y$. Each $x_i$ contributes to $y_i$ linearly. So just like a linear
equation has a unique solution as long as $A$ is of full rank, there must be a
one-to-one relationship between $T$ and $T^{*}$, which is formalized using
representation theory of functional again.</p>

<h2 id="svd">SVD</h2>

<p>SVD just factorizes the multi-projection into a rotation matrix and a scaling
matrix.</p>

<h2 id="tensor">Tensor</h2>

<p>Tensor is a generalized operator that handles multi-linearity.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Quick Survey on Distributed Log Processing: Unison and Logstash]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2016/03/15/a-quick-survey-on-log-processing/"/>
    <updated>2016-03-15T10:53:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2016/03/15/a-quick-survey-on-log-processing</id>
    <content type="html"><![CDATA[<p>Having multiple machines to train neural networks, I was trying to think how
could I automatically gather all the training logs, so I could analyze them in
one place.</p>

<p>I did a survey on technologies available. The ones that I found are: <code>Logstash</code>
and <code>unison</code>. <code>Logstash</code> seems to be the optimal solution, though may be a little
an overkill. However, I settled to use <code>unison</code> in the end. The rationale
behind this is I have to get around many VPNs. The VPNs I used, or maybe most
VPNs setups, won’t let the machines in LAN of the VPN network ping back to the
machine that uses VPN to connect. It may just not come from a security reason,
though it could be; it may just because it is not necessary to set up the route
table to the IP addresses allocated to the connecting machine. So in this case,
the log cannot be sent back to the connecting machine, as the way <code>Logstash</code>
does. A solution is needed to initialize the centralization from the connect
machine site. So <code>Logstash</code> is ruled out, and I settled with <code>unison</code>.</p>

<p>The following is a quick note on <code>unison</code> works, which took me a while to
figure out. For an practical introduction to <code>Logstash</code>, see the
<a href="http://www.slashroot.in/logstash-tutorial-linux-central-logging-server">tutorial</a>
here.</p>

<!-- more -->

<h2 id="introductory-note">Introductory Note</h2>

<p><a href="http://www.cis.upenn.edu/~bcpierce/unison/download/releases/stable/unison-manual.html#basics">Unison</a>
is a file-synchronization tool for Unix and Windows. It allows two replicas of
a collection of files and directories to be stored on different hosts (or
different disks on the same host), modified separately, and then brought up to
date by propagating the changes in each replica to the other.</p>

<p><code>Unison</code> supports transmission via <code>ssh</code>, <code>socket</code>, <code>file</code> and <code>rsh</code>. Each of
those protocols has its own setups, except for the <code>file</code> protocol — it
synchronizes files within the single machine. For instance, to use ssh, the two
machines to be able to ssh to each other.</p>

<p>The way <code>unison</code> is used is similar with <code>scp</code> or <code>cp</code> in the sense that they
are all a command line source destination style. An example would be</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">unison -testserver /tmp/ ssh://192.168.72.1//tmp
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The format of a source or destination is
<code>[protocol:]//[user@][host][:port][/path]</code>. The one with no host name is
supposed to your local machine. The <code>-testserver</code> option above is used to test
whether the network between the local machine and the remote machine is
reachable.</p>

<p>To avoid typing the addresses each time, since <code>unison</code> is about syncing, not
copying, <code>unison</code> offers a thing called profile. The command above, without the
<code>-testserver</code> option, converts to a profile is</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="nv">root</span> <span class="o">=</span> /home/use1/work/sync/
</span><span class="line"><span class="nv">root</span> <span class="o">=</span> ssh://192.168.72.1//home/user1/work/sync
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>For more sophisticated profiles, see the office
<a href="http://www.cis.upenn.edu/~bcpierce/unison/download/releases/stable/unison-manual.html#profile">doc</a>.</p>

<p>The question following is where do we store profiles?</p>

<p><code>unison</code> will store all its logistics files under the folder pointed by
environment variable <code>UNISON</code>, which if not specified, is <code>.unison</code> under the
<code>$HOME</code> folder. A profile file us named in the format of <code>*.prf</code>. Suppose the
above profile is saved as <code>test.prf</code>. To use that profile, type</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">unison <span class="nb">test</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The last thing I spent some time to figure out is how it actually sync between
machines, so it won’t mess up my data. Roughly, it adds another layer of logic
above <code>rsync</code>, which is suggested from the document</p>

<pre><code>Transfers are optimised using a version of the rsync protocol, making it ideal
for slower links. Unison has a clear and precise specification, and is
resilient to failure due to its careful handling of the replicas and its
private structures.
</code></pre>

<p>The logic is added I think is the way to get some fingerprints of the files, so
it knows which files is updated when syncing, and only sync those are really updated.</p>

<pre><code>Touching a file without changing its contents should never affect whether or
not Unison does an update. (When running with the fastcheck preference set to
true—the default on Unix systems—Unison uses file modtimes for a quick first
pass to tell which files have definitely not changed; then, for each file that
might have changed, it computes a fingerprint of the file's contents and
compares it against the last-synchronized contents. Also, the -times option
allows you to synchronize file times, but it does not cause identical files to
be changed; Unison will only modify the file times.)
</code></pre>

<p>So calling unison at either end will sync the files right.</p>

<h2 id="sync-between-more-than-machines">Sync Between More than Machines</h2>

<p>The built-in doc of <code>unison</code>, which is available via <code>unison -doc tutorial</code>,
has a section discussing how <code>unison</code> could be used to sync between more than
machines.</p>

<p>Using Unison to Synchronize More Than Two Machines</p>

<pre><code>Unison is designed for synchronizing pairs of replicas. However, it is
possible to use it to keep larger groups of machines in sync by
performing multiple pairwise synchronizations.

If you need to do this, the most reliable way to set things up is to
organize the machines into a “star topology,” with one machine
designated as the “hub” and the rest as “spokes,” and with each spoke
machine synchronizing only with the hub. The big advantage of the star
topology is that it eliminates the possibility of confusing “spurious
conflicts” arising from the fact that a separate archive is maintained
by Unison for every pair of hosts that it synchronizes.
</code></pre>

<p>I did not get what does it mean in the first reading. After that, I guess the
method it suggests is to establish a pair-wise sync among machines with certain
topology, which is to say: one needs to set up multiple profiles; each profile
sync between two machines; those machines form a topology so there is a hub
that will have a centralized sync state among all machines if you sync in the
right order.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Note on Installing Ubuntu on Newer Machines]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2016/03/08/note-on-installing-ubuntu-on-newer-machines/"/>
    <updated>2016-03-08T13:48:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2016/03/08/note-on-installing-ubuntu-on-newer-machines</id>
    <content type="html"><![CDATA[<p>This is the note supposed to note something peculiar things I met during
setting up Ubuntu on a newer machine, which is to say, with UEFI replacing
BIOS, GPT replacing MBR, Display Port and HDMI replacing VGA and DVI, and with
several GPUs.</p>

<p>Due to time reason, I fell back to my old way of installing Ubuntu, and have
not figured out what is the right way to install the newer machine mentioned
above, but the note is to note down possible guess, and the things have been
figured out, so I may have a thread to follow next time.</p>

<!-- more -->

<p>At the time I entered what was called BIOS of the machine, which is called
UEFI(Unified Extensible Firmware Interface), I was amazed that it was an almost
desktop GUI. The official lab machine of mine has a similar firmware, but since
I was not supposed to have access to it, I did not play with it much. I think
my most recent laptop should have also used UEFI, but I did not have any memory
what it is like, I guess I did not need to change things so I did not enter it.</p>

<h2 id="gpu-control-from-boot-to-os">GPU Control From Boot to OS</h2>

<p>The first thing I tried to figure out is how a GPU could be installed
properly. In other words, how could a GPU inserted in a socket could be
detected by the motherboard, then OS?</p>

<p>Normally a PC motherboard has an integrated card, and an optional discrete
graphical unit could be installed GPU card through PCIE sockets. There is a
switch in the UEFI to tell the motherboard where to look for a graphical
unit. Also, even if the GPU is not powered up, I could have a hint on the
monitor to tell me to what happened. So I think a GPU has a basic function unit
that offers motherboard basic graphical function so it could be used till the
machine boots into an OS.</p>

<p>The first problem I have met is there was no output when I connect DVI adapter
of the monitor to a TITAN Black that was inserted in the latter socket of the
motherboard, and could only get output when I connect the adapter to the GPU
inserted in the first PCIE socket. The GPU is not a TITAN Black but a 760.</p>

<p>There are two guesses why this situations happen. The first is UEFI only detect
GPU at the first PCIE socket and ignores all the remaining. This is reasonable
since no one is supposed to use multiple GPUs during boot time. But it seems I
have tried put a TITAN Black at the first socket, though I cannot clearly
remember it. So another guess is my current UEFI does not support TITAN at boot
time.</p>

<p>After the UEFI transfer the control to OS, the drivers in UEFI should not
function anymore. So drivers need to be installed in the OS.</p>

<p>The open source NVIDIA card driver, Nouveau, will be installed when installing
Ubuntu. I thought only when a NVIDIA GPU is physically inserted, its open
source driver would be installed. However, it seems the open source driver will
be installed even without a card. Maybe it is because in this case, a GPU card
could be used right after installation without re-installing kernel modules.</p>

<p>Nouveau has to be removed if I want to install the newest driver from NVIDIA
through the <code>*run</code> file, otherwise they would conflict. To do this, a file
needs to be added to <code>/etc/modprobe.d/</code> with the following contents.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line"># generated by nvidia-installer
</span><span class="line">blacklist nouveau
</span><span class="line">options nouveau modeset=0</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>It is actually generated by the <code>*run</code> file. I think it tells kernel not to
load nouveau module.</p>

<p>Then regenerate your kernel</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">update-initramfs -u</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>To completely wipe out <code>nouveau</code> — though I think even without doing it, it
would work, run the following command</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo apt-get remove xserver-xorg-video-nouveau</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>To run <code>*run</code> file, one needs to log into a text console. A way I could find
that is commonly suggested on the web is to switch to one virtual terminal by
`Ctrl+Alt+F1’, then stop the X service</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo stop lightdm   or
</span><span class="line">sudo lightdm stop</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>It did not work for me. So I need to reboot into text mode by editing
grub. More concretely, when the grub menu pops up, press <code>e</code> on the item, and
find <code>splash quiet</code>, and change it to <code>text</code>, then press <code>F10</code> to boot using
current edited entry.</p>

<p>One last thing before running the <code>*run</code> file. The <code>*run</code> file will build GPU
driver to kernel, but only once. So if your OS gets upgraded, you won’t be able
to log in desktop again. To work get around this, we need to install <code>dkms</code>, so
each time the kernel gets upgraded, all modules already built will be
automatically built again into the kernel.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">sudo apt-get install dkms</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>All are settled, just run the <code>*run</code> file to install the driver.</p>

<p>To see what modules has been managed by <code>dkms</code>, use <code>dkms status</code>.</p>

<p>One last thing that tricked me up is to tell motherboard to use discrete
graphical unit after the driver has been installed. Otherwise, I could still
make it till the GUI login page, but after I have input my password, the screen
just flash a little, and ask me to log in again. I guess the problem is
since motherboard is configured to use integrated graphical unit, so the
discrete one is actually inactive, when <code>xorg.conf' tells </code>X11` to use the
discrete graphical unit, nothing is there. The solution is just to fall back.</p>

<p>I also have some guess on how <code>X11</code> make use of multiple GPUs. I have two
versions of <code>xorg.conf</code>, one from my official lab machine, and one from the one
I am setting up.</p>

<p>The official one</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
</pre></td><td class="code"><pre><code class=""><span class="line">Section "Device"
</span><span class="line">    Identifier     "Device0"
</span><span class="line">    Driver         "nvidia"
</span><span class="line">    VendorName     "NVIDIA Corporation"
</span><span class="line">    BoardName      "GeForce GTX TITAN Black"
</span><span class="line">EndSection
</span><span class="line">
</span><span class="line">Section "Device"
</span><span class="line">    Identifier     "Device1"
</span><span class="line">    Driver         "nvidia"
</span><span class="line">    VendorName     "NVIDIA Corporation"
</span><span class="line">    BoardName      "GeForce GTX TITAN Black"
</span><span class="line">    BusID          "PCI:1:0:0"
</span><span class="line">    Screen          1
</span><span class="line">EndSection</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The other one is just</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class=""><span class="line">Section "Device"
</span><span class="line">    Identifier     "Device0"
</span><span class="line">    Driver         "nvidia"
</span><span class="line">    VendorName     "NVIDIA Corporation"
</span><span class="line">EndSection</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The first one is generated by <code>nvidia-settings', the second one is generated by
</code>*run` file. I have three TITAN Black installed on the motherboard with the
second configuration file, and they are all functional.</p>

<p>The guess now is if no bus id is specified in <code>xorg.conf</code>, <code>X11</code> could pick any
one, and prefer to the one with lowest PCI id or GPU id.</p>

<p>A side note. To get the bus id of a card, use <code>lspci</code>, which output corresponds
to GPU is</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">01:00.0 VGA compatible controller: NVIDIA Corporation GK110B [GeForce GTX TITAN Black] (rev a1)</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><code>01:00.0</code> is the bus id, though I am still now know why the period becomes a colon.</p>

<h2 id="note-on-cuda">Note On CUDA</h2>

<p>After all those efforts to install NVIDIA driver through <code>*run</code> file, I found
<code>cuda</code> installation(using the network deb provided by NVIDIA) just overrided
the driver I have just installed…</p>

<p>I am not sure whether dkms will be automatically installed, or nouveau be
automatically blacklisted, though the installation of cuda does not mention it,
which is to mean they probably will take care of this. If this is so the
installation of NVIDIA driver and CUDA will be much easier.</p>

<p>I notice the <code>xorg.conf</code> is not changed after the &#8220;new’’ driver being
installed.</p>

<h2 id="bios-mode-or-uefi-mode-ubuntu">BIOS mode or UEFI mode Ubuntu</h2>

<p>The last set of note is about installing Ubuntu in BIOS mode or UEFI mode,
which corresponds to using MBR or GPT partition table, and may be some other
difference.</p>

<p>At the time when choosing the boot media, even if your USB only has one Ubuntu
on it, the boot menu would show up as entries, one for normal BIOS mode
ubuntu(which is just named Ubuntu), one for UEFI mode(which has a UEFI in the
name). I have not tried the second one. The UEFI is a pretty new thing to me,
since the last time I learned something about it, UEFI is still in its
infancy.</p>

<p>If UEFI is chosen, the partition table of the hard disk where the boot loader
would be installed must be GPT, which is written in the new standard. The boot
loader of MBR and GPT are different(obviously). For GPT, an explicitly
formatted and allocated partition seems to be needed(not verified). As for MBR,
as everyone knows, the space for the boot loader will always be left out. Also,
to boot from a GPT partitioned disk, a boot flag need to be marked on the disk,
otherwise, OS won’t boot. There are still many other difference I have not
figured out yet.</p>

<h2 id="reference">Reference</h2>

<ol>
  <li><a href="http://www.linuxsecrets.com/blog/15questions-troubleshooting/2015/09/10/1651-how-to-remove-nouveau-and-install-nvidia-drivers">How to Remove Nouveau and Install Nvidia Drivers</a></li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VLC下使用中文字幕]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2016/02/16/vlcxia-shi-yong-zhong-wen-zi-mu/"/>
    <updated>2016-02-16T22:00:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2016/02/16/vlcxia-shi-yong-zhong-wen-zi-mu</id>
    <content type="html"><![CDATA[<p>记如何在VLC下使用中文字幕。</p>

<!-- more -->

<p>默认状态下，VLC会遇到两个问题，一是解码的codecs所用不对，而是用于显示字体的渲染
器(render)所用字体不对。</p>

<p>第一个问题的解决办法一是更改VLC所用的codecs；二是修改文件的codecs为utf-8。第一
种方法在<a href="http://bbs.feng.com/read-htm-tid-344593.html">这里</a>有介绍。第二种方法
我发现Vim可以完美完成，特此记之。</p>

<p>用Vim打开字幕文件，然后使用如下命令：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">:set fileencoding=utf-8</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>保存，退出。</p>

<p>第二个问题的解决也来自之上的
<a href="http://bbs.feng.com/read-htm-tid-344593.html">链接</a>。为防止链接失效，特此重述
方法：</p>

<p>打开VLC的<code>Preferences</code>窗口（<code>Ctrl+P</code>），点击右下角的<code>all</code>，在左边的索引中依次选择
<code>Video -&gt; Subtitles/OSD -&gt; Text renderer</code>，右边的设置区第一项为<code>Font</code>，点击<code>Browse</code>按
钮选择一项中文字体，如<code>WenQuanYi Micro Hei</code>。更改完成之后需要重启VLC。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Note on Running Tensorflow in Anaconda]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2016/01/09/note-on-running-tensorflow-in-anaconda/"/>
    <updated>2016-01-09T19:25:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2016/01/09/note-on-running-tensorflow-in-anaconda</id>
    <content type="html"><![CDATA[<p>Due to conflict of protobuf version, I have to run tensorflow in a sandbox
environment, aka anaconda. This is the note to note down some of its tricky
parts I spent some time figuring out.</p>

<!-- more -->

<p>Most of the packages are backward compatible, so normal packages could be
installed on servers normally, by <a href="http://www.ansible">Ansible</a>. But if
Caffe(it seems the newest Caffe upgraded to protobuf3) and tensorflow want to
be installed in the same machine, protobuf2 and protobuf3 have to co-exist. Due
to their
<a href="https://www.tensorflow.org/versions/master/get_started/os_setup.html#mac-os-x-typeerror-init-got-an-unexpected-keyword-argument-syntax">conflict</a>,
they cannot co-exist. So to not break others environment, I have to install
tensorflow in anaconda.</p>

<p>There are two things I think I may need in the future:</p>

<ol>
  <li>To install Anaconda and all consequent python packages in it using Ansible.</li>
  <li>A bug either in pip or in conda that needs some workaround.</li>
</ol>

<h2 id="using-ansible-with-anaconda">Using Ansible with Anaconda</h2>

<p>Conda has a <a href="http://conda.pydata.org/docs/help/silent.html">silent mode</a>, so it
could be install using shell script, which could be achieved using the <code>shell</code>
module in Ansible.</p>

<p>To install consequent packages all in Anaconda environment, we could write a
playbook that has the location of <code>pip</code> as a variable, so by passing different
<code>pip</code> binary, we could install the python packages wherever we want.</p>

<h2 id="solve-cannot-remove-entries-from-nonexistent-file--easy-installpth">Solve &#8220;Cannot remove entries from nonexistent file: … easy-install.pth’’</h2>

<p>There is a bug preventing one upgrading packages in Anaconda. Where this bug
belongs to pip/setuptools or conda seems still in
<a href="https://github.com/pypa/pip/issues/2751">consideration</a>.</p>

<p>The workaround I used, one of the workarounds suggested by the above link,
which I think is the simplest one is to add <code>--ignore-installed</code> option to
<code>pip</code>.</p>

<p>If your packages in contained in <code>conda</code>’s collection, the best solution is to
install it there. For instance, for <code>numpy</code>, it is just better to install by</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">conda update numpy
</span></code></pre></td></tr></table></div></figure></notextile></div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting up Multi-Monitor on Gnome Ubuntu 14.04 with NVIDIA Card]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2016/01/06/setting-up-multi-monitor-on-gnome-ubuntu-14-dot-04-with-nvidia-card/"/>
    <updated>2016-01-06T08:16:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2016/01/06/setting-up-multi-monitor-on-gnome-ubuntu-14-dot-04-with-nvidia-card</id>
    <content type="html"><![CDATA[<p>I got an old monitor and spent some time tweaking the dual monitor setting of
my PC in the office. Here I note down</p>

<ol>
  <li>a tweak to make multiple monitor in Gnome functions better, and some notes
on the behavior of <code>guake</code> under multi-monitor setting.</li>
  <li>a hack to let X Windows remember the monitor layout</li>
  <li>and a short comparison between Gnome 3 and KDE Plasma 4.</li>
</ol>

<!-- more -->

<h2 id="dual-monitor-on-gnome">Dual Monitor on Gnome</h2>

<p>I uses <a href="https://www.gnome.org/">Gnome</a> as my desktop environment ever since the
time I started using Linux. So naturally, the desktop I tried this time to
manage multi-monitor support is Gnome. By default, the dual monitor worked
pretty OK. The only one thing that may need a tweak is: by default, Gnome(my
version is 3.9.90), only supports workspace in primary monitor, so when you
switch back and forth between different workspace, the secondary monitor stays
fixed on whatever it is on. This kind of makes the extra monitor many times
less useful. To make the secondary monitor attach to current workspace, so it
moves when you switch workspaces, you need to configure Gnome a little bit, by
the following command</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">gsettings <span class="nb">set </span>org.gnome.shell.overrides workspaces-only-on-primary <span class="nb">false</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>One drawback of making such a change is <code>guake</code> will stay on the secondary
window forever. Did not get the time to look into the source code to know why
:(.</p>

<p><em>Updated on Jan 9, 2016.</em></p>

<strike>It is weird. `guake` functions normally now, after a reboot. This is probably
not the first time I reboot. I guess it is the package updates that fixed
something.</strike>

<p>I figured out why. When you maximize <code>guake</code> it sticks with that monitor, but
your cancel maximization, it would reconsider it position by probing which
monitor the mouse is in and set itself there.</p>

<h2 id="physical-layout-of-monitors">Physical Layout of Monitors</h2>

<p>I do not know how much it depends on operating system, drivers and
hardware. The <code>display</code>(the GUI one of Gnome) or <code>nvidia-settings</code>(the CLI one
comes with proprietary driver) forgets the monitor layout after reboot, such as
<code>DVI-I-1</code> should be on the left of <code>DVI-D-0</code>, even if I wrote it in the
<code>xorg.conf</code>.</p>

<p>Though finally I switched the monitors physically, for some other reason, and
made peace with the forgetful display manager, I struggled some time to figure
out how to make the layout right.</p>

<p>The solution I used is to use <code>xrandr</code> after the GUI session has set up by
adding a script containing the following line in the startup application of
Gnome(there is a GUI program called <code>Startup Application</code>, though I do not know
what its CLI version is).</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">xrandr --output DVI-I-1 --mode 1920x1080 --right-of DVI-D-0
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I also have tried to add the above line in <code>.profile</code>, but it seems the layout
setting is decided after <code>.profile</code> is loaded, so it did not work.</p>

<p><em>Updated on Mar 8, 2016.</em></p>

<p>I seems to figured out how gnome handle layout of monitors. The one that powered
on the first will be the primary monitor, and be put on the left. The one added
later will be put at the right. So if you want to change the layout of monitors,
just disconnect the one you want to put at the right, then connect again.</p>

<h2 id="a-peak-at-kde">A Peak at KDE</h2>

<p>At the first time I tried to set <code>workspaces-only-on-primary</code> to <code>false</code>, it
did not have any effect, and first all workspaces other than the first one
started to get severe artifact when moving windows around, then even the first
one started to get artifacts. It made the desktop unusable.</p>

<p>After trying to search for an solution in Google for a while, I learned there
seems Gnome do not have any multi-monitor support at all, at least they do not
consider this a feature to be proud of at their website for Gnome 3. So I think
maybe Gnome does not work for Gnome.</p>

<p>Then some video introducing Plasma let me pay some attention to KDE
desktop. The modern design of KDE Plasma 5.5 really impressed me, though in
retrospect I was dazzled by Gnome 3’s new design at the first I saw it. It is
just the case that I have been used to Gnome 3 for such a long time and do not
feel its design at all now.</p>

<p>I installed Plasma from source by</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">sudo apt-get install kubuntu-desktop
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This is the first time I started to feel Ubuntu 14.04 is getting old. It only
has Plasma 4.14(the minor version may not be right, but it is 4).</p>

<p>After tinkering it a little bit, though it must be the case I did not get
myself familiar with it, I have the first impression as following:</p>

<ol>
  <li>It seems to have many widgets, which is better than Gnome, whose desktop
widget feels like antiques.</li>
</ol>

<p>Besides that, I did not find an obvious solution to</p>

<ol>
  <li>multiple workspaces, which is available by default in Gnome</li>
  <li>the dash application runner(I am not sure its exact name, just you press the
<code>super</code> or <code>win</code> key in you desktop and it shows up). I tried to install
a app called <code>homerun</code>, but it just adds too much icons in the desktop and
creates too much clutters.</li>
  <li>Obvious, <code>guake</code> does not work here anymore.</li>
</ol>

<p>The above three are really killer app, and cannot not be lacking.</p>

<p>So the first impression is KDE is for the GUI users.</p>

<ol>
  <li>It has a good file manager called <code>Dolphin</code>, whose functionality is better
than <code>Nautilus</code>. But I almost do not use GUI file management since I do it
all in CLI.</li>
  <li>A similar application launcher of dash of Gnome is <code>Krunner</code>. But like
Unity, it not only search for applications, but also search for files, any
opened applications etc, which creates much clutter, though this is a
desktop user wants.</li>
  <li>Again, no easy drop-down terminal like <code>guake</code>.</li>
  <li>It adds too much clutter that may not be useful, compared with a clean wall
paper of Gnome. I think Plasma 5.5 tries to fix this, but I did not have the
chance to try.</li>
</ol>

<p>As having been said, it must be the case I am not familiar with a new desktop
environment, but I guess I will settle with it, because after I was little
frustrated with the KDE desktop, I tried to switch back to Gnome. Suddenly,
everything started to work well! Maybe it is because I installed a lot of
libraries when I was installing KDE, which fixed some problems. But now I feel
current setting, the one I mentioned at the first section of this note,
awesome.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Environment Variable Setup of Linux]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2016/01/05/on-environment-variable-setup-of-linux/"/>
    <updated>2016-01-05T11:28:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2016/01/05/on-environment-variable-setup-of-linux</id>
    <content type="html"><![CDATA[<p>I have spent some time figuring out how Linux sets up its environment variables
for login shell, non-login shell to let Emacs inherits environment variables
and make tmux loads <code>.bashrc</code> and note it here.</p>

<!-- more -->

<h2 id="login-shell">Login Shell</h2>

<p>We start by figuring out the boot process.</p>

<p>For text console,</p>

<ol>
  <li>At the end of boot the mother of all processes <code>init</code> is started. init’s
environment, including PATH, is defined in its source code and cannot be
changed at run time.</li>
  <li><code>init</code> runs the start-up scripts from <code>/etc/init.d</code> depending on the run level
set in <code>/etc/inittab</code>. Since init’s environment is very bare, the scripts
define their required environment variables within themselves.</li>
  <li><code>init</code> starts the text login process that waits for the user to log in. When
the user logs in, the login process checks <code>/etc/passwd</code> to see what shell
should be started for this particular user.</li>
  <li>The shell starts and reads its shell-specific configuration files.
    <ol>
      <li>For Bash, it first reads <code>/etc/profile</code> to get values that are defined
for all users. After reading that file, it looks for <code>~/.bash_profile</code>,
<code>~/.bash_login</code>, and <code>~/.profile</code>, in that order, and reads and executes
commands from the first of these files that exists and is readable.</li>
    </ol>
  </li>
</ol>

<p>For graphical UI,</p>

<ol>
  <li>At the end of booting, the mother of all processes – <code>init</code> – is started.</li>
  <li><code>init</code> runs the start-up scripts from <code>/etc/init.d</code> depending on the run
level set in <code>/etc/inittab</code>. Since <code>init</code>’s environment is very bare, the
scripts define required environment variables within themselves.</li>
  <li>Init starts the GDM display manager, which in turn will start the graphical
login.</li>
  <li>When the user successfully logs in, GDM starts xsession, which reads the
file <code>/etc/gdm/Xsession</code> and with it the environment variables for the
user’s session. The default version of the Xsession file first reads
<code>/etc/profile</code> for global settings and then <code>~/.profile</code> to add the user’s
individual settings.</li>
</ol>

<p>The above boot process is the process to set up environment to login shell. So
if any user specific environment that is need for a graphical program, one
could choose to put it in <code>~/.bash_profile</code>, <code>~/.bash_login</code>, and
<code>~/.profile</code>. I chose to put it in <code>.profile</code>. I actually spent time figuring
this out, so Emacs could inherit environment variables the local libraries I
installed.</p>

<p>As for system wide setup, put it in <code>/etc/profile</code>.</p>

<h2 id="non-login-shell">Non-login Shell</h2>

<p>If a shell is needed after login, the setting should go to the non-login
shell’s, <code>.bashrc</code>. For instance, the terminals created by <code>terminal</code>,
<code>terminator</code> and other terminal programs are non-login shells. This is where I
previously put all my configurations in.</p>

<p>The following comes from <code>man bash</code>’s <em>INVOCATION</em> section.</p>

<pre><code>When bash is invoked as an interactive login shell, or as a non-interactive
shell with the `--login` option, it first reads and executes commands from the
file `/etc/profile`, if that file exists.  After reading that file, it looks
for `~/.bash_profile`, `~/.bash_login`, and `~/.profile`, in that order, and
reads and executes commands from the first one that exists and is readable.
The --noprofile option may be used when the shell is started to inhibit this
behavior.

When a login shell exits, bash reads and executes commands from the file
`~/.bash_logout`, if it exists.

When an interactive shell that is not a login shell is started, bash reads and
executes commands from `/etc/bash.bashrc` and `~/.bashrc`, if these files
exist.
</code></pre>

<h2 id="distribution">Distribution</h2>

<p>Each Linux distribution may tweak files mentioned above, for instance, unset
some variables somewhere, so if you set its value before where it is unset,
your setting will not take effect. Normally, if one’s configuration does not
work, consider go through all previous configurations and understand what they
exactly do in such a distribution.</p>

<h2 id="misc">Misc</h2>

<p>If one uses <code>tmux</code>, the terminal multiplexer, note it creates login shells. So
if one wants <code>.bashrc</code>, especially aliases, to work under <code>tmux</code>, remember to
source it after whatever login shell configuration one may use.</p>

<h2 id="reference">Reference</h2>

<ol>
  <li>https://wiki.debian.org/EnvironmentVariables</li>
  <li>https://help.ubuntu.com/community/EnvironmentVariables</li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Optimization Summary]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/12/16/optimization-summary/"/>
    <updated>2015-12-16T15:06:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/12/16/optimization-summary</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>The semester’s course on Optimization is going to end. This is a summary note
to unify the mathematical picture learned during this period.</p>

<!-- more -->

<h2 id="problem-formulation">Problem Formulation</h2>

<p>In the most general form, an optimization problem is to find an extreme value,
taking minimum for ease of writing, in the following form</p>

<script type="math/tex; mode=display">
\min\ f(X)\\
G(X) \succeq \theta\\
H(X) = \theta
</script>

<p>where $X$ could be a scalar, vector or matrix; $G(X)$ is a stack of inequality
constrains, whose row is one inequality constrain; $\theta$ is zero vector;
$H(X)$ is a stack of equality constrains, whose row is one equality constrain.</p>

<p>To solve the above problem, analysis should be made to properties of $f(X),
G(X)$ and $H(X)$. Geometric intuition is the source of aspiration and algebraic
descriptions or relaxation of geometric intuition is how we could solve the
problem.</p>

<p>The geometric picture of $f(X)$ is contour of its value, whose metaphor is the
contour of the height of a mountain; or just the mountain by seeing how high
its. The first picture is in the domain of $X$, while the second one is in the
$(X, f(X))$, which adds another dimension.</p>

<p>$G(X)$ and $H(X)$ explicitly or implicitly define an sub-area of all possible
$X$. Its picture is harder to visualize, and is the difficulty aspects in
optimization.</p>

<h2 id="domain-type">Domain Type</h2>

<p>Different types of variables has different structure and properties that
enforced on the domain. As an example, Conic Linear Programming is to explore
the structureness to simplify problems.</p>

<p>It does not occur to me clearly for now how possibly the enforced peculiar
structure of Semi-definite programming or Second Order Cone Programming could
have physical meaning. But SDP how take advantage of matrix variable and
inequality is inspiring.</p>

<h2 id="geometry-of-domain">Geometry of Domain</h2>

<p>The work around is to assume only to solve problems who is convex, and call the
remaining problems non-convex, consequently classifying it as hard.</p>

<p>It seems this is rather a restrict and side step the difficult but important
problems. This was the perception I had when I first heard about Optimization
years ago. But the actually it is not after really struggling through it.</p>

<p>Convexity of a set is the atom to describe geometry of the whole set in the
following sense: if two points are in the set, then we want any points between
them are also in the set. Normally, points have some physical meaning in real
world problem, not some bizarre mathematical fabrication. In physical world,
extremity breaks thing, but not their intermediate value. Thus it is a good
model unless we discover some bizarre phenomena(which probably already exists
but I do not know).</p>

<p>Convexity and concavity of function are the only two ways how function could
vary locally. The really mattered properties of convexity is non-decreasing, so
concavity is non-increasing. They get named quasa-convex for convexity part.</p>

<p>If we could analyzed the atoms how sets compose a bigger set and how functions
composes a &#8220;larger’’ function, we begin to get a look into the whole picture.</p>

<p>Current situation of optimization is local geometry has been well studied, but
a way to generalize to the global geometry lacks. So I guess it is not the
a good understanding of convexity is the first step to understand more complex
problems.</p>

<h2 id="dimensionality-of-domain">Dimensionality of Domain</h2>

<p>This section is about the concept of <em>affine</em>.</p>

<p>The motivation of &#8220;affine’’ is to create a term that could be used to describe
the dimension of domain. So essentially, an affine set is linear variety or the
whole space. Linear variety is a concept. For example, a linear variety in 3D
space would be a line, a plane.</p>

<h2 id="unifying-optimization-duality-with-functional-analysis">Unifying Optimization Duality with Functional Analysis</h2>

<p>If we start from a Linear Programming(LP), which is the following problem</p>

<script type="math/tex; mode=display">
\min\ c^{T}x\\
Ax = b
x \geq 0
</script>

<p>and its dual</p>

<script type="math/tex; mode=display">
\max\ b^{T}y\\
A^{T}y \leq c
</script>

<p>From function analysis point of view, those two problems involves four space,
space of $x$(we call it primal space for convenience from now), dual space of
primal space, space of $y$(we call it transformed space from now), dual space
of transformed space. Duality of LP is the characterization of relationship
between those four spaces.</p>

<p>If we upgrade this view to non-linear case,</p>

<script type="math/tex; mode=display">
\min\ f(X)\\
G(X) \succeq \theta\\
H(X) = \theta
</script>

<p>we see $G(X)$ and $H(X)$ are also a transform, but not linear. But similar idea
should also apply. This is the idea of Lagrange multiplier and Lagrange
duality.</p>

<p>The idea is to analyze in the transformed space $(f(X), Z)$, where $Z$ is the
whose variables are $G(X), H(X)$(in proof involved with Lagrange duality, we
break $H(X)$ to two inequalities). $Z$ and its dual <script type="math/tex">Z^{*}</script> centralize all
duality in optimization.</p>

<p>The hard part is to analyze what the space $Z$ is like. First, different types
of variables have different properties and work with different inequalities,
such as vector and vector inequality, matrix and matrix inequality. Second,
what the transformed space is like after mapping is hard to deal with.</p>

<p>The key properties of $Z$ is whether it has an interior point and is convex,
which determines whether we could find a separating hyperplane, an element in
dual space.</p>

<p>Farkas lemma is just a special case where we could explicitly know what the
transformed space looks like.</p>

<h2 id="on-cone-and-dual-cone">On Cone and Dual Cone</h2>

<p>This section is about cone and dual cone. One more example could be seen in the
next section.</p>

<p>Cone is about direction. It is hard to describe it informally without concrete
examples. Pointed cone identifies inequality relations, which is summarized in
this <a href="http://shawnLeeZX.github.io/blog/2015/11/28/cone-dual-cone-and-generalized-inequalities/">note</a>.</p>

<p>Dual cone is about the direction that at least slightly the same with primal
cone, so in this direction values also could increase, or decrease if it is a
negative dual cone. Dual cone is a set in dual space of primal space, whose
elements are normals of hyperplane. Remember a normal of a hyperplane is about
the direction where the values goes up(or down depending on definition).</p>

<h2 id="variational-idea-kkt-condition-fritz-john-condition">Variational Idea, KKT Condition, Fritz-John Condition</h2>

<p>Existence of Lagrange multiplier is the zero order properties of a constrained
optimal point. By considering the first derivative, we could get how gradients
at the optimal point are related to each other.</p>

<p>Equality constrain in the limit is to carve subspace out, so all things should
happen in this subspace. Gradients of inequality constrains determine whether
the we could still have descent directions, which is Fritz-John or KKT
condition. The idea is if negative dual cone of positive range of gradients of
inequality constrains do not intersect with negative dual cone of $\nabla
f(x)$, then we reach a constrained minimum. The regularity conditions of KKT
just means the gradients of inequality constrains do not conflict with each
other, which if happens would only leave a feasible set that has nothing to do
with $f(x)$, which is the case the dual variable associated with $\nabla f(x)$
is zero in Fritz-John optimality condition.</p>

<h2 id="optimization-algorithms">Optimization Algorithms</h2>

<p>Few has been explored in this area for now.</p>

<p>The idea is to explore the structures of the four spaces mentioned, and the
target function to find the extreme values and corresponding solutions, either
globally, for instance using the simplex method, or locally, using gradient
based methods.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On KKT Condition and Optimality Condition of Conic Linear Programming]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/12/15/on-kkt-condition-and-optimality-condition-of-conic-linear-programming/"/>
    <updated>2015-12-15T20:18:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/12/15/on-kkt-condition-and-optimality-condition-of-conic-linear-programming</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>In LP and SDP, the complementary slackness are written as</p>

<script type="math/tex; mode=display">
(c - A^{T}y)^{T}x = 0\\
(C -\sum\limits_{i}y_{i}A_{i}) \bullet X = 0
</script>

<p>While KKT condition’s complementary slackness is</p>

<script type="math/tex; mode=display">
v^{T}g(x) = 0
</script>

<p>It is hard to connect them at first glance, since dual variable $v$ is dual to
the space of $g(x)$, which is not necessarily in the same space with $x$.
Especially when $x$ is the big matrix $X$ in SDP, $g(X)$ lies in vector space
instead of matrix space.</p>

<p>This is the note for the how they are connected.</p>

<!-- more -->

<h2 id="linear-programming">Linear Programming</h2>

<p>For linear programming, we write down its Lagrange</p>

<script type="math/tex; mode=display">
c^{T}x + y^{T}(b - Ax) - s^{T}x
</script>

<p>its gradient is $c - A^{T}y - s$</p>

<p>so its KKT condition is</p>

<script type="math/tex; mode=display">
c - A^{T}y - s = 0 \text{ Stationary point}\\
x \geq 0, Ax = b \text{ Primal feasibility}\\
s^{T}x = 0, s \geq 0 \text{ Complementary slackness}
</script>

<p>Now we see $(c - A^{T}y)^{T}x = 0$ is the combination of stationary point
condition and complementary slackness condition. The special structure occurs
here because</p>

<ol>
  <li>In linear problem, analytic solution of equation is possible.</li>
  <li>$g(x)$ is just $x$, so their spaces are the same.</li>
</ol>

<p>Note when LP reaches its optimal value, we always have $c^{T}x = b^{T}y =
(A^{T}y)^{T}x$, but there is a difference $s$ between $c$ and $A^{T}y$. Here we
see the specialty of LP. When one dimension of $s$ is not zero, we have
correspond dimension of $x$ to be zero. Due to the linearity of $c^{T}x$, the
zero directly reflects on the objective value, which makes correspond
dimension’s contribution to zero. Now it is clearer why duality gap is always
zero.</p>

<h2 id="semi-definite-programming">Semi-Definite Programming</h2>

<p>For SDP, if we want to draw analog with LP, the Lagrange may look like</p>

<script type="math/tex; mode=display">
C \bullet X + \sum\limits_{i}y_i(b_i - A_i \bullet X) - S \bullet X
</script>

<p>its gradient is <script type="math/tex">C - \sum\limits_{i}y_i A_i - S</script>.</p>

<p>So its KKT condition is</p>

<script type="math/tex; mode=display">
C - \sum\limits_{i}y_i A_i - S = 0\\
x \succeq 0, A_i x = b_i\\
S \bullet X = 0
</script>

<p>which is almost the same in format with that of LP.</p>

<p>But note</p>

<ol>
  <li>KKT does not work wit matrix inequality yet, so $S \bullet X$ cannot be
moved up to the Lagrange function. Though it does suggest some form of
KKT condition that could deal with matrix inequality may exist.</li>
  <li>The relationship is not linear anymore, both in $X \succeq 0$ and $C \bullet
X$ because the symmetric constrain enforced on $X$, which makes $X$ not just
a matrix form vector.</li>
  <li>$g(X) \succeq 0$ maps matrix to matrix space, so actually complementary
slackness also should be in the space of matrix, and in form of matrix inner
product.</li>
</ol>

<p>So there should be a duality gap normally.</p>

<h2 id="last-note-on-dual-variables">Last Note on Dual Variables</h2>

<p>From above, we see the reason we have matrix inner product style complementary
slackness is due to we are using dual variable in matrix space. It is
interesting to see how general dual variable or linear hyperplane is on solving
problem on any space, though I have not learned any proof of Lagrange on spaces
other than Euclidean space yet.</p>

<h2 id="last-note-on-linearity">Last Note on Linearity</h2>

<p>In conic linear programming,</p>

<ol>
  <li>when putting constrains in Lagrange function, it directly interacts with
objective function, meaning variable $x$ could be separated out.</li>
  <li>The specialty of $x \geq 0$ makes its dual variable the slack variable.</li>
  <li>Linearity makes dual function $\inf \ldots$ analytic solvable, so we only
see $\max$ in the dual problem.</li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Duality in Optimization]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/12/14/on-duality-in-optimization/"/>
    <updated>2015-12-14T21:09:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/12/14/on-duality-in-optimization</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>A more general picture of duality in optimization emerges itself. Some time is
still needed to unify all duality in optimization, but for now I summarize a
draft version of it here using duality of linear programming as an example.</p>

<!-- more -->

<h2 id="intuition-of-lagrange-multiplier-from-functional-analysis">Intuition of Lagrange Multiplier From Functional Analysis</h2>

<p>In its most general form, optimization problem is</p>

<script type="math/tex; mode=display">
\max\ f(x)\\
G(x) \geq 0\\
H(x) = 0
</script>

<p>where $G(x)$ is the group of inequality constrains <script type="math/tex">g_i(x)</script>; $H(x)$ is the
group of equality constrains <script type="math/tex">h_i(x)</script>; $x \in X = R^{n}$ or $x \in X = R^{m
\times n}$, where $x$ is taken as matrix variables.</p>

<p>The domain of the problem is implicitly defined by $G(x)$ and
$H(x)$. Geometrically, $H(x) = 0$ describes some surface in $X$. $G(x)$
describes some &#8220;half space’’.</p>

<p>The question to ask is: what objects should we take those constrains as? so we
could internalize and reason with them.</p>

<p>In abstract sense, if we take $f(x), G(x), H(x)$ as variables, or in a more
intuitive word, as objects, in our perception the problem seems not to be that
abstract and convoluted. Their dependency on $x$ seems to gone. This is the
idea from functional analysis. We are just doing space transform on the
original space $X$. After the transform, we try to find the extreme value of a
specific coordinate of the new space, which is $f(x)$, in a sub-area of the new
space, which describes by constrains.</p>

<p>Despite of the non-linearity, every natural phenomena in nature, thus in Math,
comes from certain metamorphism of linear phenomena. To say it in another way,
though we could create very bizarre phenomena with math trick, the real world
should be analyzed starting from the linearity, not for simplicity or
computation’s sake.</p>

<p>But how could we come up with linearity?</p>

<p>This brings to think why constrains exists. It exists because in real world,
certain things has to be satisfied, it could be a fixed location, or limited
amount of resources and so on. The real world constrain is somehow connected to
the objective function, otherwise it won’t exist at all. So how could we model
such connection?</p>

<p>This is the key to the entry of duality.</p>

<p>We have to find the connection of variables $G(x), H(x)$ to another variable
$f(x)$. As having been discussed, every natural phenomena somehow comes from
metamorphism of linear phenomena. An example is the invention of calculus,
which is the how non-linear phenomena behaves locally. The way we connect those
variables is through their linear combination. This in the language of
functional analysis is to say we analyze the primal space from its dual space.</p>

<p>Now back to Lagrange duality. Lagrange multiplier is elements in dual space of
the space made up by $f(x), G(x), H(x)$.</p>

<p>This intuition unscramble constrains.</p>

<h2 id="an-example-linear-programming">An Example: Linear Programming</h2>

<p>Linear programming is the case where all things are linear. So nothing
metamorphized. When we only deal with gradients in a specific point in
non-linear programming, such as the case of KKT condition, Fritz-John
condition, it is actually exactly the optimality condition of linear
programming.</p>

<p>Previously I have noted an possible motivation of Linear programming. Now I
could see the previous note(about two months ago) is just a special case of
previous idea.</p>

<p>For a LP problem, as the following</p>

<script type="math/tex; mode=display">
\max\ c^{T}\\
Ax = b\\
x \geq 0
</script>

<p>I tried to see it from adjoin operator point of view, which is to find
something from $A^{T}$. For details, please refer to previous
<a href="http://shawnLeeZX.github.io/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view/">note</a>.</p>

<p>Now we write the Lagrange of LP, the dual function is</p>

<script type="math/tex; mode=display">
\inf\limits_{x} c^{T}x + y^{T}(Ax - b) - s^{T}x
</script>

<p>note we have $s \geq 0$.</p>

<p>Now we see how could we deduce it from previous argument of last
<a href="http://shawnLeeZX.github.io/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view/">note</a>.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
                &  \langle 1, c^{T}x \rangle + \langle y, Ax -b \rangle - \langle s, x \rangle\\
\Leftrightarrow &  \langle c, x \rangle + \langle y, Ax -b \rangle - \langle s, x \rangle\\
\Leftrightarrow &  \langle c - s, x \rangle + \langle y, Ax \rangle - \langle y, b \rangle\\
\Leftrightarrow & - \langle y, b \rangle +  \langle c - s, x \rangle + \langle y, Ax \rangle \\
\Leftrightarrow & - \langle y, b \rangle +  \langle c - s, x \rangle + \langle A^{T}y, x \rangle \\
\Leftrightarrow & - \langle y, b \rangle +  \langle c - s + A^{T}y, x \rangle \\
\Leftrightarrow &  \langle y, b \rangle +  \langle c - s - A^{T}y, x \rangle \\
\end{align*}
 %]]&gt;</script>

<p>We find some connection between dual space and primal space now. To make the
part of dual space be a lower bound, we ask $s \geq 0$. This is just the
zooming idea previously.</p>

<p>The more geometrically picture is the geometrical intuition from Lagrange
duality.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
 & \inf\limits_{x} \langle c, x \rangle + \langle y, Ax -b \rangle - \langle s, x \rangle\\
\Leftrightarrow & \inf\limits_{x} \langle c - s, x \rangle + \langle y, Ax \rangle - \langle y, b \rangle\\
\Leftrightarrow & - \langle y, b \rangle + \inf\limits_{x} \langle c - s, x \rangle + \langle y, Ax \rangle \\
\Leftrightarrow & - \langle y, b \rangle + \inf\limits_{x} \langle c - s, x \rangle + \langle A^{T}y, x \rangle \\
\Leftrightarrow & - \langle y, b \rangle + \inf\limits_{x} \langle c - s + A^{T}y, x \rangle \\
\Leftrightarrow &  \langle y, b \rangle + \inf\limits_{x} \langle c - s - A^{T}y, x \rangle \\
\end{align*}
 %]]&gt;</script>

<p>The $\inf$ could move the dual variable down. So we have a constrained problem
again.(Details are omitted, and could be found at <em>Nonlinear Optimization
Andrzej Ruszczynski</em> or some other books talk about Lagrange duality).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[General Transform, Linear Transform, Matrix]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/11/30/general-transform-linear-transform-matrix/"/>
    <updated>2015-11-30T10:30:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/11/30/general-transform-linear-transform-matrix</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>Previously, the origin of Matrix for me comes from linear transform, and
nothing more. Most of the intuitive comes from linear space transform, as in
the example of orthogonal Fourier transform or wavelet transform. As I learn
more and more optimization, and how it computes dual, matrix as a limit case of
non-linear transform comes to me. The linear transform characteristics is just
to approximate the nonlinear transform locally.</p>

<!-- more -->

<h2 id="how-matrix-originally-arises">How Matrix Originally Arises</h2>

<p>Originally, matrix origins from linear transform, and is called linear
operator.</p>

<p>The deviation comes from how a basis of space $X$ gets mapped to another space
$Y$. For details, please refer to <em>Linear Algebra Done Right</em>, <em>Introductory
Functional Analysis with Application</em>, <em>A Friendly Introduction to Wavelet</em>, or
any other books that talks about linear algebra.</p>

<p>The underlying idea is pure linear, and does not touch nonlinear at all.</p>

<h2 id="matrix-as-limit-case-of-nonlinear-transform">Matrix As Limit Case of Nonlinear Transform</h2>

<p>The study of constrained nonlinear optimization made me thinking, when seeing
it in the most general form</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
  \text{minimize  } & f(x) \\
  \text{subject to } & g_i(x) \leq 0
\end{align*}
 %]]&gt;</script>

<p>First I simplify the problem to linear programming.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
  \text{minimize  } & f(x) \\
  \text{subject to } & Ax - b \leq 0
\end{align*}
 %]]&gt;</script>

<p>Solving system of linear equations is a central theme in Math, and it confused
me when I had no idea relating the matrix $A$ in the system of equations to the
space transform intuition of matrix.</p>

<p>If I try to explain it by normal space transform, $Ax$ becomes</p>

<script type="math/tex; mode=display">
\sum\limits_{i}\vec{a_{i}}x_{i}
</script>

<p>where $\vec{a_{i}}$ is columns of $A$.</p>

<p>It is explained as whether $b$ is in range of $A$, and range of $A$ is
picturized as a space, which have nothing to do with $A$. The connection is
broken.</p>

<p>But if we change a perspective, write $Ax$ as</p>

<script type="math/tex; mode=display">
\vec{a_{i}}^{T}x
</script>

<p>where here $\vec{a_{i}}^{T}$ is the row of $A$.</p>

<p>Denote the space $x$ belongs to as $X$, the one $Ax$ belongs to as $Y$, dual of
$X$ as $X^{*}$.</p>

<p>Assuming the bases of $X$ is orthonormal, which is normally in practice,
<script type="math/tex">\vec{a_{i}}</script> is a basis of $Y^{*}$ gets mapped to <script type="math/tex">X^*</script>.</p>

<p>Dual space $X^{*}$ consists of linear functional defined on $X$. The intuitive
of linear functional $g(x)$ is the effect of change on $x$ will influence
$g(x)$ linearly. Think it as cost or price would clarify.</p>

<p>Write <script type="math/tex">\vec{a_{i}}^{T}x = g_{i}(x)</script>, the procedure how matrix transform $x$
becomes clearer. The idea is a linear weighted combination of different
elements of $x$. <script type="math/tex">\vec{a_{i}}^{T}x = g_{i}(x) \leq b_{i}</script> means in the direction of
$\vec{a_{i}}$, the projection of $x$ on it could not exceed a certain value,
which is also could be understood as some physical meaning, such as price,
or threshold.</p>

<p>So matrix is a stack of linear functionals and overall they together make up
some linear transform. If all the linear functionals are linear independent,
the system is invertable, so the matrix has an inverse. If all the linear
functionals are orthogonal, then the weighted sum does not influence each
other.</p>

<p>Now, the rank of matrix makes a lot of sense, and matrices that are not
unitary could be of more use.</p>

<p>Back to the most general form of nonlinear optimization. When the system
approaches its optimal solution, everything becomes more and more linear. The
problem actually boiled down to again, a linear problem. Everything matters are
just <script type="math/tex">g_{i}'</script>, which is just as linear as linear programming, and as
matrix. This is the base of Lagrange multiplier methods.</p>

<h2 id="optimality-condition-of-equality-constrained-optimization-an-example">Optimality Condition of Equality Constrained Optimization: An Example</h2>

<p>An example would make the idea that matrix’s nature on linear transform
clearer. I learned this example ten days after I wrote this note.</p>

<p>Given the following equality constrained problem</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
  \text{minimize  } & f(x) \\
  \text{subject to } & H(x) = \theta
\end{align*}
 %]]&gt;</script>

<p>Note we are using vector notation now. Denote the Jacobian of $H(x)$ as
$H’(x)$; $x \in X$, $H(x) \in Z$, where $X$ is a linear vector space, while $Z$
is a normed space. Suppose this problem reaches its constrained minimum
<script type="math/tex">x^{*}</script>. We assume <script type="math/tex">x^*</script> is a regular point, which means <script type="math/tex">H'(x^*)</script> maps
$X$ onto $Z$. If $Z$ is of finite dimension, it just means <script type="math/tex">H'(x^*)</script> is of
full rank.</p>

<p>To digress a little, if the mapping <script type="math/tex">H'(^*x)</script> is not onto, it means there is
linear dependence in the gradients of <script type="math/tex">H(^*x)</script> at <script type="math/tex">x^*</script>. When equality
constrains have linear dependence, the feasible region is empty. A picture
would be two parallel hyperplane, who do not intersect each other. For more
complex examples, refer to chapters of <em>Nonlinear Programming, Bertsekas</em>, or
<em>Nonlinear Programming, Theory and Algorithms</em> on optimality conditions.</p>

<p>The above clarification of terminology is to make the argument satisfies for
both infinite dimension and finite dimension space.</p>

<p>Since <script type="math/tex">x^*</script> is minimum(for now we use $x$ instead of <script type="math/tex">x^*</script> for convenience
sake), $\nabla f(x)$ has to be orthogonal to the direction $h$, where $H’(x)h =
\theta$. It means $\nabla f(x)$ is orthogonal to $N(H’(x))$, null space of
$H’(x)$. Now we could see clearly a linear transform, aka matrix is the limit
case of a general non-linear transform, or in another word, the local
approximation of general non-linear transform $H(x)$, in form of $H’(x)$.</p>

<p>To recap the last section, from a linear algebra point of view, we internalize
$Ax$ by how a component of $x$ is represented by coordinates in the new
space. But from a transform point of view, we internalize matrix as applying
each row of $A$, a linear functional to $x$ repeatedly. So whether a matrix is
of full, or be orthogonal does not matter that much. We just create a transform
by stacking some functional row by row.</p>

<p>The latter point of view is the intrinsic nature of transform. But the reason
much efforts have been made in linear algebra is because often ultimately a
problem would be attacked from linear point of view, which is the case of this
example. With the machinery from linear algebra, a much rich structure could be
used instead of just general transform.</p>

<p>Remember in a <a href="http://shawnLeeZX.github.io/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view/">note</a>
on Linear programming, the intuition of $Ax$ is to map a point from a space $X$
of points to another space of points $Z$, then its dual map or adjoin map
$A^{T}$ is to map dual space <script type="math/tex">Z^{*}</script> to <script type="math/tex">X^{*}</script>.</p>

<p>If $\nabla f(x) \perp N(H’(x))$, it means <script type="math/tex">f(x) \in R(H'(x)^*)</script>. So there
exists a <script type="math/tex">z^{*} \in Z^{*}</script> satisfying <script type="math/tex">f(x) = z^{*}H'(x)^{*}</script>, which could
be written as</p>

<script type="math/tex; mode=display">
\nabla f(x) + \langle z^{*}, H'(x) \rangle
</script>

<p>which is the optimality condition of optimization problems with equality
constraints.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cone, Dual Cone and Generalized Inequalities]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/11/28/cone-dual-cone-and-generalized-inequalities/"/>
    <updated>2015-11-28T11:04:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/11/28/cone-dual-cone-and-generalized-inequalities</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>I did not internalized the idea that a pointed cone identifies a generalized
equality until now, and would like to note it down.</p>

<!-- more -->

<h2 id="cone">Cone</h2>

<p>Let’s start with why we want inequality. We want it because we want to compare
two objects. In $R$, an object in this domain is a scalar and represents
something like length, or so. Suppose we want to compare the length of one edge
of a table and the length of a ruler. We put those two together.  Intuitively,
a table looks longer than a ruler, so we say if we want to compare anything
similar like those again, we standardize a unit, according to the reference to
the length of table or ruler, which in the ancient time is the length of some
king’s feet, give a length $x$ to object one, $y$ to object two, then if object
one is longer than object two, we call $x &gt; y$.</p>

<p>How could we generalize this idea to higher dimensional space? Consider vector
inequality. Suppose we want buy a department. The only factors we want to
consider is which floor our department is(assume the higher the better), and
how many square meters it has. What does department $x$ is better than $y$
mean? An intuitive answer is a department that is both higher and bigger is a
better choice. We call it $x \succeq y$.</p>

<p>We could see how cone abstract things here, in vector inequality.</p>

<p>First we break thing down, if we get two rulers, whose lengths are too similar
to tell who is longer in a distance, how could we tell? Without equipment, we
just put those two rules together, and see their difference. This is the mental
procedure we execute when we compare, so in scalar case, $x &gt; y$ means $x - y
&gt; 0$; in vector case, $x \succeq y$ means $x - y \succeq 0$.</p>

<p>What is special about $x - y$?</p>

<p>If we take it abstractly, $x - y$ means how much better when $x$ is compared to
$y$. How we define inequality is determined by how we measure the &#8220;better’’
amount, so the measurement makes sense. For scalar it is just their value
difference. But for vector we cannot judge when department $x$ has higher floor
number than $y$ but smaller square meters. The solution to this dilemma is to
only compare the case where the comparison makes sense. Mathematically, it
means we only compare $x, y$ when $x, y$ stay in a domain, which depends on
both $x, y$, where the comparison makes sense. If $x - y$ is in the domain, we
compare, otherwise, do not bother.</p>

<p>Though we only convert the comparison of $x, y$ to whether $x - y$ belongs to a
domain. We do not know what the domain looks like yet. We want to reach the
conclusion the domain is a pointed cone, or in another name, proper cone.</p>

<p>The next task is to figure out the properties of the domain. What properties do
we want? Let’s list them:</p>

<ol>
  <li>Comparison is about direction and it does not matter if $x$ is a hundred
bigger than $y$, or one hundred percent bigger.</li>
  <li>Strict inequality should exist.</li>
  <li>Difference could be added together: If $x$ is better than $y$, $y$ is better
tan $z$, we should have $x$ is better than $z$.</li>
  <li>Limitation stays in the domain: slightly better is still better.</li>
</ol>

<p>Denoting the domain $K$, those four are translated to</p>

<ol>
  <li>$K$ is a pointed cone, i.e., if $u \in K$ and $-u \in K$, then $u = 0$; for
any $x \in K$ and $\alpha &gt; 0$, we have $\alpha u \in K$.</li>
  <li>$K$ has non-empty interior.</li>
  <li>$K$ is non-empty and closed under addition; i.e., for any $x, y \in K$, $x +
y \in K$.</li>
  <li>$K$ is closed.</li>
</ol>

<h2 id="dual-cone">Dual Cone</h2>

<p>Dual cone is about giving each dimension of an object a linear measurement of
weight, or more intuitively a price, so:</p>

<ol>
  <li>A integrated &#8220;betterness’’ could be quantified for all pairs instead of
just $K$, a restricted set of comparable pairs.</li>
  <li>consequently could lead to dual problem, which is a upper and lower bound by
linear approximation.</li>
</ol>

<p>Due to time limitation, I will just write those for now. Some pictures and
example of minimum elements and minimal elements from <em>Chapter 2, Convex
Optimization Bloyd</em> could be helpful.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Semidefinite Programming]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/11/15/notes-on-semidefinite-programming/"/>
    <updated>2015-11-15T21:23:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/11/15/notes-on-semidefinite-programming</id>
    <content type="html"><![CDATA[<p>Yet another note on math.</p>

<!-- more -->

<p>I understand SDP as a relaxation techniques that explode the problem to a
higher dimension space with a symmetrical matrix special structure. The
original problem corresponds to a manifold in the new higher dimensional
space. So we are allowed to approach problems in direction that is not possible
in the low dimensional space. An illustration idea could be the worm hole idea
in physics. Another benefits could be to linearize original non-linear
problem. In the simplest case, $x$ gets mapped to $X = xx^{T}$, then relaxed to
$X \in S_{+}$. It just puts the second order terms of $x$ in the
matrix. Similar things could be done in for higher order terms, and finally,
any order polynomial could be achieved in some way.</p>

<strike>
I am still not thinking very clearly how operator could be put into the context
since here matrices are just a special way to put a really long
vector. Frobenius inner product is just normal inner product. The power of SDP
comes from infinite number of quadratic constraints enforced by the PD
condition.
</strike>

<p><br /></p>

<strike>
The connection could possibly be this way. It is inspired by how matrix
derivative is defined using Frobenius norm. Though operator&#8217;s nature comes from
it spectral structure, it is still made by $m \times n$ numbers put in a
special way. So a corresponding change in the corresponding position means
something. By measuring the norm, and inner product in this way, we are somehow
connected to the spectral structure of the matrix. By adding different matrices
together this way, we are somehow combining different operations, though I have
not learned any applications that matches this exactly. An perfect example of
this is the perturbation of a matrix. The perturbation in each dimension is
added on the face value(the number displayed on the entries of the matrix), so
it natural to think regardless of the spectral structure of the matrix,
whatever it may be normalized then or by changed by some other operations, the
face value change is how we perceive physical beings.
</strike>

<p><br /></p>

<p>The structure of <script type="math/tex">S_{+}</script> is defined in the exactly same way with vector
space. Its inner product is Frobenius inner product, which just vectorize
matrices and calculate normal vector inner product. The difference is the
symmetric constraints and positive semi-definite constraints.</p>

<p>But what is the connection between seeing matrix as a vector and as an operator?</p>

<p>Such structure does not connect to spectral properties of a symmetric
matrix. Frobenius norm also is not related to the semantics of an operator —
transform.</p>

<p>The intuition of the PSD matrix is to transform a vector to another vector
within the positive dual cone of the original vector. (I guess this is the
definition that could extended to asymmetric matrices, taking rotation matrix
within 90 degree as an example).</p>

<p>Spectral properties of symmetric matrices mean if we choose the bases right,
matrix multiplication is just component-wise scaling.</p>

<p>Those are illustrating pictures coming from linear algebra. But in the general
form, matrix multiplication $Ax$ works as <script type="math/tex">a_{i}^{T}x, i = 1 \ldots n</script>, where
<script type="math/tex">a_{i}^{T}</script> is the ith row of $A$, not as <script type="math/tex">\sum\limits_{i}a_{i}x_i</script>, where
<script type="math/tex">a_{i}</script> is ith column of $A$, which is the picture from linear algebra.</p>

<p>The transform is formed by stacking <script type="math/tex">n</script> linear hyperplanes, or linear
functionals together. Since linear functional is in the dual space <script type="math/tex">X^{*}</script> of
$X$, and also in finite dimensional case <script type="math/tex">X^{*}</script> equivalent to $X$, this is
the reason we could get useful relations in $X$.</p>

<p>In this case, $A - B$ calculates the difference in the linear functionals of
$A, B$. $A \bullet B$ calculates a sum value of inner products of all
functionals of $A, B$. Inner product is the way we measure difference between
hyperplanes.</p>

<p>Now, from the above sense, the structure defined before is for the matrix
itself, but not for how it could transform a vector.</p>

<p>A last note on how to solve SDP.</p>

<p>SDP is solved using interior point methods, whose intuitive is to guide the
optimization by the objective function, using gradient and hessian
information. To make sure the point stay feasible when optimizing, a term $-log
det|A|$ is added to the objective function, who will ensure $X \succeq \mu I$
for some $\mu$, meaning it is positive definite.</p>

<p>This is a very beautiful idea. Actually it is what I was looking for in
non-linear optimization so that the boundary information not only guide
optimization only when the point is near the boundary but also when it is far
in the boundary.</p>

<strike>More thinking may be added when I learned more about SDP, matrix algebra and
matrix calculus.</strike>

<p><br /></p>

<p>I guess this note is roughly finalized.</p>

<p>Updated on Dec 18th, 2015</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Note On Compiling Tensorflow]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/11/13/note-on-compiling-tensorflow/"/>
    <updated>2015-11-13T11:35:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/11/13/note-on-compiling-tensorflow</id>
    <content type="html"><![CDATA[<p>A note on setting library path for cudnn when compiling tensorflow, in case I
forget it next time.</p>

<!-- more -->

<p>Directly providing the library path to cudnn, which has the following directory
structure,</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">include  lib
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>does not work.</p>

<p>According to the error message, it seems that the configuration tries to find
header file and shared libraries directly under the provided folder, thus this
time, I made another folder specifically for tensorflow — it is not in
<code>LD_LIBRARY</code>, so other programs do not use it — then the compilation works.</p>

<p>Since I just want to make sure I could get through the compilation process to
make sure if in case there are features not provided in the official binary, I
have a way to use it, I settled for now. But since the default folder is just
where CUDA lives, I guess if I made the directory structure of CUDNN the same
as CUDA, which I just need to add a symbol link <code>lib64</code> to <code>lib</code>, the
compilation may still work.</p>

<p>So this is the note.</p>

<hr />

<p>Updated on Jan 7, 2016</p>

<p>Adding a symbol link <code>lib64</code> to <code>lib</code> under where cudnn lives does work. The
folder structure is the one mentioned above.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Linear Programming, Duality of LP and Operator, a Functional Analysis Point of view]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view/"/>
    <updated>2015-10-18T12:39:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view</id>
    <content type="html"><![CDATA[<p>Yet another note on Math, about visualization of Linear Programming, some
thoughts on Operator and Adjoin.</p>

<!-- more -->

<h2 id="dual-map-and-adjoin-operator">Dual Map and Adjoin Operator</h2>

<p>Denote the matrix of a linear mapping $T$ corresponds to natural bases as $A$,
then $A^{T}$ is the matrix corresponds to the dual map of $T$. More
specifically, if the bases of the two space, the original space and its dual,
are orthogonal respectively, $A^{T}$ is the adjoin operator of $A$. Now suppose
$A \in R^{M \times N}$.</p>

<p>For a linear programming problem, the constraints are $Ax \leq b$, intuitively,
we understand it as a polyhedron, the intersection of finite number of
halfspaces.</p>

<p>We could also understand it from linear mapping and dual bases point of view.</p>

<p>$A$ maps points from $R^{N}$ to $R^{M}$. A column of $A$ corresponds to the
coordinates of the an natural basis of $R^{N}$ in the new $R^{M}$, meanwhile,
a column of $A^{T}$, which is a row of $A$, corresponds to the coordinates of a
dual basis of a natural basis of $R^{M}$ in $R^{N}$ if it is transformed by
$A^{T}$.</p>

<p>So a row $\vec{a_{i}}$ of $A$ is a $A^{T}$ transformed dual basis that
corresponds to one natural basis in $R^{M}$, which corresponds to a hyperplane
in $R^{N}$, also an element of the dual space of $R^{N}$.</p>

<h2 id="on-the-feasible-region-of-linear-programming">On the Feasible Region of Linear Programming</h2>

<p>Now we try to see whether we could get a picture from the above concepts.</p>

<h3 id="a-previous-possibly-wrong-attempt">A Previous Possibly Wrong Attempt</h3>

<p>This is the first visualization I have tried. It turns out not that right after
I discussed the idea with my optimization course teacher.</p>

<p>From this perspective, the polyhedron in $R^{N}$ is a squashed(or enlarged)
quadrant of $R^{M}$, which is the intersection of hyperplanes whose normals are
natural bases of $R^{M}$ — the hyperplanes only need to pass a constant
value, probably corresponding dimension of $b$(have not thought very clearly),
do not necessarily pass origin point.</p>

<p>If $M &gt; N$, $R^{M}$ gets squashed, and there are much freedom in the way how
the space gets squashed, which is analog with we have more equations than
variables, so it is easier to get solutions. Otherwise, $R^{M}$ gets enlarged,
however, no matter how the enlarging is done, it still remains as a low
dimensional space in the high dimensional space, thus the freedom is limited,
which is analog with more variables than equations, so it is harder or
impossible to get solutions.</p>

<h3 id="reason-in-the-standard-form">Reason In the Standard Form</h3>

<p>If we convert the problem to the standard form of linear programming, we could
see that the dimension or the original space is usually larger than the dual
space, otherwise, the feasible region could be easily empty. In this case, the
argument, that the dual space gets squashed, does not hold. But a different
picture turns out immediately.</p>

<p>Intuitively, which means it is not necessarily right, as long the objective
function does not parallel with one of the constraints, the solution to LP is a
basic solution, otherwise, we could move the point along one of the constraint
to change the objective value. Now we only discuss basic solutions.</p>

<p>For every basic solution, we have $N$ constraints being active. Note that $N$
is the dimension of the dual space. Thus, each set of $N$ active constraints
corresponds to a version of dual space that gets mapped from the standard basis
of dual space by those columns of $A^{T}$. More specifically, each active
hyperplane in the $R^{M}$ is a basis in the dual space $R^{N}$.</p>

<h2 id="on-duality-of-linear-programming">On Duality of Linear Programming</h2>

<p>This leads to an intuition about how on earth the dual problem of LP is
conceived in the first place, and makes its extension to conic programming
clear.</p>

<p>From my understanding, the dual in Optimization is to find a lower bound of the
distance in the original space in its dual space, being it the shortest
distance from a point to a subspace, from a point to a closed convex set or a
convex epigraph to a concave epigraph of functions. Those three examples, which
is three types of dual in infinite dimensional optimization try to find a
linear approximate lower bound for a normally non-linear objective
function. Their dual problems are usually some kind of LP, or other forms could
be easily solved. But for LP, all parts in the problem is already linear. From
this line of thinking, the dual of LP is to find a lower bound in the dual
space of $Ax$. How could this be? The key lies in the adjoin operator.</p>

<p>The standard form of LP is</p>

<script type="math/tex; mode=display">
\max\ c^{T}\\
Ax = b\\
x \geq 0
</script>

<p>We have an adjoin relation</p>

<script type="math/tex; mode=display">
\langle Ax, y\rangle = \langle x, A^{T}y \rangle\\
\langle b, y\rangle = \langle x, A^{T}y\rangle\\ 
\langle b, y\rangle \leq \langle x, c\rangle
</script>

<p>The last inequality holds because we have $x \geq 0$.</p>

<p>Conic programming just extends the meaning of $\geq$ and inner product.</p>

<h2 id="the-algebraic-nature-of-operator">The Algebraic Nature of Operator</h2>

<p>All of the thinking about originates from the way I learn Math. I want to get a
picture from the symbols and relationships. Up to now, all concepts I know
could be put into a 2D or 3D system with or without coordinates in my mind,
being it topology space, metric space, Banach space, inner product space,
Hilbert space, dual space, affine set, convex set, cone, measure, probability
measure and so on. Dual in optimization is a new concept, and I want to put it
somewhere in the system. In about 4 days, I almost finished the book
<em>Optimization by Vector Space Method</em>, which is about infinite dimensional
optimization, and learned the three kinds of duals mentioned before. I indeed
came up with similar visualization tricks mentioned in the book before I read
them. But in the section, some words rang a bell, after a few days I read them
and pondered on the meaning of adjoin, the relation between the $Ax$ and $X$.</p>

<p>At the beginning of the Chapter Six</p>

<blockquote>
  <p>Because it is difficult to obtain a simple geometric representation of an
arbitrary linear operator, the material in this chapter tends to be somewhat
more algebraic in character than that of other chapters. Effort is made,
however, to extend some of the geometric ideas used for the study of linear
functionals to general linear operators and also to interpret adjoints in
terms of relations among hyperplanes.</p>
</blockquote>

<p>The point of an operator $A$ is to transform a point in $X$ to a new space for
some purpose. It is not clear to see why it helps in the general form, but
think Fourier Transform, which is to transform a point in the signal space to
the frequency space. Boom, science advanced. In retrospect, do we need to keep
a picture in the natural basis of signal space when considering the frequency
space? No. In some research paper I read, for instance, scattering transform,
the whole is to make sure the new space has good properties.</p>

<p>So for operators, the most important is to enforce regularities to make sure
there is good properties in the new space, which is algebraic in nature instead
of geometrical.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Teaching and Learning]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/09/21/on-teaching-and-learning/"/>
    <updated>2015-09-21T20:37:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/09/21/on-teaching-and-learning</id>
    <content type="html"><![CDATA[<p>Some notes on teaching and learning.</p>

<!-- more -->

<p>The way we teach reflects the way we learn and the way we communicate. Teaching
gives me another perspective on how may I could be taught or just, learn from
others. Such experience has led me a long way to realize my deficiency.</p>

<p>I attended a workshop on teaching today and want to note down something
learned. It is the first time I got systematical training on how to teaching,
which is equivalent to how to learn.</p>

<p>The following stuffs come from the handout in the workshop.</p>

<h3 id="seven-research-based-principles-for-smart-teaching">Seven Research-based Principles for Smart Teaching</h3>

<p>There are seven principles in teaching, which also means in learning.</p>

<ol>
  <li>prior knowledge</li>
  <li>structuring knowledge</li>
  <li>motivation</li>
  <li>self-awareness on the one’s situation</li>
  <li>synthesis of different parts of acquired knowledge </li>
  <li>feedback</li>
  <li>emotional feeling, social and intellectual climate</li>
</ol>

<p>This is a systematical summary of elements that involves in the learning
process. Prior knowledge is what you already have; structuring knowledge is how
you condense your knowledge and build inner connection between different
separate parts; motivation is power house behind all your efforts;
self-awareness is about a clear awareness of one’s current situation — what
one have known, what one do not know, what one know one do not know; synthesis
is where something new could happen; feedback is how people get to know their
mistakes, so they could fix them, and learn new things; emotional stability and
social support is very important for long term learning.</p>

<p>After some thinking, it seems that nothing is really new for me, but the
summary is helpful.</p>

<h3 id="on-motivation">On Motivation</h3>

<p>Another important part on teaching and learning discussed is what motivate
people to do better on tasks that need cognitive thinking.</p>

<p>I long have known only money is not enough. Today I got a very good summary:</p>

<blockquote>
  <p>Pay people enough to take the issue of money off the table.</p>
</blockquote>

<p>The factors that motivate to do great work is autonomy, mastery and
purpose. There should many materials online talking on this given this is a
research output from scientists.</p>

<h3 id="three-skills-on-communication">Three Skills On Communication</h3>

<ol>
  <li>Storytelling</li>
  <li>Contextualizing</li>
  <li>Sketching</li>
</ol>

<p>All those are kind of one thing. People learn from examples, so we need
storytelling; people learn from examples that they know the most, so we need
contextualize the examples with their experience; people learn bit by bit,
dynamically, so we need to present it bit by bit, like sketching a picture.</p>

<p>It is easier said that done. Practice is important.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Linux Laptop Battery]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/09/07/on-linux-laptop-battery/"/>
    <updated>2015-09-07T14:15:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/09/07/on-linux-laptop-battery</id>
    <content type="html"><![CDATA[<p>I would like to note down the how to prolong battery life, in term of both
hours and years.</p>

<p>Roughly, there are three tricks to extend battery time, <code>laptop-mode</code>,
<code>powertop</code> and changing screen lightness, and one trick to extend life,
stopping battery charging according to thresholds. The last trick is only
applicable to Thinkpad.</p>

<!-- more -->

<h2 id="extending-battery-time">Extending Battery Time</h2>

<p>I have three tricks for extending the battery time, <code>laptop_mode</code>, <code>powertop</code>
and setting the lightness of the screen. The exact number is forgotten, but
roughly if the original time of the laptop is around 4 hours, enabling
<code>laptop_mode</code> will get me one extra, <code>powertop</code> two extra and lightness one
extra.</p>

<p>Normally, people are told to use one battery management programs considering
that different programs may conflict, but I found <code>laptop_mode</code> works well with
<code>powertop</code>, a long time ago. So I settled with it.</p>

<h3 id="laptopmode">laptop_mode</h3>

<p><a href="http://www.samwel.tk/laptop_mode/">laptop_mode</a> is about:</p>

<blockquote>
  <p>Laptop mode is a kernel “mode” that allows you to extend the battery life of
your laptop. It does this by making disk write activity “bursty”, so that only
reads of uncached data result in a disk spinup. It causes a significant
improvement in battery life (for usage patterns that allow it).</p>
</blockquote>

<p>There are configuration files for it, but I did not try to use them.</p>

<h3 id="powertop">powertop</h3>

<p><a href="https://01.org/powertop">powertop</a> is from Intel. It could tune a number of
options to save power usage. For details, refer to
<a href="https://wiki.archlinux.org/index.php/Powertop">here</a>. I will only note down
normal usage scenario.</p>

<p>After first installation, we need to run:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">sudo powertop --calibrate
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>It will take several minutes to finish, and the screen could be dark for tens
of seconds. So do not panic when this happens. Also, do not touch your laptop
when powertop is calibrating. After this, you could get power consumption in
Watt and estimated remaining time in <code>powertop</code>. If you skip this, <code>powertop</code>
will still work, but not those two information.</p>

<p>Start <code>powertop</code>, in the <code>tunable</code> tab, you could see a number of tunable
options to improve your battery time. Make all the “Bad” to “Good” will save a
lot of power consumption. However, the change is lost after you reboot. To make
it permanent, do the following.</p>

<p>Run,</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">sudo powertop --html
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>You will get a html version report under current directory. Go to the tunable
page, you could see the command to tune, similar with the following:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="c"># Those following are the settings from powertop to save power.</span>
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;1500&#39;</span> &gt; <span class="s1">&#39;/proc/sys/vm/dirty_writeback_centisecs&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;0&#39;</span> &gt; <span class="s1">&#39;/proc/sys/kernel/nmi_watchdog&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;min_power&#39;</span> &gt; <span class="s1">&#39;/sys/class/scsi_host/host1/link_power_management_policy&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;min_power&#39;</span> &gt; <span class="s1">&#39;/sys/class/scsi_host/host2/link_power_management_policy&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;min_power&#39;</span> &gt; <span class="s1">&#39;/sys/class/scsi_host/host0/link_power_management_policy&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;auto&#39;</span> &gt; <span class="s1">&#39;/sys/bus/usb/devices/1-6/power/control&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;auto&#39;</span> &gt; <span class="s1">&#39;/sys/bus/pci/devices/0000:00:16.0/power/control&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;auto&#39;</span> &gt; <span class="s1">&#39;/sys/bus/pci/devices/0000:02:00.0/power/control&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;auto&#39;</span> &gt; <span class="s1">&#39;/sys/bus/pci/devices/0000:00:1f.3/power/control&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;auto&#39;</span> &gt; <span class="s1">&#39;/sys/bus/pci/devices/0000:00:1f.2/power/control&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;auto&#39;</span> &gt; <span class="s1">&#39;/sys/bus/pci/devices/0000:00:1f.6/power/control&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;auto&#39;</span> &gt; <span class="s1">&#39;/sys/bus/pci/devices/0000:00:1f.0/power/control&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;auto&#39;</span> &gt; <span class="s1">&#39;/sys/bus/pci/devices/0000:00:1d.0/power/control&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;auto&#39;</span> &gt; <span class="s1">&#39;/sys/bus/pci/devices/0000:00:1c.1/power/control&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;auto&#39;</span> &gt; <span class="s1">&#39;/sys/bus/pci/devices/0000:00:1c.0/power/control&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;auto&#39;</span> &gt; <span class="s1">&#39;/sys/bus/pci/devices/0000:00:14.0/power/control&#39;</span>;
</span><span class="line"><span class="nb">echo</span> <span class="s1">&#39;auto&#39;</span> &gt; <span class="s1">&#39;/sys/bus/pci/devices/0000:00:02.0/power/control&#39;</span>;
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Copy all those lines, or only the ones you want to tune to <code>/etc/rc.local</code>,
then the change will be permanent.</p>

<h3 id="set-default-lightness">Set Default Lightness</h3>

<p>As least for me, no new packages are needed. Just put the following line in
<code>/etc/rc.local</code> is OK. Given the difference in hardwares, the <code>intel_backlight</code>
may be something else.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="c"># Set default brightness. The maximum value is 852. The value 350 here is hard tuned.</span>
</span><span class="line"><span class="nb">echo </span>350 &gt; /sys/class/backlight/intel_backlight/brightness
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="extending-battery-life">Extending Battery Life</h2>

<p>I am delighted to find out that Thinkpad has such a good support for
Linux. Lenovo has a good mechanism to extend battery life on Windows. It will
stop charging the battery if the battery level has exceeded some threshold,
which would prolong the battery life given the properties of Lithium battery.</p>

<p>Thinkpad still support it if you are using Linux. The solution depends on the
hardware of your Thinkpad. If you are installing on a recent Thinkpad that has
an Ivy Bridge or newer processor (X230, T430, T530, etc.), like me, you could
use a utility named <a href="http://www.thinkwiki.org/wiki/Tpacpi-bat">tpacpi-bat</a>.</p>

<p>If you are using an old Thinkpad, go
<a href="http://www.thinkwiki.org/wiki/Tp_smapi">there</a> for a solution.</p>

<h3 id="install-acpi-call">Install <code>acpi-call</code></h3>

<p>I use Ubuntu 14.04, so the procedure to install <code>tpacpi-bat</code> on Ubuntu will be
described.</p>

<p><code>tpacpi-bat</code> has a <a href="https://launchpad.net/~morgwai/+archive/ubuntu/tpbat">ppa</a>
for Debian based system. Just add the ppa to your source:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">sudo sudo add-apt-repository PPA_NAME
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><code>tpacpi-bat</code> depends on <code>acpi-call</code>, which is part of <code>acpi-call-dkms</code>. So we
need to install <code>acpi-call-dkms</code> first. It is also part of the ppa.</p>

<p>The main reason that I write this note is that the package has a bug. This bug
is widely
<a href="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=762281">discovered</a> almost
one year ago, it is strange why it is not fixed yet. After setting up the ppa,
first try installing <code>acpi-call-dkms</code>.</p>

<p>The installation will fail with message similar with the following:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">make <span class="nv">KERNELRELEASE</span><span class="o">=</span>3.19.0-26-generic <span class="nv">KVERSION</span><span class="o">=</span>3.19.0-26-generic <span class="nv">KDIR</span><span class="o">=</span>/lib/modules/3.19.0-26-generic/build....<span class="o">(</span>bad <span class="nb">exit </span>status: 2<span class="o">)</span>
</span><span class="line">Error! Bad <span class="k">return </span>status <span class="k">for </span>module build on kernel: 3.19.0-26-generic <span class="o">(</span>x86_64<span class="o">)</span>
</span><span class="line">Consult /var/lib/dkms/acpi_call/1.1.0/build/make.log <span class="k">for </span>more information.
</span><span class="line">dpkg: error processing package acpi-call-dkms <span class="o">(</span>--configure<span class="o">)</span>:
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Have a check at the <code>make.log</code>, the compilation error is:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">Please don<span class="err">&#39;</span>t include &lt;acpi/acpi.h&gt; directly, include &lt;linux/acpi.h&gt; instead
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>So I go to <code>/var/lib/dkms/acpi_call/1.1.0/source/acpi_call.c</code>, where the
installer unpacks the source. I changed <code>&lt;acpi/acpi.h&gt;</code> to <code>&lt;linux/acpi.h&gt;</code>,
then try the installation(wit the same command) again. Wow, the compilation
passed and the installation is successful.</p>

<h3 id="install-tpacpi-bat">Install <code>tpacpi-bat</code></h3>

<p>This step is easy, just normal apt installation.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">sudo apt-get install tpacpi-bat
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="configuration">Configuration</h3>

<p>After installation, now we need to configure the charge threshold of our
battery. The help message of <code>tpacpi-bat</code> does not seem to be very clear for
me. So I will note down how to get and set charging battery thresholds.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="c"># tpacpi-bat get start-threshold primary-battery</span>
</span><span class="line">tpacpi-bat   -g   ST             1
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Thinkpad has one primary battery and one secondary battery. 1 stands for
primary battery and 2 stands for secondary battery.</p>

<p>To set the start threshold for starting charging:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="c"># tpacpi-bat set start-threshold primary-battery   start-threshold</span>
</span><span class="line">tpacpi-bat   -s  ST              1                 40
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>It means the battery won’t charge if the its battery level is higher than 40.</p>

<p>To make the change permanent, add the following lines in <code>/etc/rc.local</code>.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="c"># Primary battery</span>
</span><span class="line">tpacpi-bat -s ST 1 40 <span class="c"># Start threshold</span>
</span><span class="line">tpacpi-bat -s SP 1 80 <span class="c"># Stop threshold</span>
</span><span class="line"><span class="c"># Secondary battery</span>
</span><span class="line">tpacpi-bat -s ST 2 40
</span><span class="line">tpacpi-bat -s SP 2 80
</span></code></pre></td></tr></table></div></figure></notextile></div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Note on texlive from Ubuntu Source]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/09/02/note-on-texlive-from-ubuntu-source/"/>
    <updated>2015-09-02T21:49:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/09/02/note-on-texlive-from-ubuntu-source</id>
    <content type="html"><![CDATA[<p>Some note on the installing LaTeX packages from network.</p>

<!-- more -->

<p>It is very convenient to install packages using <code>tlmgr</code>. However, sometimes
the mirror you are trying to connect would be extremely slow, which makes it
feel like tlmgr has stuck.</p>

<p>If you have not been deal with texlive installation for a long time, like me
today, the first thoughts stuck me is that there is something wrong with
texlive installation given that I was doing a fresh installation on a new
laptop I did not familiar. Only after about a hour’s debugging, the idea that
maybe it is the network issue struck me.</p>

<p>Here I want to note down some points about how tlmgr choose its mirror servers,
so I won’t be figuring out the same problem next time.</p>

<p>By default, <code>tlmgr</code>’s default repository url is a multiplexor, which will
choose a mirror in the closest network proximity to your location. See the
right pane of this <a href="ctan.org/mirrors">page</a>.</p>

<p>To set such behavior of <code>tlmgr</code>, the command is like this:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">tlmgr option repository ctan
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>where <code>ctan</code> is the nickname for tlmgr for the multiplexor mirror.</p>

<p>Today the multiplexor gave me a server that is not working my LAN
network. Luckily that I get a workable mirror from my PC and use that to solve
the network problem.</p>

<p>The url of a mirror is like this:
<code>http://ftp.yzu.edu.tw/CTAN/systems/texlive/tlnet</code>. The example here is the
mirror that works for me this time.</p>

<p><em>NOTE FOR ME</em>:</p>

<p>I have set this mirror as default for my laptop.</p>
]]></content>
  </entry>
  
</feed>
