<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Note | SHAWN LEE]]></title>
  <link href="http://shawnLeeZX.github.io/blog/categories/note/atom.xml" rel="self"/>
  <link href="http://shawnLeeZX.github.io/"/>
  <updated>2016-01-07T22:13:00+08:00</updated>
  <id>http://shawnLeeZX.github.io/</id>
  <author>
    <name><![CDATA[Shawn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setting up Multi-Monitor on Gnome Ubuntu 14.04 with NVIDIA Card]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2016/01/06/setting-up-multi-monitor-on-gnome-ubuntu-14-dot-04-with-nvidia-card/"/>
    <updated>2016-01-06T08:16:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2016/01/06/setting-up-multi-monitor-on-gnome-ubuntu-14-dot-04-with-nvidia-card</id>
    <content type="html"><![CDATA[<p>I got an old monitor and spent some time tweaking the dual monitor setting of
my PC in the office. Here I note down</p>

<ol>
  <li>a tweak to make multiple monitor in Gnome functions better</li>
  <li>a hack to let X Windows remember the monitor layout</li>
  <li>and a short comparison between Gnome 3 and KDE Plasma 4.</li>
</ol>

<!-- more -->

<h2 id="dual-monitor-on-gnome">Dual Monitor on Gnome</h2>

<p>I uses <a href="https://www.gnome.org/">Gnome</a> as my desktop environment ever since the
time I started using Linux. So naturally, the desktop I tried this time to
manage multi-monitor support is Gnome. By default, the dual monitor worked
pretty OK. The only one thing that may need a tweak is: by default, Gnome(my
version is 3.9.90), only supports workspace in primary monitor, so when you
switch back and forth between different workspace, the secondary monitor stays
fixed on whatever it is on. This kind of makes the extra monitor many times
less useful. To make the secondary monitor attach to current workspace, so it
moves when you switch workspaces, you need to configure Gnome a little bit, by
the following command</p>

<p><code>bash
gsettings set org.gnome.shell.overrides workspaces-only-on-primary false
</code></p>

<p>One drawback of making such a change is <code>guake</code> will stay on the secondary
window forever. Did not get the time to look into the source code to know why
:(.</p>

<h2 id="physical-layout-of-monitors">Physical Layout of Monitors</h2>

<p>I do not know how much it depends on operating system, drivers and
hardware. The <code>display</code>(the GUI one of Gnome) or <code>nvidia-settings</code>(the CLI one
comes with proprietary driver) forgets the monitor layout after reboot, such as
<code>DVI-I-1</code> should be on the left of <code>DVI-D-0</code>, even if I wrote it in the
<code>xorg.conf</code>.</p>

<p>Though finally I switched the monitors physically, for some other reason, and
made peace with the forgetful display manager, I struggled some time to figure
out how to make the layout right.</p>

<p>The solution I used is to use <code>xrandr</code> after the GUI session has set up by
adding a script containing the following line in the startup application of
Gnome(there is a GUI program called <code>Startup Application</code>, though I do not know
what its CLI version is).</p>

<p><code>
xrandr --output DVI-I-1 --mode 1920x1080 --right-of DVI-D-0
</code></p>

<p>I also have tried to add the above line in <code>.profile</code>, but it seems the layout
setting is decided after <code>.profile</code> is loaded, so it did not work.</p>

<h2 id="a-peak-at-kde">A Peak at KDE</h2>

<p>At the first time I tried to set <code>workspaces-only-on-primary</code> to <code>false</code>, it
did not have any effect, and first all workspaces other than the first one
started to get severe artifact when moving windows around, then even the first
one started to get artifacts. It made the desktop unusable.</p>

<p>After trying to search for an solution in Google for a while, I learned there
seems Gnome do not have any multi-monitor support at all, at least they do not
consider this a feature to be proud of at their website for Gnome 3. So I think
maybe Gnome does not work for Gnome.</p>

<p>Then some video introducing Plasma let me pay some attention to KDE
desktop. The modern design of KDE Plasma 5.5 really impressed me, though in
retrospect I was dazzled by Gnome 3’s new design at the first I saw it. It is
just the case that I have been used to Gnome 3 for such a long time and do not
feel its design at all now.</p>

<p>I installed Plasma from source by</p>

<p><code>bash
sudo apt-get install kubuntu-desktop
</code></p>

<p>This is the first time I started to feel Ubuntu 14.04 is getting old. It only
has Plasma 4.14(the minor version may not be right, but it is 4).</p>

<p>After tinkering it a little bit, though it must be the case I did not get
myself familiar with it, I have the first impression as following:</p>

<ol>
  <li>It seems to have many widgets, which is better than Gnome, whose desktop
widget feels like antiques.</li>
</ol>

<p>Besides that, I did not find an obvious solution to</p>

<ol>
  <li>multiple workspaces, which is available by default in Gnome</li>
  <li>the dash application runner(I am not sure its exact name, just you press the
<code>super</code> or <code>win</code> key in you desktop and it shows up). I tried to install
a app called <code>homerun</code>, but it just adds too much icons in the desktop and
creates too much clutters.</li>
  <li>Obvious, <code>guake</code> does not work here anymore.</li>
</ol>

<p>The above three are really killer app, and cannot not be lacking.</p>

<p>So the first impression is KDE is for the GUI users.</p>

<ol>
  <li>It has a good file manager called <code>Dolphin</code>, whose functionality is better
than <code>Nautilus</code>. But I almost do not use GUI file management since I do it
all in CLI.</li>
  <li>A similar application launcher of dash of Gnome is <code>Krunner</code>. But like
Unity, it not only search for applications, but also search for files, any
opened applications etc, which creates much clutter, though this is a
desktop user wants.</li>
  <li>Again, no easy drop-down terminal like <code>guake</code>.</li>
  <li>It adds too much clutter that may not be useful, compared with a clean wall
paper of Gnome. I think Plasma 5.5 tries to fix this, but I did not have the
chance to try.</li>
</ol>

<p>As having been said, it must be the case I am not familiar with a new desktop
environment, but I guess I will settle with it, because after I was little
frustrated with the KDE desktop, I tried to switch back to Gnome. Suddenly,
everything started to work well! Maybe it is because I installed a lot of
libraries when I was installing KDE, which fixed some problems. But now I feel
current setting, the one I mentioned at the first section of this note,
awesome.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Environment Variable Setup of Linux]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2016/01/05/on-environment-variable-setup-of-linux/"/>
    <updated>2016-01-05T11:28:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2016/01/05/on-environment-variable-setup-of-linux</id>
    <content type="html"><![CDATA[<p>I have spent some time figuring out how Linux sets up its environment variables
for login shell, non-login shell to let Emacs inherits environment variables
and make tmux loads <code>.bashrc</code> and note it here.</p>

<!-- more -->

<h2 id="login-shell">Login Shell</h2>

<p>We start by figuring out the boot process.</p>

<p>For text console,</p>

<ol>
  <li>At the end of boot the mother of all processes <code>init</code> is started. init’s
environment, including PATH, is defined in its source code and cannot be
changed at run time.</li>
  <li><code>init</code> runs the start-up scripts from <code>/etc/init.d</code> depending on the run level
set in <code>/etc/inittab</code>. Since init’s environment is very bare, the scripts
define their required environment variables within themselves.</li>
  <li><code>init</code> starts the text login process that waits for the user to log in. When
the user logs in, the login process checks <code>/etc/passwd</code> to see what shell
should be started for this particular user.</li>
  <li>The shell starts and reads its shell-specific configuration files.
    <ol>
      <li>For Bash, it first reads <code>/etc/profile</code> to get values that are defined
for all users. After reading that file, it looks for <code>~/.bash_profile</code>,
<code>~/.bash_login</code>, and <code>~/.profile</code>, in that order, and reads and executes
commands from the first of these files that exists and is readable.</li>
    </ol>
  </li>
</ol>

<p>For graphical UI,</p>

<ol>
  <li>At the end of booting, the mother of all processes – <code>init</code> – is started.</li>
  <li><code>init</code> runs the start-up scripts from <code>/etc/init.d</code> depending on the run
level set in <code>/etc/inittab</code>. Since <code>init</code>’s environment is very bare, the
scripts define required environment variables within themselves.</li>
  <li>Init starts the GDM display manager, which in turn will start the graphical
login.</li>
  <li>When the user successfully logs in, GDM starts xsession, which reads the
file <code>/etc/gdm/Xsession</code> and with it the environment variables for the
user’s session. The default version of the Xsession file first reads
<code>/etc/profile</code> for global settings and then <code>~/.profile</code> to add the user’s
individual settings.</li>
</ol>

<p>The above boot process is the process to set up environment to login shell. So
if any user specific environment that is need for a graphical program, one
could choose to put it in <code>~/.bash_profile</code>, <code>~/.bash_login</code>, and
<code>~/.profile</code>. I chose to put it in <code>.profile</code>. I actually spent time figuring
this out, so Emacs could inherit environment variables the local libraries I
installed.</p>

<p>As for system wide setup, put it in <code>/etc/profile</code>.</p>

<h2 id="non-login-shell">Non-login Shell</h2>

<p>If a shell is needed after login, the setting should go to the non-login
shell’s, <code>.bashrc</code>. For instance, the terminals created by <code>terminal</code>,
<code>terminator</code> and other terminal programs are non-login shells. This is where I
previously put all my configurations in.</p>

<p>The following comes from <code>man bash</code>’s <em>INVOCATION</em> section.</p>

<pre><code>When bash is invoked as an interactive login shell, or as a non-interactive
shell with the `--login` option, it first reads and executes commands from the
file `/etc/profile`, if that file exists.  After reading that file, it looks
for `~/.bash_profile`, `~/.bash_login`, and `~/.profile`, in that order, and
reads and executes commands from the first one that exists and is readable.
The --noprofile option may be used when the shell is started to inhibit this
behavior.

When a login shell exits, bash reads and executes commands from the file
`~/.bash_logout`, if it exists.

When an interactive shell that is not a login shell is started, bash reads and
executes commands from `/etc/bash.bashrc` and `~/.bashrc`, if these files
exist.
</code></pre>

<h2 id="distribution">Distribution</h2>

<p>Each Linux distribution may tweak files mentioned above, for instance, unset
some variables somewhere, so if you set its value before where it is unset,
your setting will not take effect. Normally, if one’s configuration does not
work, consider go through all previous configurations and understand what they
exactly do in such a distribution.</p>

<h2 id="misc">Misc</h2>

<p>If one uses <code>tmux</code>, the terminal multiplexer, note it creates login shells. So
if one wants <code>.bashrc</code>, especially aliases, to work under <code>tmux</code>, remember to
source it after whatever login shell configuration one may use.</p>

<h2 id="reference">Reference</h2>

<ol>
  <li>https://wiki.debian.org/EnvironmentVariables</li>
  <li>https://help.ubuntu.com/community/EnvironmentVariables</li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Optimization Summary]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/12/16/optimization-summary/"/>
    <updated>2015-12-16T15:06:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/12/16/optimization-summary</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>The semester’s course on Optimization is going to end. This is a summary note
to unify the mathematical picture learned during this period.</p>

<!-- more -->

<h2 id="problem-formulation">Problem Formulation</h2>

<p>In the most general form, an optimization problem is to find an extreme value,
taking minimum for ease of writing, in the following form</p>

<script type="math/tex; mode=display">
\min\ f(X)\\
G(X) \succeq \theta\\
H(X) = \theta
</script>

<p>where $X$ could be a scalar, vector or matrix; $G(X)$ is a stack of inequality
constrains, whose row is one inequality constrain; $\theta$ is zero vector;
$H(X)$ is a stack of equality constrains, whose row is one equality constrain.</p>

<p>To solve the above problem, analysis should be made to properties of $f(X),
G(X)$ and $H(X)$. Geometric intuition is the source of aspiration and algebraic
descriptions or relaxation of geometric intuition is how we could solve the
problem.</p>

<p>The geometric picture of $f(X)$ is contour of its value, whose metaphor is the
contour of the height of a mountain; or just the mountain by seeing how high
its. The first picture is in the domain of $X$, while the second one is in the
$(X, f(X))$, which adds another dimension.</p>

<p>$G(X)$ and $H(X)$ explicitly or implicitly define an sub-area of all possible
$X$. Its picture is harder to visualize, and is the difficulty aspects in
optimization.</p>

<h2 id="domain-type">Domain Type</h2>

<p>Different types of variables has different structure and properties that
enforced on the domain. As an example, Conic Linear Programming is to explore
the structureness to simplify problems.</p>

<p>It does not occur to me clearly for now how possibly the enforced peculiar
structure of Semi-definite programming or Second Order Cone Programming could
have physical meaning. But SDP how take advantage of matrix variable and
inequality is inspiring.</p>

<h2 id="geometry-of-domain">Geometry of Domain</h2>

<p>The work around is to assume only to solve problems who is convex, and call the
remaining problems non-convex, consequently classifying it as hard.</p>

<p>It seems this is rather a restrict and side step the difficult but important
problems. This was the perception I had when I first heard about Optimization
years ago. But the actually it is not after really struggling through it.</p>

<p>Convexity of a set is the atom to describe geometry of the whole set in the
following sense: if two points are in the set, then we want any points between
them are also in the set. Normally, points have some physical meaning in real
world problem, not some bizarre mathematical fabrication. In physical world,
extremity breaks thing, but not their intermediate value. Thus it is a good
model unless we discover some bizarre phenomena(which probably already exists
but I do not know).</p>

<p>Convexity and concavity of function are the only two ways how function could
vary locally. The really mattered properties of convexity is non-decreasing, so
concavity is non-increasing. They get named quasa-convex for convexity part.</p>

<p>If we could analyzed the atoms how sets compose a bigger set and how functions
composes a ``larger’’ function, we begin to get a look into the whole picture.</p>

<p>Current situation of optimization is local geometry has been well studied, but
a way to generalize to the global geometry lacks. So I guess it is not the
a good understanding of convexity is the first step to understand more complex
problems.</p>

<h2 id="dimensionality-of-domain">Dimensionality of Domain</h2>

<p>This section is about the concept of <em>affine</em>.</p>

<p>The motivation of ``affine’’ is to create a term that could be used to describe
the dimension of domain. So essentially, an affine set is linear variety or the
whole space. Linear variety is a concept. For example, a linear variety in 3D
space would be a line, a plane.</p>

<h2 id="unifying-optimization-duality-with-functional-analysis">Unifying Optimization Duality with Functional Analysis</h2>

<p>If we start from a Linear Programming(LP), which is the following problem</p>

<script type="math/tex; mode=display">
\min\ c^{T}x\\
Ax = b
x \geq 0
</script>

<p>and its dual</p>

<script type="math/tex; mode=display">
\max\ b^{T}y\\
A^{T}y \leq c
</script>

<p>From function analysis point of view, those two problems involves four space,
space of $x$(we call it primal space for convenience from now), dual space of
primal space, space of $y$(we call it transformed space from now), dual space
of transformed space. Duality of LP is the characterization of relationship
between those four spaces.</p>

<p>If we upgrade this view to non-linear case,</p>

<script type="math/tex; mode=display">
\min\ f(X)\\
G(X) \succeq \theta\\
H(X) = \theta
</script>

<p>we see $G(X)$ and $H(X)$ are also a transform, but not linear. But similar idea
should also apply. This is the idea of Lagrange multiplier and Lagrange
duality.</p>

<p>The idea is to analyze in the transformed space $(f(X), Z)$, where $Z$ is the
whose variables are $G(X), H(X)$(in proof involved with Lagrange duality, we
break $H(X)$ to two inequalities). $Z$ and its dual <script type="math/tex">Z^{*}</script> centralize all
duality in optimization.</p>

<p>The hard part is to analyze what the space $Z$ is like. First, different types
of variables have different properties and work with different inequalities,
such as vector and vector inequality, matrix and matrix inequality. Second,
what the transformed space is like after mapping is hard to deal with.</p>

<p>The key properties of $Z$ is whether it has an interior point and is convex,
which determines whether we could find a separating hyperplane, an element in
dual space.</p>

<p>Farkas lemma is just a special case where we could explicitly know what the
transformed space looks like.</p>

<h2 id="on-cone-and-dual-cone">On Cone and Dual Cone</h2>

<p>This section is about cone and dual cone. One more example could be seen in the
next section.</p>

<p>Cone is about direction. It is hard to describe it informally without concrete
examples. Pointed cone identifies inequality relations, which is summarized in
this <a href="/blog/2015/11/28/cone-dual-cone-and-generalized-inequalities/">note</a>.</p>

<p>Dual cone is about the direction that at least slightly the same with primal
cone, so in this direction values also could increase, or decrease if it is a
negative dual cone. Dual cone is a set in dual space of primal space, whose
elements are normals of hyperplane. Remember a normal of a hyperplane is about
the direction where the values goes up(or down depending on definition).</p>

<h2 id="variational-idea-kkt-condition-fritz-john-condition">Variational Idea, KKT Condition, Fritz-John Condition</h2>

<p>Existence of Lagrange multiplier is the zero order properties of a constrained
optimal point. By considering the first derivative, we could get how gradients
at the optimal point are related to each other.</p>

<p>Equality constrain in the limit is to carve subspace out, so all things should
happen in this subspace. Gradients of inequality constrains determine whether
the we could still have descent directions, which is Fritz-John or KKT
condition. The idea is if negative dual cone of positive range of gradients of
inequality constrains do not intersect with negative dual cone of $\nabla
f(x)$, then we reach a constrained minimum. The regularity conditions of KKT
just means the gradients of inequality constrains do not conflict with each
other, which if happens would only leave a feasible set that has nothing to do
with $f(x)$, which is the case the dual variable associated with $\nabla f(x)$
is zero in Fritz-John optimality condition.</p>

<h2 id="optimization-algorithms">Optimization Algorithms</h2>

<p>Few has been explored in this area for now.</p>

<p>The idea is to explore the structures of the four spaces mentioned, and the
target function to find the extreme values and corresponding solutions, either
globally, for instance using the simplex method, or locally, using gradient
based methods.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On KKT Condition and Optimality Condition of Conic Linear Programming]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/12/15/on-kkt-condition-and-optimality-condition-of-conic-linear-programming/"/>
    <updated>2015-12-15T20:18:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/12/15/on-kkt-condition-and-optimality-condition-of-conic-linear-programming</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>In LP and SDP, the complementary slackness are written as</p>

<script type="math/tex; mode=display">
(c - A^{T}y)^{T}x = 0\\
(C -\sum\limits_{i}y_{i}A_{i}) \bullet X = 0
</script>

<p>While KKT condition’s complementary slackness is</p>

<script type="math/tex; mode=display">
v^{T}g(x) = 0
</script>

<p>It is hard to connect them at first glance, since dual variable $v$ is dual to
the space of $g(x)$, which is not necessarily in the same space with $x$.
Especially when $x$ is the big matrix $X$ in SDP, $g(X)$ lies in vector space
instead of matrix space.</p>

<p>This is the note for the how they are connected.</p>

<!-- more -->

<h2 id="linear-programming">Linear Programming</h2>

<p>For linear programming, we write down its Lagrange</p>

<script type="math/tex; mode=display">
c^{T}x + y^{T}(b - Ax) - s^{T}x
</script>

<p>its gradient is $c - A^{T}y - s$</p>

<p>so its KKT condition is</p>

<script type="math/tex; mode=display">
c - A^{T}y - s = 0 \text{ Stationary point}\\
x \geq 0, Ax = b \text{ Primal feasibility}\\
s^{T}x = 0, s \geq 0 \text{ Complementary slackness}
</script>

<p>Now we see $(c - A^{T}y)^{T}x = 0$ is the combination of stationary point
condition and complementary slackness condition. The special structure occurs
here because</p>

<ol>
  <li>In linear problem, analytic solution of equation is possible.</li>
  <li>$g(x)$ is just $x$, so their spaces are the same.</li>
</ol>

<p>Note when LP reaches its optimal value, we always have $c^{T}x = b^{T}y =
(A^{T}y)^{T}x$, but there is a difference $s$ between $c$ and $A^{T}y$. Here we
see the specialty of LP. When one dimension of $s$ is not zero, we have
correspond dimension of $x$ to be zero. Due to the linearity of $c^{T}x$, the
zero directly reflects on the objective value, which makes correspond
dimension’s contribution to zero. Now it is clearer why duality gap is always
zero.</p>

<h2 id="semi-definite-programming">Semi-Definite Programming</h2>

<p>For SDP, if we want to draw analog with LP, the Lagrange may look like</p>

<script type="math/tex; mode=display">
C \bullet X + \sum\limits_{i}y_i(b_i - A_i \bullet X) - S \bullet X
</script>

<p>its gradient is <script type="math/tex">C - \sum\limits_{i}y_i A_i - S</script>.</p>

<p>So its KKT condition is</p>

<script type="math/tex; mode=display">
C - \sum\limits_{i}y_i A_i - S = 0\\
x \succeq 0, A_i x = b_i\\
S \bullet X = 0
</script>

<p>which is almost the same in format with that of LP.</p>

<p>But note</p>

<ol>
  <li>KKT does not work wit matrix inequality yet, so $S \bullet X$ cannot be
moved up to the Lagrange function. Though it does suggest some form of
KKT condition that could deal with matrix inequality may exist.</li>
  <li>The relationship is not linear anymore, both in $X \succeq 0$ and $C \bullet
X$ because the symmetric constrain enforced on $X$, which makes $X$ not just
a matrix form vector.</li>
  <li>$g(X) \succeq 0$ maps matrix to matrix space, so actually complementary
slackness also should be in the space of matrix, and in form of matrix inner
product.</li>
</ol>

<p>So there should be a duality gap normally.</p>

<h2 id="last-note-on-dual-variables">Last Note on Dual Variables</h2>

<p>From above, we see the reason we have matrix inner product style complementary
slackness is due to we are using dual variable in matrix space. It is
interesting to see how general dual variable or linear hyperplane is on solving
problem on any space, though I have not learned any proof of Lagrange on spaces
other than Euclidean space yet.</p>

<h2 id="last-note-on-linearity">Last Note on Linearity</h2>

<p>In conic linear programming,</p>

<ol>
  <li>when putting constrains in Lagrange function, it directly interacts with
objective function, meaning variable $x$ could be separated out.</li>
  <li>The specialty of $x \geq 0$ makes its dual variable the slack variable.</li>
  <li>Linearity makes dual function $\inf \ldots$ analytic solvable, so we only
see $\max$ in the dual problem.</li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Duality in Optimization]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/12/14/on-duality-in-optimization/"/>
    <updated>2015-12-14T21:09:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/12/14/on-duality-in-optimization</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>A more general picture of duality in optimization emerges itself. Some time is
still needed to unify all duality in optimization, but for now I summarize a
draft version of it here using duality of linear programming as an example.</p>

<!-- more -->

<h2 id="intuition-of-lagrange-multiplier-from-functional-analysis">Intuition of Lagrange Multiplier From Functional Analysis</h2>

<p>In its most general form, optimization problem is</p>

<script type="math/tex; mode=display">
\max\ f(x)\\
G(x) \geq 0\\
H(x) = 0
</script>

<p>where $G(x)$ is the group of inequality constrains <script type="math/tex">g_i(x)</script>; $H(x)$ is the
group of equality constrains <script type="math/tex">h_i(x)</script>; $x \in X = R^{n}$ or $x \in X = R^{m
\times n}$, where $x$ is taken as matrix variables.</p>

<p>The domain of the problem is implicitly defined by $G(x)$ and
$H(x)$. Geometrically, $H(x) = 0$ describes some surface in $X$. $G(x)$
describes some ``half space’’.</p>

<p>The question to ask is: what objects should we take those constrains as? so we
could internalize and reason with them.</p>

<p>In abstract sense, if we take $f(x), G(x), H(x)$ as variables, or in a more
intuitive word, as objects, in our perception the problem seems not to be that
abstract and convoluted. Their dependency on $x$ seems to gone. This is the
idea from functional analysis. We are just doing space transform on the
original space $X$. After the transform, we try to find the extreme value of a
specific coordinate of the new space, which is $f(x)$, in a sub-area of the new
space, which describes by constrains.</p>

<p>Despite of the non-linearity, every natural phenomena in nature, thus in Math,
comes from certain metamorphism of linear phenomena. To say it in another way,
though we could create very bizarre phenomena with math trick, the real world
should be analyzed starting from the linearity, not for simplicity or
computation’s sake.</p>

<p>But how could we come up with linearity?</p>

<p>This brings to think why constrains exists. It exists because in real world,
certain things has to be satisfied, it could be a fixed location, or limited
amount of resources and so on. The real world constrain is somehow connected to
the objective function, otherwise it won’t exist at all. So how could we model
such connection?</p>

<p>This is the key to the entry of duality.</p>

<p>We have to find the connection of variables $G(x), H(x)$ to another variable
$f(x)$. As having been discussed, every natural phenomena somehow comes from
metamorphism of linear phenomena. An example is the invention of calculus,
which is the how non-linear phenomena behaves locally. The way we connect those
variables is through their linear combination. This in the language of
functional analysis is to say we analyze the primal space from its dual space.</p>

<p>Now back to Lagrange duality. Lagrange multiplier is elements in dual space of
the space made up by $f(x), G(x), H(x)$.</p>

<p>This intuition unscramble constrains.</p>

<h2 id="an-example-linear-programming">An Example: Linear Programming</h2>

<p>Linear programming is the case where all things are linear. So nothing
metamorphized. When we only deal with gradients in a specific point in
non-linear programming, such as the case of KKT condition, Fritz-John
condition, it is actually exactly the optimality condition of linear
programming.</p>

<p>Previously I have noted an possible motivation of Linear programming. Now I
could see the previous note(about two months ago) is just a special case of
previous idea.</p>

<p>For a LP problem, as the following</p>

<script type="math/tex; mode=display">
\max\ c^{T}\\
Ax = b\\
x \geq 0
</script>

<p>I tried to see it from adjoin operator point of view, which is to find
something from $A^{T}$. For details, please refer to previous
<a href="/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view/">note</a>.</p>

<p>Now we write the Lagrange of LP, the dual function is</p>

<script type="math/tex; mode=display">
\inf\limits_{x} c^{T}x + y^{T}(Ax - b) - s^{T}x
</script>

<p>note we have $s \geq 0$.</p>

<p>Now we see how could we deduce it from previous argument of last
<a href="/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view/">note</a>.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
                &  \langle 1, c^{T}x \rangle + \langle y, Ax -b \rangle - \langle s, x \rangle\\
\Leftrightarrow &  \langle c, x \rangle + \langle y, Ax -b \rangle - \langle s, x \rangle\\
\Leftrightarrow &  \langle c - s, x \rangle + \langle y, Ax \rangle - \langle y, b \rangle\\
\Leftrightarrow & - \langle y, b \rangle +  \langle c - s, x \rangle + \langle y, Ax \rangle \\
\Leftrightarrow & - \langle y, b \rangle +  \langle c - s, x \rangle + \langle A^{T}y, x \rangle \\
\Leftrightarrow & - \langle y, b \rangle +  \langle c - s + A^{T}y, x \rangle \\
\Leftrightarrow &  \langle y, b \rangle +  \langle c - s - A^{T}y, x \rangle \\
\end{align*}
 %]]&gt;</script>

<p>We find some connection between dual space and primal space now. To make the
part of dual space be a lower bound, we ask $s \geq 0$. This is just the
zooming idea previously.</p>

<p>The more geometrically picture is the geometrical intuition from Lagrange
duality.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
 & \inf\limits_{x} \langle c, x \rangle + \langle y, Ax -b \rangle - \langle s, x \rangle\\
\Leftrightarrow & \inf\limits_{x} \langle c - s, x \rangle + \langle y, Ax \rangle - \langle y, b \rangle\\
\Leftrightarrow & - \langle y, b \rangle + \inf\limits_{x} \langle c - s, x \rangle + \langle y, Ax \rangle \\
\Leftrightarrow & - \langle y, b \rangle + \inf\limits_{x} \langle c - s, x \rangle + \langle A^{T}y, x \rangle \\
\Leftrightarrow & - \langle y, b \rangle + \inf\limits_{x} \langle c - s + A^{T}y, x \rangle \\
\Leftrightarrow &  \langle y, b \rangle + \inf\limits_{x} \langle c - s - A^{T}y, x \rangle \\
\end{align*}
 %]]&gt;</script>

<p>The $\inf$ could move the dual variable down. So we have a constrained problem
again.(Details are omitted, and could be found at <em>Nonlinear Optimization
Andrzej Ruszczynski</em> or some other books talk about Lagrange duality).</p>
]]></content>
  </entry>
  
</feed>
