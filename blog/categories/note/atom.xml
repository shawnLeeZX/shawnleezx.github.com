<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Note | SHAWN LEE]]></title>
  <link href="http://shawnLeeZX.github.io/blog/categories/note/atom.xml" rel="self"/>
  <link href="http://shawnLeeZX.github.io/"/>
  <updated>2015-08-01T14:17:25+08:00</updated>
  <id>http://shawnLeeZX.github.io/</id>
  <author>
    <name><![CDATA[Shawn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Add Text to Scanned Pdf Without Text]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/07/14/how-to-add-text-to-scanned-pdf-without-text/"/>
    <updated>2015-07-14T10:43:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/07/14/how-to-add-text-to-scanned-pdf-without-text</id>
    <content type="html"><![CDATA[<p>I am reading some paper that is decades ago so no pdf file with text could
found, so I looked up to some solution. Here it is.</p>

<!-- more -->

<p><a href="https://github.com/gkovacs/pdfocr">Pdfocr</a> is a ruby script that integrates
open source tools to add text layer to pdf files.</p>

<p>To be able to use it, there are some dependence needed, just as shell scripts.</p>

<p><code>bash
sudo apt-get install pdftk
sudo apt-get install tesseract-ocr tesseract-ocr-eng exactimage
</code></p>

<p>You can infer what those programs are used for based what the author
<a href="http://ubuntuforums.org/showthread.php?t=1456756">said</a> below.</p>

<blockquote>
  <p>pdfocr was written by me (Geza Kovacs). It is simply a script which automates
the following process:</p>

  <ol>
    <li>Splitting the PDF file into separate pages using pdftk</li>
    <li>Extracting out the image data using pdfimages</li>
    <li>Doing OCR (optical character recognition) using cuneiform</li>
    <li>Embedding the detected text back into the PDF file using hocr2pdf</li>
    <li>Merging together the files using pdftk.</li>
  </ol>
</blockquote>

<p>Lastly, clone the ruby code.</p>

<p><code>bash
git clone https://github.com/gkovacs/pdfocr
</code></p>

<p>Then enjoy.</p>

<p><code>bash
pdfocr.rb -i foo.pdf -o out.pdf
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zoom In and Out Gnome Using Mouse]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/07/02/zoom-in-and-out-gnome-using-mouse/"/>
    <updated>2015-07-02T17:18:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/07/02/zoom-in-and-out-gnome-using-mouse</id>
    <content type="html"><![CDATA[<p>Just tried to find a solution to zoom in and out in the GNOME desktop, so that
things could be seen better in case they are too small during presentation.</p>

<!-- more -->

<p>The solution a program called <code>mousewheelzoom</code>, whose source is held at github:
<a href="https://github.com/tobiasquinn/gnome-shell-mousewheel-zoom">gnome-shell-mousewheel-zoom</a>.</p>

<p>The installation instructions could be found there.</p>

<hr />

<p>Besides the solution that works, two solutions that did not work are also going
to noted below.</p>

<h3 id="compiz-setting">Compiz Setting</h3>

<p>At first I tried to changing setting in Compizconfig Settingsmanager, as
suggested in the
<a href="http://askubuntu.com/questions/82398/how-to-zoom-inzoom-out">link</a>. It turns
out that compiz is used by Unity, not GNOME. Since I have changed the desktop
environment, it does not obviously.</p>

<h3 id="gnome-native-solution">GNOME Native solution</h3>

<p>Then GNOME also offers such function directly in their <em>Universal Access</em>,
under <em>Seeing</em> tab, in category Zoom — just type Universal Access in the
Dock. However, it does not work either.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Connection between PCA and Fourier Transform]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/06/05/connection-between-pca/"/>
    <updated>2015-06-05T21:10:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/06/05/connection-between-pca</id>
    <content type="html"><![CDATA[<!-- more -->

<p>When we are running PCA on images, the first principle component we get is
normally looks like the DC term, which is the first Fourier bases of
images. After some analysis, I found that this makes a lot of sense.</p>

<p>We could unify PCA and Fourier Transform from filter bank or linear operator
point of view. Actually those two point of views are just different way to say
the same thing in signal processing and mathematics.</p>

<p>From filter bank point of view, the process of getting the PCA coefficients and
Fourier coefficients could be regarded as passing the original signal to a
Linear Time Invariant system. Each Fourier basis or eigenvector is a
filter. All of them make up the filter bank.</p>

<p>From linear operator point of view, such process could be taken as an operator
to project the point in one space to another space.</p>

<p>It turns out that Fourier basis $e^{i\omega t}$ and eigenvector $v$ of covariance
matrix are both eigenvector(this eigenvector is not specific to the eigenvector
computed from covariance matrix of the data) of the LTI system.</p>

<p>The Fourier basis eigenvectors could be taken as the eigenvector of covariance
of all possible images that could be formed by spatial complex exponential
signal. So it is fixed. As for the data driven eigenvector $v$, it is only the
eigenvector of that amount of data. They are both orthogonal with other
eigenvectors within their sets. If data driven eigenvector get normalized, it
has unit norm as well.</p>

<p>This could explain why the first principal components of covariance looks a lot
similar do the first Fourier basis. Because DC term of images affect all
pixels, naming all dimensions. So its variance is the largest, which makes it
similar with first fourier basis. I think similar argument could be made to
other principal components and Fourier bases as well. But they may be less
obvious then the first one.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Concept Explanation on Composition of Matrice Representing Linear Transform]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/05/22/a-concept-explanation-on-composition-of-matrice-representing-linear-transform/"/>
    <updated>2015-05-22T22:18:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/05/22/a-concept-explanation-on-composition-of-matrice-representing-linear-transform</id>
    <content type="html"><![CDATA[<!-- more -->

<p>It is a bit hard to internalize the fact that the composition of finite
dimensional linear transformation is the multiplication of the matrices
representing them. I figured out a concept explanation.</p>

<p>Denoting the first transformation matrix as $A$ and the second as $B$,
$\vec{a_{i}}$ as the row of $A$ and $\vec{b_{j}}$ as the row of $B$, the to
be transformed vector as $x$, basically, each $\vec{a}_{i}$ determines how
each coordinate $x_{i}$ of $\vec{x}$ will contribute to the result
coordinates.</p>

<p>Then if you do another linear transform on $\vec{Ax}$, each $\vec{b}_{j}$ determines how
each coordinate of $\vec{Ax}$ will contribute to the final vector. Since the transform is
linear, we could first consider how $\vec{a_{i}}$ contributes to the final
vector, its final result with $x$ is just a multiple of $x_{i}$. Each
component of $\vec{a_{i}}$ actually could be taken as a to be transformed
vector of $B$. Each row of $AB$ could be viewed in the following way.</p>

<p>$\vec{a_{i}}$ is the new coordinate of standard basis $\vec{e_{i}}$ after
applying $A$. Starting from there, the second new coordinates of those first
new coordinates are obtained by applying $B$. So if we apply $AB$ on standard
basis $\vec{e_{i}}$, the result is going to be the second new
coordinates. Actually, such coordinates are:</p>

<script type="math/tex; mode=display">
\sum\vec{b_{j}}(\vec{a_{i}})_{j}
</script>

<p>which is just $\vec{(AB)_{i}}$.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What A Mathematical Object Random Variable Really Is]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/05/14/what-mathematical-object-random-really-is/"/>
    <updated>2015-05-14T11:14:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/05/14/what-mathematical-object-random-really-is</id>
    <content type="html"><![CDATA[<p>An example will be noted here to illustrate what a random variable really is.</p>

<p>Sadly, things will be clearer with graph but for now I do not have a solution
to make good illustration really quickly enough — normally I write a post in
about half an hour.</p>

<!-- more -->

<p>The problem setting is that if we toss a coin twice, there are four possible
outcomes that we could get — $(hh, ht, th, tt)$, where $h$ stands for head
while $t$ stands for tail. If we denote the results as tuples, we could
represent the experiment results as $((1, 1), (1, 0), (0, 1), (0, 0))$. If we
take each element of the tuple with two elements as dimension of a vector
space, each tuple could be regarded as an element in a vector space. we could
represent our experiment result in a 2D plane.</p>

<p>Consider that a random variable $X$ which is used to denote the number of heads
in this experiment. Its value would be 0, 1 or 2.</p>

<p>Now we make the statement that random variable a mapping, which maps the
original event space $\Omega$(the four tuples here) to a subset$T’$(a set of
${0, 1, 2}$ here) of a set $T$(the positive integer set $N^{+}$). Let’s see
what this actually means.</p>

<p>In the first level, the understanding could be pretty straight forward. $X$
in some sense agglomerate the event space in the most preliminary level – the
atomic exclusive level to a coarser level — the number of heads. So $(1, 0)$
and $(0, 1)$ all collapse to the point ${1}$ in the set $T$. Correspondingly,
the probability measure assigned on each event is changed. Mathematically,
given a set $A$ in $T$, its probability now is </p>

<script type="math/tex; mode=display">
P^{X}(A) = P(\{\omega: X(\omega) \in A \}) = P(X^{-1}(A)) = P(X \in A)
</script>

<p>However, there is a way to think in the framework of functional analysis. Each
mapping, precisely, function, in a vector space(Hilbert space could be a
properer setting) could be regarded as projecting points in the vector space to
the basis determined by the adjoint operator of this function.</p>

<p>In previous example, a two dimensional space is reduced into one dimensional
space, whose basis is the adjoint operator of the function $X$. And the
probability measure of a point in this new one dimensional space is the sum of
all points in the two dimensional points that will be projected at that
point. Now, let’s see what those statements mean.</p>

<p>If we draw a line crossing origin with slope one, all points with the same
number of heads will be projected at the same point in this line. The point
with two heads, which is $(1, 1)$, will be projected at point $(1, 1)$. The
points with one head, which is $(1, 0) and (0, 1)$, will be projected at
$(\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2})$. Finally, the point with no head
will be projected at $(0, 0)$. Now we drop the second dimension, each point in
this line will semantically mean the number of heads, where the random variable
will vary. Their probability measure will be summed while points are being
projecting.</p>

<p>The counting of heads is also represented by the function $(x + y)$(the value
in each dimension of previous two dimensional space actually means the number
of heads, so their sum will the number of heads), which is the adjoint operator
of basis $(1, 1)$. This form is the representation of the basis of our new
space in our old two dimensional space.</p>

<p>One last note. When we are reducing dimensionality, we are also discarding the
information of how many number of tail we get.</p>

<p>In conclusion, what random variable really is?</p>

<p>Random variable first maps original event space onto another event space, then
defines a new probability distribution on the new space according its relation
to the old event space. The intuitive meaning of the random variable is how it
deals with original event space, which is counting the number of heads
here. But essentially, it defines a distribution or in other words, probability
measure at each point(the number of heads), in the new space.</p>
]]></content>
  </entry>
  
</feed>
