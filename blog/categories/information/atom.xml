<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Information | SHAWN LEE]]></title>
  <link href="http://shawnLeeZX.github.io/blog/categories/information/atom.xml" rel="self"/>
  <link href="http://shawnLeeZX.github.io/"/>
  <updated>2015-10-18T13:34:59+08:00</updated>
  <id>http://shawnLeeZX.github.io/</id>
  <author>
    <name><![CDATA[Shawn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Understanding Entropy(Shannon)]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2013/12/17/understanding-entropy/"/>
    <updated>2013-12-17T09:09:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2013/12/17/understanding-entropy</id>
    <content type="html"><![CDATA[<p>The original intuitive is the smaller the probability one event may happen, the
more its uncertainty is. This make sense. It converts the unmeasurable into the
measurable.</p>

<p>Now the problem becomes what is the relation between the probability of events
and the overall uncertainty of the state?</p>

<p>One state may contain several events, which each of them have their own
probability to happen. Using the terminology from Thermodynamics, one
microstates could consist of a number of states. Returning back to the
probability, it means one state contains a number of events. To take into
account the weight of different events, the probabilities are used.</p>

<!-- more -->

<p>See the $p_i$ in: $entropy = \Sigma(p_i * log(p_i))$</p>

<p>Now, the weight part has done, how can we measure uncertainty?</p>

<p>In the book <em>Warmth Disperses and Time Passes</em>, page 106, writen by Hans
Christian Von Baeyers:</p>

<blockquote>
  <p>Whenever you multiply two integers, the numbers of their respective
digits add.</p>

  <p>eg: 60 x 600 = 36,000</p>

  <p>so two digits plus three digits equals five digits (the rule
sometimes misses by one digit, as in 3x3 = 9, but that’s a
negligible error in view of the vastness of the number of molecules
in a gas.)</p>

  <p>So Boltzmann made the bold, inspired guess that entropy equals the
number of digits of the corresponding probability</p>
</blockquote>

<p>I have not read this book. The material comes from this
<a href="http://ask.metafilter.com/128814/Help-me-Understand-Boltzmanns-entropy-formula-S-k-log-W">url</a>.</p>

<p>So, this solved my long puzzled question why log is used. We have to
think of a way to measure the uncertainty in one event. And such
measurement should be mathematically correct. That means, it should
satisfy several criteria(and obviously I do not know exactly what are
them all):</p>

<ol>
  <li>If only one event is contained in the state, the entropy should be 0.</li>
  <li>If all events happen equally, the entropy should be maximum.</li>
  <li>…</li>
</ol>

<p>Logarithm does a great job. But what is the theory underlying it? Keep
reading.</p>

<h2 id="another-example">Another Example</h2>

<p>I also tried to understand entropy from encoding perspecitive, by
comparing average encoding length(AEL) with entropy.</p>

<p>The AEL is computed as following:</p>

<p>$AEL = \Sigma(p_i * l_i)$</p>

<p>Where $P_i$ stands for the probability of one information $i$(can be
string, etc.) being chosen, and $l_i$ stands for the length of the
encoding of the information.</p>

<p>In this equation, $l_i$ is semantically similar to the corresponding
$log(p_i)$ part of the Shannon entropy.</p>

<p>Huffman encoding is a perfect example combine encoding with
entropy(The detail of Huffman encoding is omitted).</p>

<p>The main intuitive is the longer the encoding of one information, the
smaller the probability it may happen. The smaller the AEL of one
encoding method is, the better, meaning less chaotic, it is.</p>

<p>Combined with the AEL formula above, the length of the encoding of one
information is actually similar to the number of digitals the
probability have.  And, this connects the dots.</p>

<p>you can get the pdf version
<a href="https://github.com/hhiker/ml_math/blob/master/information_theory/understanding_entropy/understanding_entropy.pdf">here</a>.</p>
]]></content>
  </entry>
  
</feed>
