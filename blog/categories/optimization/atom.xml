<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Optimization | SHAWN LEE]]></title>
  <link href="http://shawnLeeZX.github.io/blog/categories/optimization/atom.xml" rel="self"/>
  <link href="http://shawnLeeZX.github.io/"/>
  <updated>2015-12-15T21:15:39+08:00</updated>
  <id>http://shawnLeeZX.github.io/</id>
  <author>
    <name><![CDATA[Shawn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[On KKT Condition and Optimality Condition of Conic Linear Programming]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/12/15/on-kkt-condition-and-optimality-condition-of-conic-linear-programming/"/>
    <updated>2015-12-15T20:18:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/12/15/on-kkt-condition-and-optimality-condition-of-conic-linear-programming</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>In LP and SDP, the complementary slackness are written as</p>

<script type="math/tex; mode=display">
(c - A^{T}y)^{T}x = 0\\
(C -\sum\limits_{i}y_{i}A_{i}) \bullet X = 0
</script>

<p>While KKT condition’s complementary slackness is</p>

<script type="math/tex; mode=display">
v^{T}g(x) = 0
</script>

<p>It is hard to connect them at first glance, since dual variable $v$ is dual to
the space of $g(x)$, which is not necessarily in the same space with $x$.
Especially when $x$ is the big matrix $X$ in SDP, $g(X)$ lies in vector space
instead of matrix space.</p>

<p>This is the note for the how they are connected.</p>

<!-- more -->

<h2 id="linear-programming">Linear Programming</h2>

<p>For linear programming, we write down its Lagrange</p>

<script type="math/tex; mode=display">
c^{T}x + y^{T}(b - Ax) - s^{T}x
</script>

<p>its gradient is $c - A^{T}y - s$</p>

<p>so its KKT condition is</p>

<script type="math/tex; mode=display">
c - A^{T}y - s = 0 \text{ Stationary point}\\
x \geq 0, Ax = b \text{ Primal feasibility}\\
s^{T}x = 0, s \geq 0 \text{ Complementary slackness}
</script>

<p>Now we see $(c - A^{T}y)^{T}x = 0$ is the combination of stationary point
condition and complementary slackness condition. The special structure occurs
here because</p>

<ol>
  <li>In linear problem, analytic solution of equation is possible.</li>
  <li>$g(x)$ is just $x$, so their spaces are the same.</li>
</ol>

<p>Note when LP reaches its optimal value, we always have $c^{T}x = b^{T}y =
(A^{T}y)^{T}x$, but there is a difference $s$ between $c$ and $A^{T}y$. Here we
see the specialty of LP. When one dimension of $s$ is not zero, we have
correspond dimension of $x$ to be zero. Due to the linearity of $c^{T}x$, the
zero directly reflects on the objective value, which makes correspond
dimension’s contribution to zero. Now it is clearer why duality gap is always
zero.</p>

<h2 id="semi-definite-programming">Semi-Definite Programming</h2>

<p>For SDP, if we want to draw analog with LP, the Lagrange may look like</p>

<script type="math/tex; mode=display">
C \bullet X + \sum\limits_{i}y_i(b_i - A_i \bullet X) - S \bullet X
</script>

<p>its gradient is <script type="math/tex">C - \sum\limits_{i}y_i A_i - S</script>.</p>

<p>So its KKT condition is</p>

<script type="math/tex; mode=display">
C - \sum\limits_{i}y_i A_i - S = 0\\
x \succeq 0, A_i x = b_i\\
S \bullet X = 0
</script>

<p>which is almost the same in format with that of LP.</p>

<p>But note</p>

<ol>
  <li>KKT does not work wit matrix inequality yet, so $S \bullet X$ cannot be
moved up to the Lagrange function. Though it does suggest some form of
KKT condition that could deal with matrix inequality may exist.</li>
  <li>The relationship is not linear anymore, both in $X \succeq 0$ and $C \bullet
X$ because the symmetric constrain enforced on $X$, which makes $X$ not just
a matrix form vector.</li>
  <li>$g(X) \succeq 0$ maps matrix to matrix space, so actually complementary
slackness also should be in the space of matrix, and in form of matrix inner
product.</li>
</ol>

<p>So there should be a duality gap normally.</p>

<h2 id="last-note-on-dual-variables">Last Note on Dual Variables</h2>

<p>From above, we see the reason we have matrix inner product style complementary
slackness is due to we are using dual variable in matrix space. It is
interesting to see how general dual variable or linear hyperplane is on solving
problem on any space, though I have not learned any proof of Lagrange on spaces
other than Euclidean space yet.</p>

<h2 id="last-note-on-linearity">Last Note on Linearity</h2>

<p>In conic linear programming,</p>

<ol>
  <li>when putting constrains in Lagrange function, it directly interacts with
objective function, meaning variable $x$ could be separated out.</li>
  <li>The specialty of $x \geq 0$ makes its dual variable the slack variable.</li>
  <li>Linearity makes dual function $\inf \ldots$ analytic solvable, so we only
see $\max$ in the dual problem.</li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Duality in Optimization]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/12/14/on-duality-in-optimization/"/>
    <updated>2015-12-14T21:09:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/12/14/on-duality-in-optimization</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>A more general picture of duality in optimization emerges itself. Some time is
still needed to unify all duality in optimization, but for now I summarize a
draft version of it here using duality of linear programming as an example.</p>

<!-- more -->

<h2 id="intuition-of-lagrange-multiplier-from-functional-analysis">Intuition of Lagrange Multiplier From Functional Analysis</h2>

<p>In its most general form, optimization problem is</p>

<script type="math/tex; mode=display">
\max\ f(x)\\
G(x) \geq 0\\
H(x) = 0
</script>

<p>where $G(x)$ is the group of inequality constrains <script type="math/tex">g_i(x)</script>; $H(x)$ is the
group of equality constrains <script type="math/tex">h_i(x)</script>; $x \in X = R^{n}$ or $x \in X = R^{m
\times n}$, where $x$ is taken as matrix variables.</p>

<p>The domain of the problem is implicitly defined by $G(x)$ and
$H(x)$. Geometrically, $H(x) = 0$ describes some surface in $X$. $G(x)$
describes some ``half space’’.</p>

<p>The question to ask is: what objects should we take those constrains as? so we
could internalize and reason with them.</p>

<p>In abstract sense, if we take $f(x), G(x), H(x)$ as variables, or in a more
intuitive word, as objects, in our perception the problem seems not to be that
abstract and convoluted. Their dependency on $x$ seems to gone. This is the
idea from functional analysis. We are just doing space transform on the
original space $X$. After the transform, we try to find the extreme value of a
specific coordinate of the new space, which is $f(x)$, in a sub-area of the new
space, which describes by constrains.</p>

<p>Despite of the non-linearity, every natural phenomena in nature, thus in Math,
comes from certain metamorphism of linear phenomena. To say it in another way,
though we could create very bizarre phenomena with math trick, the real world
should be analyzed starting from the linearity, not for simplicity or
computation’s sake.</p>

<p>But how could we come up with linearity?</p>

<p>This brings to think why constrains exists. It exists because in real world,
certain things has to be satisfied, it could be a fixed location, or limited
amount of resources and so on. The real world constrain is somehow connected to
the objective function, otherwise it won’t exist at all. So how could we model
such connection?</p>

<p>This is the key to the entry of duality.</p>

<p>We have to find the connection of variables $G(x), H(x)$ to another variable
$f(x)$. As having been discussed, every natural phenomena somehow comes from
metamorphism of linear phenomena. An example is the invention of calculus,
which is the how non-linear phenomena behaves locally. The way we connect those
variables is through their linear combination. This in the language of
functional analysis is to say we analyze the primal space from its dual space.</p>

<p>Now back to Lagrange duality. Lagrange multiplier is elements in dual space of
the space made up by $f(x), G(x), H(x)$.</p>

<p>This intuition unscramble constrains.</p>

<h2 id="an-example-linear-programming">An Example: Linear Programming</h2>

<p>Linear programming is the case where all things are linear. So nothing
metamorphized. When we only deal with gradients in a specific point in
non-linear programming, such as the case of KKT condition, Fritz-John
condition, it is actually exactly the optimality condition of linear
programming.</p>

<p>Previously I have noted an possible motivation of Linear programming. Now I
could see the previous note(about two months ago) is just a special case of
previous idea.</p>

<p>For a LP problem, as the following</p>

<script type="math/tex; mode=display">
\max\ c^{T}\\
Ax = b\\
x \geq 0
</script>

<p>I tried to see it from adjoin operator point of view, which is to find
something from $A^{T}$. For details, please refer to previous
<a href="/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view/">note</a>.</p>

<p>Now we write the Lagrange of LP, the dual function is</p>

<script type="math/tex; mode=display">
\inf\limits_{x} c^{T}x + y^{T}(Ax - b) - s^{T}x
</script>

<p>note we have $s \geq 0$.</p>

<p>Now we see how could we deduce it from previous argument of last
<a href="/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view/">note</a>.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
                &  \langle 1, c^{T}x \rangle + \langle y, Ax -b \rangle - \langle s, x \rangle\\
\Leftrightarrow &  \langle c, x \rangle + \langle y, Ax -b \rangle - \langle s, x \rangle\\
\Leftrightarrow &  \langle c - s, x \rangle + \langle y, Ax \rangle - \langle y, b \rangle\\
\Leftrightarrow & - \langle y, b \rangle +  \langle c - s, x \rangle + \langle y, Ax \rangle \\
\Leftrightarrow & - \langle y, b \rangle +  \langle c - s, x \rangle + \langle A^{T}y, x \rangle \\
\Leftrightarrow & - \langle y, b \rangle +  \langle c - s + A^{T}y, x \rangle \\
\Leftrightarrow &  \langle y, b \rangle +  \langle c - s - A^{T}y, x \rangle \\
\end{align*}
 %]]&gt;</script>

<p>We find some connection between dual space and primal space now. To make the
part of dual space be a lower bound, we ask $s \geq 0$. This is just the
zooming idea previously.</p>

<p>The more geometrically picture is the geometrical intuition from Lagrange
duality.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
 & \inf\limits_{x} \langle c, x \rangle + \langle y, Ax -b \rangle - \langle s, x \rangle\\
\Leftrightarrow & \inf\limits_{x} \langle c - s, x \rangle + \langle y, Ax \rangle - \langle y, b \rangle\\
\Leftrightarrow & - \langle y, b \rangle + \inf\limits_{x} \langle c - s, x \rangle + \langle y, Ax \rangle \\
\Leftrightarrow & - \langle y, b \rangle + \inf\limits_{x} \langle c - s, x \rangle + \langle A^{T}y, x \rangle \\
\Leftrightarrow & - \langle y, b \rangle + \inf\limits_{x} \langle c - s + A^{T}y, x \rangle \\
\Leftrightarrow &  \langle y, b \rangle + \inf\limits_{x} \langle c - s - A^{T}y, x \rangle \\
\end{align*}
 %]]&gt;</script>

<p>The $\inf$ could move the dual variable down. So we have a constrained problem
again.(Details are omitted, and could be found at <em>Nonlinear Optimization
Andrzej Ruszczynski</em> or some other books talk about Lagrange duality).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cone, Dual Cone and Generalized Inequalities]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/11/28/cone-dual-cone-and-generalized-inequalities/"/>
    <updated>2015-11-28T11:04:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/11/28/cone-dual-cone-and-generalized-inequalities</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>I did not internalized the idea that a pointed cone identifies a generalized
equality until now, and would like to note it down.</p>

<!-- more -->

<h2 id="cone">Cone</h2>

<p>Let’s start with why we want inequality. We want it because we want to compare
two objects. In $R$, an object in this domain is a scalar and represents
something like length, or so. Suppose we want to compare the length of one edge
of a table and the length of a ruler. We put those two together.  Intuitively,
a table looks longer than a ruler, so we say if we want to compare anything
similar like those again, we standardize a unit, according to the reference to
the length of table or ruler, which in the ancient time is the length of some
king’s feet, give a length $x$ to object one, $y$ to object two, then if object
one is longer than object two, we call $x &gt; y$.</p>

<p>How could we generalize this idea to higher dimensional space? Consider vector
inequality. Suppose we want buy a department. The only factors we want to
consider is which floor our department is(assume the higher the better), and
how many square meters it has. What does department $x$ is better than $y$
mean? An intuitive answer is a department that is both higher and bigger is a
better choice. We call it $x \succeq y$.</p>

<p>We could see how cone abstract things here, in vector inequality.</p>

<p>First we break thing down, if we get two rulers, whose lengths are too similar
to tell who is longer in a distance, how could we tell? Without equipment, we
just put those two rules together, and see their difference. This is the mental
procedure we execute when we compare, so in scalar case, $x &gt; y$ means $x - y
&gt; 0$; in vector case, $x \succeq y$ means $x - y \succeq 0$.</p>

<p>What is special about $x - y$?</p>

<p>If we take it abstractly, $x - y$ means how much better when $x$ is compared to
$y$. How we define inequality is determined by how we measure the ``better’’
amount, so the measurement makes sense. For scalar it is just their value
difference. But for vector we cannot judge when department $x$ has higher floor
number than $y$ but smaller square meters. The solution to this dilemma is to
only compare the case where the comparison makes sense. Mathematically, it
means we only compare $x, y$ when $x, y$ stay in a domain, which depends on
both $x, y$, where the comparison makes sense. If $x - y$ is in the domain, we
compare, otherwise, do not bother.</p>

<p>Though we only convert the comparison of $x, y$ to whether $x - y$ belongs to a
domain. We do not know what the domain looks like yet. We want to reach the
conclusion the domain is a pointed cone, or in another name, proper cone.</p>

<p>The next task is to figure out the properties of the domain. What properties do
we want? Let’s list them:</p>

<ol>
  <li>Comparison is about direction and it does not matter if $x$ is a hundred
bigger than $y$, or one hundred percent bigger.</li>
  <li>Strict inequality should exist.</li>
  <li>Difference could be added together: If $x$ is better than $y$, $y$ is better
tan $z$, we should have $x$ is better than $z$.</li>
  <li>Limitation stays in the domain: slightly better is still better.</li>
</ol>

<p>Denoting the domain $K$, those four are translated to</p>

<ol>
  <li>$K$ is a pointed cone, i.e., if $u \in K$ and $-u \in K$, then $u = 0$; for
any $x \in K$ and $\alpha &gt; 0$, we have $\alpha u \in K$.</li>
  <li>$K$ has non-empty interior.</li>
  <li>$K$ is non-empty and closed under addition; i.e., for any $x, y \in K$, $x +
y \in K$.</li>
  <li>$K$ is closed.</li>
</ol>

<h2 id="dual-cone">Dual Cone</h2>

<p>Dual cone is about giving each dimension of an object a linear measurement of
weight, or more intuitively a price, so:</p>

<ol>
  <li>A integrated ``betterness’’ could be quantified for all pairs instead of
just $K$, a restricted set of comparable pairs.</li>
  <li>consequently could lead to dual problem, which is a upper and lower bound by
linear approximation.</li>
</ol>

<p>Due to time limitation, I will just write those for now. Some pictures and
example of minimum elements and minimal elements from <em>Chapter 2, Convex
Optimization Bloyd</em> could be helpful.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Semidefinite Programming]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/11/15/notes-on-semidefinite-programming/"/>
    <updated>2015-11-15T21:23:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/11/15/notes-on-semidefinite-programming</id>
    <content type="html"><![CDATA[<p>Yet another note on math.</p>

<!-- more -->

<p>SDP is actually a relaxation techniques that projects the problem to a higher
dimension space with a symmetrical matrix special structure. So the original
non-linear problem gets linearized. In the simplest case, $x$ gets mapped to
$X = xx^{T}$, then relaxed to $X \in S_{+}$ It just puts the second order terms
of $x$ in the matrix. Similar things could be done in for higher order terms,
and finally, any order polynomial could be achieved in some way.</p>

<p>I am still not thinking very clearly how operator could be put into the context
since here matrices are just a special way to put a really long
vector. Frobenius inner product is just normal inner product. The power of SDP
comes from infinite number of quadratic constraints enforced by the PD
condition.</p>

<p>The connection could possibly be this way. It is inspired by how matrix
derivative is defined using Frobenius norm. Though operator’s nature comes from
it spectral structure, it is still made by $m \times n$ numbers put in a
special way. So a corresponding change in the corresponding position means
something. By measuring the norm, and inner product in this way, we are somehow
connected to the spectral structure of the matrix. By adding different matrices
together this way, we are somehow combining different operations, though I have
not learned any applications that matches this exactly. An perfect example of
this is the perturbation of a matrix. The perturbation in each dimension is
added on the face value(the number displayed on the entries of the matrix), so
it natural to think regardless of the spectral structure of the matrix,
whatever it may be normalized then or by changed by some other operations, the
face value change is how we perceive physical beings.</p>

<p>SDP is solved using interior point methods, whose intuitive is to guide the
optimization by the objective function, using gradient and hessian
information. To make sure the point stay feasible when optimizing, a term $-log
det|A|$ is added to the objective function, who will ensure $X \succeq \mu I$
for some $\mu$, meaning it is positive definite.</p>

<p>This is a very beautiful idea. Actually it is what I was looking for in
non-linear optimization so that the boundary information not only guide
optimization only when the point is near the boundary but also when it is far
in the boundary.</p>

<p>More thinking may be added when I learned more about SDP, matrix algebra and
matrix calculus.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Linear Programming, Duality of LP and Operator, a Functional Analysis Point of view]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view/"/>
    <updated>2015-10-18T12:39:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view</id>
    <content type="html"><![CDATA[<p>Yet another note on Math, about visualization of Linear Programming, some
thoughts on Operator and Adjoin.</p>

<!-- more -->

<h2 id="dual-map-and-adjoin-operator">Dual Map and Adjoin Operator</h2>

<p>Denote the matrix of a linear mapping $T$ corresponds to natural bases as $A$,
then $A^{T}$ is the matrix corresponds to the dual map of $T$. More
specifically, if the bases of the two space, the original space and its dual,
are orthogonal respectively, $A^{T}$ is the adjoin operator of $A$. Now suppose
$A \in R^{M \times N}$.</p>

<p>For a linear programming problem, the constraints are $Ax \leq b$, intuitively,
we understand it as a polyhedron, the intersection of finite number of
halfspaces.</p>

<p>We could also understand it from linear mapping and dual bases point of view.</p>

<p>$A$ maps points from $R^{N}$ to $R^{M}$. A column of $A$ corresponds to the
coordinates of the an natural basis of $R^{N}$ in the new $R^{M}$, meanwhile,
a column of $A^{T}$, which is a row of $A$, corresponds to the coordinates of a
dual basis of a natural basis of $R^{M}$ in $R^{N}$ if it is transformed by
$A^{T}$.</p>

<p>So a row $\vec{a_{i}}$ of $A$ is a $A^{T}$ transformed dual basis that
corresponds to one natural basis in $R^{M}$, which corresponds to a hyperplane
in $R^{N}$, also an element of the dual space of $R^{N}$.</p>

<h2 id="on-the-feasible-region-of-linear-programming">On the Feasible Region of Linear Programming</h2>

<p>Now we try to see whether we could get a picture from the above concepts.</p>

<h3 id="a-previous-possibly-wrong-attempt">A Previous Possibly Wrong Attempt</h3>

<p>This is the first visualization I have tried. It turns out not that right after
I discussed the idea with my optimization course teacher.</p>

<p>From this perspective, the polyhedron in $R^{N}$ is a squashed(or enlarged)
quadrant of $R^{M}$, which is the intersection of hyperplanes whose normals are
natural bases of $R^{M}$ — the hyperplanes only need to pass a constant
value, probably corresponding dimension of $b$(have not thought very clearly),
do not necessarily pass origin point.</p>

<p>If $M &gt; N$, $R^{M}$ gets squashed, and there are much freedom in the way how
the space gets squashed, which is analog with we have more equations than
variables, so it is easier to get solutions. Otherwise, $R^{M}$ gets enlarged,
however, no matter how the enlarging is done, it still remains as a low
dimensional space in the high dimensional space, thus the freedom is limited,
which is analog with more variables than equations, so it is harder or
impossible to get solutions.</p>

<h3 id="reason-in-the-standard-form">Reason In the Standard Form</h3>

<p>If we convert the problem to the standard form of linear programming, we could
see that the dimension or the original space is usually larger than the dual
space, otherwise, the feasible region could be easily empty. In this case, the
argument, that the dual space gets squashed, does not hold. But a different
picture turns out immediately.</p>

<p>Intuitively, which means it is not necessarily right, as long the objective
function does not parallel with one of the constraints, the solution to LP is a
basic solution, otherwise, we could move the point along one of the constraint
to change the objective value. Now we only discuss basic solutions.</p>

<p>For every basic solution, we have $N$ constraints being active. Note that $N$
is the dimension of the dual space. Thus, each set of $N$ active constraints
corresponds to a version of dual space that gets mapped from the standard basis
of dual space by those columns of $A^{T}$. More specifically, each active
hyperplane in the $R^{M}$ is a basis in the dual space $R^{N}$.</p>

<h2 id="on-duality-of-linear-programming">On Duality of Linear Programming</h2>

<p>This leads to an intuition about how on earth the dual problem of LP is
conceived in the first place, and makes its extension to conic programming
clear.</p>

<p>From my understanding, the dual in Optimization is to find a lower bound of the
distance in the original space in its dual space, being it the shortest
distance from a point to a subspace, from a point to a closed convex set or a
convex epigraph to a concave epigraph of functions. Those three examples, which
is three types of dual in infinite dimensional optimization try to find a
linear approximate lower bound for a normally non-linear objective
function. Their dual problems are usually some kind of LP, or other forms could
be easily solved. But for LP, all parts in the problem is already linear. From
this line of thinking, the dual of LP is to find a lower bound in the dual
space of $Ax$. How could this be? The key lies in the adjoin operator.</p>

<p>The standard form of LP is</p>

<script type="math/tex; mode=display">
\max\ c^{T}\\
Ax = b\\
x \geq 0
</script>

<p>We have an adjoin relation</p>

<script type="math/tex; mode=display">
\langle Ax, y\rangle = \langle x, A^{T}y \rangle\\
\langle b, y\rangle = \langle x, A^{T}y\rangle\\ 
\langle b, y\rangle \leq \langle x, c\rangle
</script>

<p>The last inequality holds because we have $x \geq 0$.</p>

<p>Conic programming just extends the meaning of $\geq$ and inner product.</p>

<h2 id="the-algebraic-nature-of-operator">The Algebraic Nature of Operator</h2>

<p>All of the thinking about originates from the way I learn Math. I want to get a
picture from the symbols and relationships. Up to now, all concepts I know
could be put into a 2D or 3D system with or without coordinates in my mind,
being it topology space, metric space, Banach space, inner product space,
Hilbert space, dual space, affine set, convex set, cone, measure, probability
measure and so on. Dual in optimization is a new concept, and I want to put it
somewhere in the system. In about 4 days, I almost finished the book
<em>Optimization by Vector Space Method</em>, which is about infinite dimensional
optimization, and learned the three kinds of duals mentioned before. I indeed
came up with similar visualization tricks mentioned in the book before I read
them. But in the section, some words rang a bell, after a few days I read them
and pondered on the meaning of adjoin, the relation between the $Ax$ and $X$.</p>

<p>At the beginning of the Chapter Six</p>

<blockquote>
  <p>Because it is difficult to obtain a simple geometric representation of an
arbitrary linear operator, the material in this chapter tends to be somewhat
more algebraic in character than that of other chapters. Effort is made,
however, to extend some of the geometric ideas used for the study of linear
functionals to general linear operators and also to interpret adjoints in
terms of relations among hyperplanes.</p>
</blockquote>

<p>The point of an operator $A$ is to transform a point in $X$ to a new space for
some purpose. It is not clear to see why it helps in the general form, but
think Fourier Transform, which is to transform a point in the signal space to
the frequency space. Boom, science advanced. In retrospect, do we need to keep
a picture in the natural basis of signal space when considering the frequency
space? No. In some research paper I read, for instance, scattering transform,
the whole is to make sure the new space has good properties.</p>

<p>So for operators, the most important is to enforce regularities to make sure
there is good properties in the new space, which is algebraic in nature instead
of geometrical.</p>
]]></content>
  </entry>
  
</feed>
