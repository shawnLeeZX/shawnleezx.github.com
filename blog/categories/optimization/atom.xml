<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Optimization | SHAWN LEE]]></title>
  <link href="http://shawnLeeZX.github.io/blog/categories/optimization/atom.xml" rel="self"/>
  <link href="http://shawnLeeZX.github.io/"/>
  <updated>2016-01-07T22:13:00+08:00</updated>
  <id>http://shawnLeeZX.github.io/</id>
  <author>
    <name><![CDATA[Shawn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Optimization Summary]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/12/16/optimization-summary/"/>
    <updated>2015-12-16T15:06:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/12/16/optimization-summary</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>The semester’s course on Optimization is going to end. This is a summary note
to unify the mathematical picture learned during this period.</p>

<!-- more -->

<h2 id="problem-formulation">Problem Formulation</h2>

<p>In the most general form, an optimization problem is to find an extreme value,
taking minimum for ease of writing, in the following form</p>

<script type="math/tex; mode=display">
\min\ f(X)\\
G(X) \succeq \theta\\
H(X) = \theta
</script>

<p>where $X$ could be a scalar, vector or matrix; $G(X)$ is a stack of inequality
constrains, whose row is one inequality constrain; $\theta$ is zero vector;
$H(X)$ is a stack of equality constrains, whose row is one equality constrain.</p>

<p>To solve the above problem, analysis should be made to properties of $f(X),
G(X)$ and $H(X)$. Geometric intuition is the source of aspiration and algebraic
descriptions or relaxation of geometric intuition is how we could solve the
problem.</p>

<p>The geometric picture of $f(X)$ is contour of its value, whose metaphor is the
contour of the height of a mountain; or just the mountain by seeing how high
its. The first picture is in the domain of $X$, while the second one is in the
$(X, f(X))$, which adds another dimension.</p>

<p>$G(X)$ and $H(X)$ explicitly or implicitly define an sub-area of all possible
$X$. Its picture is harder to visualize, and is the difficulty aspects in
optimization.</p>

<h2 id="domain-type">Domain Type</h2>

<p>Different types of variables has different structure and properties that
enforced on the domain. As an example, Conic Linear Programming is to explore
the structureness to simplify problems.</p>

<p>It does not occur to me clearly for now how possibly the enforced peculiar
structure of Semi-definite programming or Second Order Cone Programming could
have physical meaning. But SDP how take advantage of matrix variable and
inequality is inspiring.</p>

<h2 id="geometry-of-domain">Geometry of Domain</h2>

<p>The work around is to assume only to solve problems who is convex, and call the
remaining problems non-convex, consequently classifying it as hard.</p>

<p>It seems this is rather a restrict and side step the difficult but important
problems. This was the perception I had when I first heard about Optimization
years ago. But the actually it is not after really struggling through it.</p>

<p>Convexity of a set is the atom to describe geometry of the whole set in the
following sense: if two points are in the set, then we want any points between
them are also in the set. Normally, points have some physical meaning in real
world problem, not some bizarre mathematical fabrication. In physical world,
extremity breaks thing, but not their intermediate value. Thus it is a good
model unless we discover some bizarre phenomena(which probably already exists
but I do not know).</p>

<p>Convexity and concavity of function are the only two ways how function could
vary locally. The really mattered properties of convexity is non-decreasing, so
concavity is non-increasing. They get named quasa-convex for convexity part.</p>

<p>If we could analyzed the atoms how sets compose a bigger set and how functions
composes a ``larger’’ function, we begin to get a look into the whole picture.</p>

<p>Current situation of optimization is local geometry has been well studied, but
a way to generalize to the global geometry lacks. So I guess it is not the
a good understanding of convexity is the first step to understand more complex
problems.</p>

<h2 id="dimensionality-of-domain">Dimensionality of Domain</h2>

<p>This section is about the concept of <em>affine</em>.</p>

<p>The motivation of ``affine’’ is to create a term that could be used to describe
the dimension of domain. So essentially, an affine set is linear variety or the
whole space. Linear variety is a concept. For example, a linear variety in 3D
space would be a line, a plane.</p>

<h2 id="unifying-optimization-duality-with-functional-analysis">Unifying Optimization Duality with Functional Analysis</h2>

<p>If we start from a Linear Programming(LP), which is the following problem</p>

<script type="math/tex; mode=display">
\min\ c^{T}x\\
Ax = b
x \geq 0
</script>

<p>and its dual</p>

<script type="math/tex; mode=display">
\max\ b^{T}y\\
A^{T}y \leq c
</script>

<p>From function analysis point of view, those two problems involves four space,
space of $x$(we call it primal space for convenience from now), dual space of
primal space, space of $y$(we call it transformed space from now), dual space
of transformed space. Duality of LP is the characterization of relationship
between those four spaces.</p>

<p>If we upgrade this view to non-linear case,</p>

<script type="math/tex; mode=display">
\min\ f(X)\\
G(X) \succeq \theta\\
H(X) = \theta
</script>

<p>we see $G(X)$ and $H(X)$ are also a transform, but not linear. But similar idea
should also apply. This is the idea of Lagrange multiplier and Lagrange
duality.</p>

<p>The idea is to analyze in the transformed space $(f(X), Z)$, where $Z$ is the
whose variables are $G(X), H(X)$(in proof involved with Lagrange duality, we
break $H(X)$ to two inequalities). $Z$ and its dual <script type="math/tex">Z^{*}</script> centralize all
duality in optimization.</p>

<p>The hard part is to analyze what the space $Z$ is like. First, different types
of variables have different properties and work with different inequalities,
such as vector and vector inequality, matrix and matrix inequality. Second,
what the transformed space is like after mapping is hard to deal with.</p>

<p>The key properties of $Z$ is whether it has an interior point and is convex,
which determines whether we could find a separating hyperplane, an element in
dual space.</p>

<p>Farkas lemma is just a special case where we could explicitly know what the
transformed space looks like.</p>

<h2 id="on-cone-and-dual-cone">On Cone and Dual Cone</h2>

<p>This section is about cone and dual cone. One more example could be seen in the
next section.</p>

<p>Cone is about direction. It is hard to describe it informally without concrete
examples. Pointed cone identifies inequality relations, which is summarized in
this <a href="/blog/2015/11/28/cone-dual-cone-and-generalized-inequalities/">note</a>.</p>

<p>Dual cone is about the direction that at least slightly the same with primal
cone, so in this direction values also could increase, or decrease if it is a
negative dual cone. Dual cone is a set in dual space of primal space, whose
elements are normals of hyperplane. Remember a normal of a hyperplane is about
the direction where the values goes up(or down depending on definition).</p>

<h2 id="variational-idea-kkt-condition-fritz-john-condition">Variational Idea, KKT Condition, Fritz-John Condition</h2>

<p>Existence of Lagrange multiplier is the zero order properties of a constrained
optimal point. By considering the first derivative, we could get how gradients
at the optimal point are related to each other.</p>

<p>Equality constrain in the limit is to carve subspace out, so all things should
happen in this subspace. Gradients of inequality constrains determine whether
the we could still have descent directions, which is Fritz-John or KKT
condition. The idea is if negative dual cone of positive range of gradients of
inequality constrains do not intersect with negative dual cone of $\nabla
f(x)$, then we reach a constrained minimum. The regularity conditions of KKT
just means the gradients of inequality constrains do not conflict with each
other, which if happens would only leave a feasible set that has nothing to do
with $f(x)$, which is the case the dual variable associated with $\nabla f(x)$
is zero in Fritz-John optimality condition.</p>

<h2 id="optimization-algorithms">Optimization Algorithms</h2>

<p>Few has been explored in this area for now.</p>

<p>The idea is to explore the structures of the four spaces mentioned, and the
target function to find the extreme values and corresponding solutions, either
globally, for instance using the simplex method, or locally, using gradient
based methods.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On KKT Condition and Optimality Condition of Conic Linear Programming]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/12/15/on-kkt-condition-and-optimality-condition-of-conic-linear-programming/"/>
    <updated>2015-12-15T20:18:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/12/15/on-kkt-condition-and-optimality-condition-of-conic-linear-programming</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>In LP and SDP, the complementary slackness are written as</p>

<script type="math/tex; mode=display">
(c - A^{T}y)^{T}x = 0\\
(C -\sum\limits_{i}y_{i}A_{i}) \bullet X = 0
</script>

<p>While KKT condition’s complementary slackness is</p>

<script type="math/tex; mode=display">
v^{T}g(x) = 0
</script>

<p>It is hard to connect them at first glance, since dual variable $v$ is dual to
the space of $g(x)$, which is not necessarily in the same space with $x$.
Especially when $x$ is the big matrix $X$ in SDP, $g(X)$ lies in vector space
instead of matrix space.</p>

<p>This is the note for the how they are connected.</p>

<!-- more -->

<h2 id="linear-programming">Linear Programming</h2>

<p>For linear programming, we write down its Lagrange</p>

<script type="math/tex; mode=display">
c^{T}x + y^{T}(b - Ax) - s^{T}x
</script>

<p>its gradient is $c - A^{T}y - s$</p>

<p>so its KKT condition is</p>

<script type="math/tex; mode=display">
c - A^{T}y - s = 0 \text{ Stationary point}\\
x \geq 0, Ax = b \text{ Primal feasibility}\\
s^{T}x = 0, s \geq 0 \text{ Complementary slackness}
</script>

<p>Now we see $(c - A^{T}y)^{T}x = 0$ is the combination of stationary point
condition and complementary slackness condition. The special structure occurs
here because</p>

<ol>
  <li>In linear problem, analytic solution of equation is possible.</li>
  <li>$g(x)$ is just $x$, so their spaces are the same.</li>
</ol>

<p>Note when LP reaches its optimal value, we always have $c^{T}x = b^{T}y =
(A^{T}y)^{T}x$, but there is a difference $s$ between $c$ and $A^{T}y$. Here we
see the specialty of LP. When one dimension of $s$ is not zero, we have
correspond dimension of $x$ to be zero. Due to the linearity of $c^{T}x$, the
zero directly reflects on the objective value, which makes correspond
dimension’s contribution to zero. Now it is clearer why duality gap is always
zero.</p>

<h2 id="semi-definite-programming">Semi-Definite Programming</h2>

<p>For SDP, if we want to draw analog with LP, the Lagrange may look like</p>

<script type="math/tex; mode=display">
C \bullet X + \sum\limits_{i}y_i(b_i - A_i \bullet X) - S \bullet X
</script>

<p>its gradient is <script type="math/tex">C - \sum\limits_{i}y_i A_i - S</script>.</p>

<p>So its KKT condition is</p>

<script type="math/tex; mode=display">
C - \sum\limits_{i}y_i A_i - S = 0\\
x \succeq 0, A_i x = b_i\\
S \bullet X = 0
</script>

<p>which is almost the same in format with that of LP.</p>

<p>But note</p>

<ol>
  <li>KKT does not work wit matrix inequality yet, so $S \bullet X$ cannot be
moved up to the Lagrange function. Though it does suggest some form of
KKT condition that could deal with matrix inequality may exist.</li>
  <li>The relationship is not linear anymore, both in $X \succeq 0$ and $C \bullet
X$ because the symmetric constrain enforced on $X$, which makes $X$ not just
a matrix form vector.</li>
  <li>$g(X) \succeq 0$ maps matrix to matrix space, so actually complementary
slackness also should be in the space of matrix, and in form of matrix inner
product.</li>
</ol>

<p>So there should be a duality gap normally.</p>

<h2 id="last-note-on-dual-variables">Last Note on Dual Variables</h2>

<p>From above, we see the reason we have matrix inner product style complementary
slackness is due to we are using dual variable in matrix space. It is
interesting to see how general dual variable or linear hyperplane is on solving
problem on any space, though I have not learned any proof of Lagrange on spaces
other than Euclidean space yet.</p>

<h2 id="last-note-on-linearity">Last Note on Linearity</h2>

<p>In conic linear programming,</p>

<ol>
  <li>when putting constrains in Lagrange function, it directly interacts with
objective function, meaning variable $x$ could be separated out.</li>
  <li>The specialty of $x \geq 0$ makes its dual variable the slack variable.</li>
  <li>Linearity makes dual function $\inf \ldots$ analytic solvable, so we only
see $\max$ in the dual problem.</li>
</ol>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Duality in Optimization]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/12/14/on-duality-in-optimization/"/>
    <updated>2015-12-14T21:09:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/12/14/on-duality-in-optimization</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>A more general picture of duality in optimization emerges itself. Some time is
still needed to unify all duality in optimization, but for now I summarize a
draft version of it here using duality of linear programming as an example.</p>

<!-- more -->

<h2 id="intuition-of-lagrange-multiplier-from-functional-analysis">Intuition of Lagrange Multiplier From Functional Analysis</h2>

<p>In its most general form, optimization problem is</p>

<script type="math/tex; mode=display">
\max\ f(x)\\
G(x) \geq 0\\
H(x) = 0
</script>

<p>where $G(x)$ is the group of inequality constrains <script type="math/tex">g_i(x)</script>; $H(x)$ is the
group of equality constrains <script type="math/tex">h_i(x)</script>; $x \in X = R^{n}$ or $x \in X = R^{m
\times n}$, where $x$ is taken as matrix variables.</p>

<p>The domain of the problem is implicitly defined by $G(x)$ and
$H(x)$. Geometrically, $H(x) = 0$ describes some surface in $X$. $G(x)$
describes some ``half space’’.</p>

<p>The question to ask is: what objects should we take those constrains as? so we
could internalize and reason with them.</p>

<p>In abstract sense, if we take $f(x), G(x), H(x)$ as variables, or in a more
intuitive word, as objects, in our perception the problem seems not to be that
abstract and convoluted. Their dependency on $x$ seems to gone. This is the
idea from functional analysis. We are just doing space transform on the
original space $X$. After the transform, we try to find the extreme value of a
specific coordinate of the new space, which is $f(x)$, in a sub-area of the new
space, which describes by constrains.</p>

<p>Despite of the non-linearity, every natural phenomena in nature, thus in Math,
comes from certain metamorphism of linear phenomena. To say it in another way,
though we could create very bizarre phenomena with math trick, the real world
should be analyzed starting from the linearity, not for simplicity or
computation’s sake.</p>

<p>But how could we come up with linearity?</p>

<p>This brings to think why constrains exists. It exists because in real world,
certain things has to be satisfied, it could be a fixed location, or limited
amount of resources and so on. The real world constrain is somehow connected to
the objective function, otherwise it won’t exist at all. So how could we model
such connection?</p>

<p>This is the key to the entry of duality.</p>

<p>We have to find the connection of variables $G(x), H(x)$ to another variable
$f(x)$. As having been discussed, every natural phenomena somehow comes from
metamorphism of linear phenomena. An example is the invention of calculus,
which is the how non-linear phenomena behaves locally. The way we connect those
variables is through their linear combination. This in the language of
functional analysis is to say we analyze the primal space from its dual space.</p>

<p>Now back to Lagrange duality. Lagrange multiplier is elements in dual space of
the space made up by $f(x), G(x), H(x)$.</p>

<p>This intuition unscramble constrains.</p>

<h2 id="an-example-linear-programming">An Example: Linear Programming</h2>

<p>Linear programming is the case where all things are linear. So nothing
metamorphized. When we only deal with gradients in a specific point in
non-linear programming, such as the case of KKT condition, Fritz-John
condition, it is actually exactly the optimality condition of linear
programming.</p>

<p>Previously I have noted an possible motivation of Linear programming. Now I
could see the previous note(about two months ago) is just a special case of
previous idea.</p>

<p>For a LP problem, as the following</p>

<script type="math/tex; mode=display">
\max\ c^{T}\\
Ax = b\\
x \geq 0
</script>

<p>I tried to see it from adjoin operator point of view, which is to find
something from $A^{T}$. For details, please refer to previous
<a href="/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view/">note</a>.</p>

<p>Now we write the Lagrange of LP, the dual function is</p>

<script type="math/tex; mode=display">
\inf\limits_{x} c^{T}x + y^{T}(Ax - b) - s^{T}x
</script>

<p>note we have $s \geq 0$.</p>

<p>Now we see how could we deduce it from previous argument of last
<a href="/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view/">note</a>.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
                &  \langle 1, c^{T}x \rangle + \langle y, Ax -b \rangle - \langle s, x \rangle\\
\Leftrightarrow &  \langle c, x \rangle + \langle y, Ax -b \rangle - \langle s, x \rangle\\
\Leftrightarrow &  \langle c - s, x \rangle + \langle y, Ax \rangle - \langle y, b \rangle\\
\Leftrightarrow & - \langle y, b \rangle +  \langle c - s, x \rangle + \langle y, Ax \rangle \\
\Leftrightarrow & - \langle y, b \rangle +  \langle c - s, x \rangle + \langle A^{T}y, x \rangle \\
\Leftrightarrow & - \langle y, b \rangle +  \langle c - s + A^{T}y, x \rangle \\
\Leftrightarrow &  \langle y, b \rangle +  \langle c - s - A^{T}y, x \rangle \\
\end{align*}
 %]]&gt;</script>

<p>We find some connection between dual space and primal space now. To make the
part of dual space be a lower bound, we ask $s \geq 0$. This is just the
zooming idea previously.</p>

<p>The more geometrically picture is the geometrical intuition from Lagrange
duality.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{align*}
 & \inf\limits_{x} \langle c, x \rangle + \langle y, Ax -b \rangle - \langle s, x \rangle\\
\Leftrightarrow & \inf\limits_{x} \langle c - s, x \rangle + \langle y, Ax \rangle - \langle y, b \rangle\\
\Leftrightarrow & - \langle y, b \rangle + \inf\limits_{x} \langle c - s, x \rangle + \langle y, Ax \rangle \\
\Leftrightarrow & - \langle y, b \rangle + \inf\limits_{x} \langle c - s, x \rangle + \langle A^{T}y, x \rangle \\
\Leftrightarrow & - \langle y, b \rangle + \inf\limits_{x} \langle c - s + A^{T}y, x \rangle \\
\Leftrightarrow &  \langle y, b \rangle + \inf\limits_{x} \langle c - s - A^{T}y, x \rangle \\
\end{align*}
 %]]&gt;</script>

<p>The $\inf$ could move the dual variable down. So we have a constrained problem
again.(Details are omitted, and could be found at <em>Nonlinear Optimization
Andrzej Ruszczynski</em> or some other books talk about Lagrange duality).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cone, Dual Cone and Generalized Inequalities]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/11/28/cone-dual-cone-and-generalized-inequalities/"/>
    <updated>2015-11-28T11:04:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/11/28/cone-dual-cone-and-generalized-inequalities</id>
    <content type="html"><![CDATA[<p>Yet another note on Math.</p>

<p>I did not internalized the idea that a pointed cone identifies a generalized
equality until now, and would like to note it down.</p>

<!-- more -->

<h2 id="cone">Cone</h2>

<p>Let’s start with why we want inequality. We want it because we want to compare
two objects. In $R$, an object in this domain is a scalar and represents
something like length, or so. Suppose we want to compare the length of one edge
of a table and the length of a ruler. We put those two together.  Intuitively,
a table looks longer than a ruler, so we say if we want to compare anything
similar like those again, we standardize a unit, according to the reference to
the length of table or ruler, which in the ancient time is the length of some
king’s feet, give a length $x$ to object one, $y$ to object two, then if object
one is longer than object two, we call $x &gt; y$.</p>

<p>How could we generalize this idea to higher dimensional space? Consider vector
inequality. Suppose we want buy a department. The only factors we want to
consider is which floor our department is(assume the higher the better), and
how many square meters it has. What does department $x$ is better than $y$
mean? An intuitive answer is a department that is both higher and bigger is a
better choice. We call it $x \succeq y$.</p>

<p>We could see how cone abstract things here, in vector inequality.</p>

<p>First we break thing down, if we get two rulers, whose lengths are too similar
to tell who is longer in a distance, how could we tell? Without equipment, we
just put those two rules together, and see their difference. This is the mental
procedure we execute when we compare, so in scalar case, $x &gt; y$ means $x - y
&gt; 0$; in vector case, $x \succeq y$ means $x - y \succeq 0$.</p>

<p>What is special about $x - y$?</p>

<p>If we take it abstractly, $x - y$ means how much better when $x$ is compared to
$y$. How we define inequality is determined by how we measure the ``better’’
amount, so the measurement makes sense. For scalar it is just their value
difference. But for vector we cannot judge when department $x$ has higher floor
number than $y$ but smaller square meters. The solution to this dilemma is to
only compare the case where the comparison makes sense. Mathematically, it
means we only compare $x, y$ when $x, y$ stay in a domain, which depends on
both $x, y$, where the comparison makes sense. If $x - y$ is in the domain, we
compare, otherwise, do not bother.</p>

<p>Though we only convert the comparison of $x, y$ to whether $x - y$ belongs to a
domain. We do not know what the domain looks like yet. We want to reach the
conclusion the domain is a pointed cone, or in another name, proper cone.</p>

<p>The next task is to figure out the properties of the domain. What properties do
we want? Let’s list them:</p>

<ol>
  <li>Comparison is about direction and it does not matter if $x$ is a hundred
bigger than $y$, or one hundred percent bigger.</li>
  <li>Strict inequality should exist.</li>
  <li>Difference could be added together: If $x$ is better than $y$, $y$ is better
tan $z$, we should have $x$ is better than $z$.</li>
  <li>Limitation stays in the domain: slightly better is still better.</li>
</ol>

<p>Denoting the domain $K$, those four are translated to</p>

<ol>
  <li>$K$ is a pointed cone, i.e., if $u \in K$ and $-u \in K$, then $u = 0$; for
any $x \in K$ and $\alpha &gt; 0$, we have $\alpha u \in K$.</li>
  <li>$K$ has non-empty interior.</li>
  <li>$K$ is non-empty and closed under addition; i.e., for any $x, y \in K$, $x +
y \in K$.</li>
  <li>$K$ is closed.</li>
</ol>

<h2 id="dual-cone">Dual Cone</h2>

<p>Dual cone is about giving each dimension of an object a linear measurement of
weight, or more intuitively a price, so:</p>

<ol>
  <li>A integrated ``betterness’’ could be quantified for all pairs instead of
just $K$, a restricted set of comparable pairs.</li>
  <li>consequently could lead to dual problem, which is a upper and lower bound by
linear approximation.</li>
</ol>

<p>Due to time limitation, I will just write those for now. Some pictures and
example of minimum elements and minimal elements from <em>Chapter 2, Convex
Optimization Bloyd</em> could be helpful.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Semidefinite Programming]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/11/15/notes-on-semidefinite-programming/"/>
    <updated>2015-11-15T21:23:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/11/15/notes-on-semidefinite-programming</id>
    <content type="html"><![CDATA[<p>Yet another note on math.</p>

<!-- more -->

<p>I understand SDP as a relaxation techniques that explode the problem to a
higher dimension space with a symmetrical matrix special structure. The
original problem corresponds to a manifold in the new higher dimensional
space. So we are allowed to approach problems in direction that is not possible
in the low dimensional space. An illustration idea could be the worm hole idea
in physics. Another benefits could be to linearize original non-linear
problem. In the simplest case, $x$ gets mapped to $X = xx^{T}$, then relaxed to
$X \in S_{+}$. It just puts the second order terms of $x$ in the
matrix. Similar things could be done in for higher order terms, and finally,
any order polynomial could be achieved in some way.</p>

<strike>
I am still not thinking very clearly how operator could be put into the context
since here matrices are just a special way to put a really long
vector. Frobenius inner product is just normal inner product. The power of SDP
comes from infinite number of quadratic constraints enforced by the PD
condition.
</strike>

<p><br /></p>

<strike>
The connection could possibly be this way. It is inspired by how matrix
derivative is defined using Frobenius norm. Though operator's nature comes from
it spectral structure, it is still made by $m \times n$ numbers put in a
special way. So a corresponding change in the corresponding position means
something. By measuring the norm, and inner product in this way, we are somehow
connected to the spectral structure of the matrix. By adding different matrices
together this way, we are somehow combining different operations, though I have
not learned any applications that matches this exactly. An perfect example of
this is the perturbation of a matrix. The perturbation in each dimension is
added on the face value(the number displayed on the entries of the matrix), so
it natural to think regardless of the spectral structure of the matrix,
whatever it may be normalized then or by changed by some other operations, the
face value change is how we perceive physical beings.
</strike>

<p><br /></p>

<p>The structure of <script type="math/tex">S_{+}</script> is defined in the exactly same way with vector
space. Its inner product is Frobenius inner product, which just vectorize
matrices and calculate normal vector inner product. The difference is the
symmetric constraints and positive semi-definite constraints.</p>

<p>But what is the connection between seeing matrix as a vector and as an operator?</p>

<p>Such structure does not connect to spectral properties of a symmetric
matrix. Frobenius norm also is not related to the semantics of an operator —
transform.</p>

<p>The intuition of the PSD matrix is to transform a vector to another vector
within the positive dual cone of the original vector. (I guess this is the
definition that could extended to asymmetric matrices, taking rotation matrix
within 90 degree as an example).</p>

<p>Spectral properties of symmetric matrices mean if we choose the bases right,
matrix multiplication is just component-wise scaling.</p>

<p>Those are illustrating pictures coming from linear algebra. But in the general
form, matrix multiplication $Ax$ works as <script type="math/tex">a_{i}^{T}x, i = 1 \ldots n</script>, where
<script type="math/tex">a_{i}^{T}</script> is the ith row of $A$, not as <script type="math/tex">\sum\limits_{i}a_{i}x_i</script>, where
<script type="math/tex">a_{i}</script> is ith column of $A$, which is the picture from linear algebra.</p>

<p>The transform is formed by stacking <script type="math/tex">n</script> linear hyperplanes, or linear
functionals together. Since linear functional is in the dual space <script type="math/tex">X^{*}</script> of
$X$, and also in finite dimensional case <script type="math/tex">X^{*}</script> equivalent to $X$, this is
the reason we could get useful relations in $X$.</p>

<p>In this case, $A - B$ calculates the difference in the linear functionals of
$A, B$. $A \bullet B$ calculates a sum value of inner products of all
functionals of $A, B$. Inner product is the way we measure difference between
hyperplanes.</p>

<p>Now, from the above sense, the structure defined before is for the matrix
itself, but not for how it could transform a vector.</p>

<p>A last note on how to solve SDP.</p>

<p>SDP is solved using interior point methods, whose intuitive is to guide the
optimization by the objective function, using gradient and hessian
information. To make sure the point stay feasible when optimizing, a term $-log
det|A|$ is added to the objective function, who will ensure $X \succeq \mu I$
for some $\mu$, meaning it is positive definite.</p>

<p>This is a very beautiful idea. Actually it is what I was looking for in
non-linear optimization so that the boundary information not only guide
optimization only when the point is near the boundary but also when it is far
in the boundary.</p>

<strike>More thinking may be added when I learned more about SDP, matrix algebra and
matrix calculus.</strike>

<p><br /></p>

<p>I guess this note is roughly finalized.</p>

<p>Updated on Dec 18th, 2015</p>
]]></content>
  </entry>
  
</feed>
