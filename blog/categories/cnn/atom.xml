<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: CNN | SHAWN LEE]]></title>
  <link href="http://shawnLeeZX.github.io/blog/categories/cnn/atom.xml" rel="self"/>
  <link href="http://shawnLeeZX.github.io/"/>
  <updated>2015-12-14T22:29:56+08:00</updated>
  <id>http://shawnLeeZX.github.io/</id>
  <author>
    <name><![CDATA[Shawn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Preliminary Summary On What CNN Is Doing Mathematically]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/04/07/a-preliminary-summary-on-what-cnn-is-doing-mathematically/"/>
    <updated>2015-04-07T19:05:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/04/07/a-preliminary-summary-on-what-cnn-is-doing-mathematically</id>
    <content type="html"><![CDATA[<p>There is too much to stuff all those notes into one weekly summary, so I decide
to decouple it into this post. It is about a description of the process what
Convolutional Neural Network(CNN) is doing mathematically.</p>

<!-- more -->

<p>Last week I have written an informal one. The informality was resulted from an
unclear understanding of space, mapping and the way CNN works. Things get much
clearer now.</p>

<p>The task CNN is trying to achieve is the same with all the other machine
learning algorithms – finding a mapping that will transform the preliminary
input to one kind of output that makes the task easier to solve or directly
solve the task. The formed one is called feature learning and the latter is
called classification or anything else that is the target.</p>

<p>The problem is how exactly such mapping is found? Or more specifically, why CNN
is more powerful than other algorithms?</p>

<h2 id="how-mapping-is-found-mathematically">How Mapping Is Found Mathematically</h2>

<p>First we address the first problem, though those two are actually the same.</p>

<p>Instead of trying to find a mapping to our problem directly, which are methods
called shallow model today, CNN tries to find a sequence of mappings to
decouple information from its most primitive form. If we rephrase the word
mapping to operation, it may sound more like plain English. Then judging from
the information extracted, it answers whatever relevant questions we are
interested, such as classification of images of different animals.</p>

<p>More mathematically, CNN tries to find the mapping by finding a representation
of the mapping on relevant bases which are found by compositing preliminary
bases. Then in the space represented by the relevant bases, divide the new
space using previous shallow model.</p>

<p>The latter part has not been investigated fully yet. I guess support vector
machine may be the key to this problem.</p>

<p>Further zooming in the process, in each layer, every kernel of CNN will
convolute with the input. In the first layer, the input is the raw signal. In
the following layer, the input is the output of last layer. A kernel here is a
basis. Convoluting with input is actually taking inner product with input.</p>

<p>In the first layer, by computing the inner product of a basis function with a
signal, we get a value. This value is one way to look at the signal, or the
information extracted which may be comprehended by human, like the existence of
a certain of kind edge. By taking inner product of all bases, we get the
signal’s representation in form of coordinates of the set of bases.</p>

<p>From now on, the task is to find combinations of the first layer’s bases that
are more informative. By using more layers, we hope to find relevant
combinations in exponential number of possible ones.</p>

<p>The most preliminary bases actually have mathematical terminology: in the
continuous case, it is called Schauder basis; in the discrete case, it is
called Hamel basis. In this case, the coordinates of the signal is the same
with its ground truth value.</p>

<p>Feature mappings from previous layer to current layer are the key to find a
more complex bases. It brings the search in the space of preliminary bases to a
more advanced space of more advanced bases. A best example is an input in form
of a three channel image will have three feature mappings, one corresponding to
one color. If we just concatenate three pixels of different color, we have no
idea it corresponds to different frequencies of light. But if we take it from
three channels, we are using the high-level information of colors. Another
example is provided in the next section.</p>

<p>Now the process up to the first part of the process is done. The second part
will not be described given that I have not understood it fully yet.</p>

<p>The followings are some notes on properties of such process:</p>

<ol>
  <li>The key of depth is reusing information hierarchically and preventing
exploration on meaningless part of function space. This is the main reason
why deep model works and will be elaborated with an example in the next
section.</li>
  <li>Coordinates are coordinates. There is only one ground truth of
nature. Coordinates are the way we see the ground truth. Different spaces
are actually different with different levels of abstraction. Different
levels of abstraction are achieved using different sets of bases.</li>
  <li>Taking inner product is the process to map point in one space to another
space. This is also the intuition behind Riesz Theorem that any linear
functional could be represented using inner product.</li>
  <li>The intuition of taking inner product is also computing correlation between
the basis and the signal. The more they are similar, the bigger the
coordinate(in CNN terminology system, the bigger the response). </li>
  <li>Bases are connected with hidden factors referred by Bengio.</li>
  <li>Spatial information is kept by the intuition of cascading local receptive
fields.</li>
</ol>

<hr />

<h2 id="analytically-why-more-depth-is-more-powerful">Analytically Why More Depth Is More Powerful</h2>

<p>Imagine a square whose components are two horizontal edges and two vertical
edges. To detect such shape, we could imagine a three layer convolutional
neural network. In the first layer of filters, we have two kernels – one
for horizontal edges and one for vertical edges. Write them in matrix form,
they are like this:</p>

<p>Horizontal Edge Kernel</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
0 & 0 & 0 \\
1 & 1 & 1 \\
0 & 0 & 0
\end{bmatrix}
 %]]&gt;</script>

<p>and Vertical Edge Kernel</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0
\end{bmatrix}
 %]]&gt;</script>

<p>The square we are going to detect is like this:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
1   & 1 & 1 & 1 &  1 \\
1   & 0 & 0 & 0 &  1 \\
1   & 0 & 0 & 0 &  1 \\
1   & 0 & 0 & 0 &  1 \\
1   & 1 & 1 & 1 &  1 
\end{bmatrix}
 %]]&gt;</script>

<p>As we moving the two kernels, we get response similar like this:</p>

<p>feature map for horizontal edge detection kernel:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
x & 1 & x \\
0 & 0 & 0 \\
x & 1 & x
\end{bmatrix}
 %]]&gt;</script>

<p>and feature map for vertical edge detection kernel:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
x & 0 & x \\
1 & 0 & 1 \\
x & 0 & x
\end{bmatrix}
 %]]&gt;</script>

<p>$x$ is the response of angle, which may not be black and white. And since they
are not very influential to our discussion, it is denoted $x$.</p>

<p>In the next convolution layer, the input are two feature maps, then the kernel
for detecting square will be like:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
x & 1 & x \\
0 & 0 & 0 \\
x & 1 & x
\end{bmatrix}
and
\begin{bmatrix}
 x & 0 & x \\
 1 & 0 & 1 \\
 x & 0 & x
\end{bmatrix}
 %]]&gt;</script>

<p>The final inner product will be normalized to one. In this case, we are only
using four kernels.</p>

<p>If we switch to one layer CNN, The kernel for detecting square will be like:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
x   & 1 & 1 & 1 & x \\
1   & 0 & 0 & 0 & 1 \\
1   & 0 & 0 & 0 & 1 \\
1   & 0 & 0 & 0 & 1 \\
x   & 1 & 1 & 1 & x
\end{bmatrix}
 %]]&gt;</script>

<p>The zero and one is expanded because each edges consist of more than one
pixels. Three here is used for illustration.</p>

<p>It seems that we can detect shape only using one kernel. But wait… we are not
only trying to detect only square. We should also try to detect circle,
rectangles etc… So we will have more than more kernels. In this case, the
space of possible number of kernels will exponentially explode given the
possible changes we may swap the zero and one in the pixels of kernels.</p>

<p>But in previous three layer cases, the space of possible changes are shrunk
dramatically. The key here is explained in previous section, we are reusing the
information that only consecutive edges are sensible combination of pixels and
the number of possibilities of changing locations of edges is much smaller than
changing values of pixels.</p>

]]></content>
  </entry>
  
</feed>
