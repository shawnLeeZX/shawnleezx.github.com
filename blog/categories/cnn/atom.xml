<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: CNN | SHAWN LEE]]></title>
  <link href="http://shawnLeeZX.github.io/blog/categories/cnn/atom.xml" rel="self"/>
  <link href="http://shawnLeeZX.github.io/"/>
  <updated>2016-08-30T11:46:55+08:00</updated>
  <id>http://shawnLeeZX.github.io/</id>
  <author>
    <name><![CDATA[Shawn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Key points of Understanding Deep Conv Net]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2016/08/29/key-points-of-understanding-deep-conv-net/"/>
    <updated>2016-08-29T10:41:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2016/08/29/key-points-of-understanding-deep-conv-net</id>
    <content type="html"><![CDATA[<p>This note tries to summarize the keypoints of the paper by Mallat:
<a href="http://www.ncbi.nlm.nih.gov/pubmed/26953183">Understanding deep convolutional networks</a>.</p>

<p>Key concepts: contraction, linearization, fibre, parallel transport,
multi-scale support vector</p>

<!-- more -->

<p>Mechanically, Convolutional Neural Network could be formalized using cascading
the following of operators that compose a function $f$, whose value is in $R$
for regression problems, and is the index of the sample’s class for
classification problems.</p>

<script type="math/tex; mode=display">
x_j = \rho W_j x_{j-1}
</script>

<p>where $j$ indicates the depth of a network.</p>

<p>Essentially, neural network is a way to combat the curse of dimensionality. In
the paper, Mallat formalizes this process using linearization of hierarchical
symmetry, space contraction and sparse separation. In short, it is done by
defining a new variable $\Phi(x)$, where $\Phi$ is a <em>contractive</em> operator
that reduces the range of variations of $x$, while still <em>separating</em> different
values of $f: \Phi(x) \not= \Phi(x’)$ if $f(x) \not= f(x’)$. $\Phi$ should have
the properties to <strong>linearize hierarchical symmetries, does space contraction and
achieve sparse separation</strong> to keep classification margin (value margin for
regression). $\Phi$ is called the new representation of $x$.</p>

<h2 id="linearization-of-symmetries">Linearization of symmetries</h2>

<p>Neural network finds equivalence classes $\Phi(x)$ that linearize
$f$. $\Phi(x)$ is built by collapsing complex symmetry groups.</p>

<h3 id="linearization">Linearization</h3>

<p>By definition, linearization means to find a representation $\Phi$ that $f$ on
$x \in \Omega$, where the domain $\Omega$ is a high dimensional open set, which
could be $L^2(R^n)$ or $R^d$, could be approximated by a linear combination</p>

<p><script type="math/tex">\bar{f} = \sum_i\Phi_i(x)</script>.</p>

<p>The idea of linearization is to find important yet highly complex basis vector
that linearly contributes to $f$, so $f$ is a linear projection of
<script type="math/tex">\Phi_i(x)</script>. An example would be the word vector, like <code>actor - man + woman =
actress</code>. Similar phenomenon is observed in images as well.</p>

<p>On the other hand, it also means $\Phi(x)$ absorbs in the variability that is
not related to $f$. An example could be translation variability, which is
absorbed by subsampling in NN.</p>

<p>It is difficulty for me to understand what does Mallat mean by</p>

<p>``We can then optimize a low-dimensional linear projection along directions where
$f$ is constant.’’</p>

<p>The above is the meaning I have guessed. In summary, if $\Phi$ is fixed, the
optimization works on the linear projection’s weight. ``$f$ is constant’’ means
after the change of variable, the variability in the original space $\Omega$
has been absorbed, so a variability in the direction that does not change
$\Phi$ does not change $f$.</p>

<h3 id="space-collapse">Space collapse</h3>

<p>The key is to find $\Phi$ that linearizes $f$, which is to find $\Phi$ that
collapses the directions that $f$ remains constant. Mallat formalizes this
``collapse’’ in term of group theory, or more particularly the group of
transformations.</p>

<p>The word collapse is my own creation, which tries to summarize the key point of
building invariant representation $\Phi$ utilizing groups of
symmetry. It corresponds to the absorbing of variability in previous
section. <script type="math/tex">\Phi_i(x)</script> represents an equivalence class (could be the orbit of
the group, more on this later) of $x$. Collapse is the process of combining
those classes together to form an equivalence class.</p>

<p>Formally, if $f$ is invariant to an operator $g$ that preserves the value of
$f$, which is to say $f(gx) = f(x)$, we can safely incorporate $g$ in $\Phi(x)$
(putting $gx$ in an equivalence class), so any transformation $g$ on $x$, aka
$gx$, does not change $\Phi(x)$, consequently it does not change $f$. Acting as
an equivalence class, they contribute to the linearization of $f$.</p>

<p>An example could be the translation group as an example, $f(gx) = f(x - u)$,
where $u \in \Omega$, representing the displacement of the translation. $f$’s
value does not change by translation, so by averaging (subsampling) spatially,
a direction that $f$ remains constant is collapsed out. This direction is part
of a equivalence class formed by collapsing out the translation group.</p>

<p>By pinpointing those equivalence classes $\Phi(x)$, linearization is achieved
by doing linearly projection on them.</p>

<h2 id="more-details">More details</h2>

<p>The high level plan stops here. Those above two are the main components making
up the framework of NN. Now we fill in the details, aka sparse separation,
space contraction, separation of scales and complex interaction among
multi-scale.</p>

<h3 id="the-functional-form-of-phi">The functional form of $\Phi$</h3>

<p>$\Phi$ is handcrafted basis when the time of dictionary learning or deep
learning has not come. It is a vector that captures important (in any criteria
matters for the problem at hand) features.</p>

<h3 id="contraction-for-sparsity">Contraction for sparsity</h3>

<p>There are some general criteria for basis $\Phi$, one of which may require them
be sparse. A sparse signal brings to better linear separation. Or in other
word, factors of variation are better disentangled.</p>

<p>For fixed basis vectors, such as wavelets in scattering network they are
defined to separate scale and frequency. Thus for signals that only contains a
few of those, the wavelet coefficients would be rather sparse.</p>

<p>For an adaptive basis vector, to promote sparsity, Mallat puts forward the
concept of space contraction, which is achieved through the non-linear
activation function, a modulus or a ReLU. It explicitly contract the volume of
the space to achieve a sparse representation.</p>

<p>In the case of fixed basis like wavelet, if the representation is already
sparse, non-linear activation won’t reduce the volume too much since most of
the dimensions are already zero.</p>

<h3 id="diffeomorphism-calls-for-multi-scale-separation">Diffeomorphism calls for multi-scale separation</h3>

<p>Diffeomorphism can also be modeled a group symmetry. Diffeomorphism could be
factorized as a translation and a scaling effects. So to linearize
diffeomorphism, a multi-scale basis to separate the scaling effects are needed.</p>

<h3 id="multi-scale-interaction">Multi-scale interaction</h3>

<p>It is not enough to only separate signal using basis vectors, but also to model
their interactions. This justifies a deep architecture.</p>

<h2 id="combining-all-the-above-together">Combining all the above together</h2>

<p>Now we could map each symbol in the following to their functional roles.</p>

<script type="math/tex; mode=display">
x_j = \rho W_j x_{j-1}
</script>

<p>Each row of $W$ is the basis vector; $\rho$ is the contraction operator to
promote sparsity; <script type="math/tex">x_j = ... x_{j-1}</script> is the recursive form that takes cares
multi-scale separation and interaction.</p>

<p>At the same time, <script type="math/tex">W_{j-1}x_{j-1}</script> linearizes <script type="math/tex">W_j x_j</script>. If we adding
pooling in, it corresponds to collapsing the direction where $f$ remains
constant.</p>

<h2 id="recast-in-term-of-differential-geometry">Recast in term of differential geometry</h2>

<p>Mallat use the formalism in differential geometry to succinctly define the
hierarchically built $\Phi$.</p>

<p>According to Mallat, the power of Neural Network is it is able learn such a
representation that linearize hierarchical symmetry groups, which is formalized
using fibres of hierarchical symmetry groups. Nonlinear contraction reduces the
variability of each fibre, so removing the variability that is not directly
related to currently sample being processed. Fibres of current layer are
transported to fibres of next layers, which are fibres of large
groups. Cascadingly applying the parallel transport on a sample, we obtain
multi-scale support vectors that are sparse.</p>

<p>However, it is hard to map these math names to the operational name of conv
net. Here is my best guess up to now.</p>

<p>Each channel of a conv filter is a part of the orbits.
The equivalence class is the cross-channel sum of orbits of filters. The index
$P_j$ is the indices of orbits, which could be understood as the spatial
indices (orbits of translation group) and channel indices (orbits of those
groups, such as rotation groups). Since we have done a summation, how each 2D
filter corresponds to each part of the orbit does not matter, thus we get an
equivalence class.</p>

<p>Each equivalence class is called a fibre, which is just the fancy in
differential geometry to call an operator. In this case, the fibre is just the
cascadingly built operator $ρW_j$ mentioned at the beginning section. $ρW_j$
computes an approximate mapping from fibre of layer $j-1$ to $j$, which is
called a parallel transport in $P_j$.
A parallel transport is defined by a group $G_j$ of symmetries acting on the
index set $P_j$ of a layer $x_j$.
It is called an approximate mapping because fibre is supposed to be an
equivalence class of orbits of $x_{j-1}$, and the transport be mapping from
those equivalence class. however $pW_j$ starts from $x_{j-1}$, not
equivalence classes of orbits of $x_{j-1}$. So it is assumed to be approximated
by applying $g$ to $x_j$, we get $\bar{g}$ on $x_{j-1}$. Again implicitly
$x_{j}$ will be unrolled to the equivalence class of orbit of this layer,
which is a larger groups which is semidirect products of groups of layer $j-1$,
and groups (corresponding to channels in layer $j$) of layer $j$. The
approximation only makes sense by corresponding each channel of a filter to be
a part of a orbit of $g$, as in the previous paragraph. The channel of filters
are the groups of layer $j-1$, while the number of filters (output channel
number in most convnet implementations) is the groups of the layer
$j$. Assuming each filter channel corresponds to a part of the
orbit, $g.x_{j} = g.[\rho W_j x_{j-1}] \approx \rho W_j[\bar{g}.x_{j-1}]$
in the paper makes sense, since the summation cross channel would absorb $g$ in
$W_{j}$.</p>

<p>So the convolution is called convolutions along the fibres. Implicitly, $\rho
W_j$ expands $x_{j-1}$ to its orbits, then computes equivalence classes of
orbits of $x_{j}$ by cross-channelly adding the result of applying filters on
those orbits. If in this sense, $x_{j-1}$ is indeed an equivalence class of
orbits. I guess the approximation means that this is just an assumption, since
no constrains are enforced to make sure channels of a filter are orbits of
$x_{j-1}$.</p>

<p>In this model, network filters are guiding nonlinear contractions, to reduce the
data variability in directions of local symmetries. The classification margin
can be controlled by sparse separations along network fibres</p>

<p>Each fibre is complex basis vector. Sparsity ensures separateness. Mallat
introduces the concept of multi-scale support vector, explained in the
following. To avoid further contracting their distance, they can be separated
along different fibres indexed by $b$. The separation is achieved by filters
$w_{j,h.b}$, which transform $x_{j−1}$ and $x’_{j−1}$ into $x_{j}(g, h,
b)$and $x’_{j}(g, h, b)$ having sparse supports on different fibres $b$. The
next contraction $ρW_{j+1}$ reduces distances along fibres indexed by $(g, h)
\in G_j$,but not across $b \in B_j$,which preserves distances.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Preliminary Summary On What CNN Is Doing Mathematically]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/04/07/a-preliminary-summary-on-what-cnn-is-doing-mathematically/"/>
    <updated>2015-04-07T19:05:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/04/07/a-preliminary-summary-on-what-cnn-is-doing-mathematically</id>
    <content type="html"><![CDATA[<p>There is too much to stuff all those notes into one weekly summary, so I decide
to decouple it into this post. It is about a description of the process what
Convolutional Neural Network(CNN) is doing mathematically.</p>

<!-- more -->

<p>Last week I have written an informal one. The informality was resulted from an
unclear understanding of space, mapping and the way CNN works. Things get much
clearer now.</p>

<p>The task CNN is trying to achieve is the same with all the other machine
learning algorithms – finding a mapping that will transform the preliminary
input to one kind of output that makes the task easier to solve or directly
solve the task. The formed one is called feature learning and the latter is
called classification or anything else that is the target.</p>

<p>The problem is how exactly such mapping is found? Or more specifically, why CNN
is more powerful than other algorithms?</p>

<h2 id="how-mapping-is-found-mathematically">How Mapping Is Found Mathematically</h2>

<p>First we address the first problem, though those two are actually the same.</p>

<p>Instead of trying to find a mapping to our problem directly, which are methods
called shallow model today, CNN tries to find a sequence of mappings to
decouple information from its most primitive form. If we rephrase the word
mapping to operation, it may sound more like plain English. Then judging from
the information extracted, it answers whatever relevant questions we are
interested, such as classification of images of different animals.</p>

<p>More mathematically, CNN tries to find the mapping by finding a representation
of the mapping on relevant bases which are found by compositing preliminary
bases. Then in the space represented by the relevant bases, divide the new
space using previous shallow model.</p>

<p>The latter part has not been investigated fully yet. I guess support vector
machine may be the key to this problem.</p>

<p>Further zooming in the process, in each layer, every kernel of CNN will
convolute with the input. In the first layer, the input is the raw signal. In
the following layer, the input is the output of last layer. A kernel here is a
basis. Convoluting with input is actually taking inner product with input.</p>

<p>In the first layer, by computing the inner product of a basis function with a
signal, we get a value. This value is one way to look at the signal, or the
information extracted which may be comprehended by human, like the existence of
a certain of kind edge. By taking inner product of all bases, we get the
signal’s representation in form of coordinates of the set of bases.</p>

<p>From now on, the task is to find combinations of the first layer’s bases that
are more informative. By using more layers, we hope to find relevant
combinations in exponential number of possible ones.</p>

<p>The most preliminary bases actually have mathematical terminology: in the
continuous case, it is called Schauder basis; in the discrete case, it is
called Hamel basis. In this case, the coordinates of the signal is the same
with its ground truth value.</p>

<p>Feature mappings from previous layer to current layer are the key to find a
more complex bases. It brings the search in the space of preliminary bases to a
more advanced space of more advanced bases. A best example is an input in form
of a three channel image will have three feature mappings, one corresponding to
one color. If we just concatenate three pixels of different color, we have no
idea it corresponds to different frequencies of light. But if we take it from
three channels, we are using the high-level information of colors. Another
example is provided in the next section.</p>

<p>Now the process up to the first part of the process is done. The second part
will not be described given that I have not understood it fully yet.</p>

<p>The followings are some notes on properties of such process:</p>

<ol>
  <li>The key of depth is reusing information hierarchically and preventing
exploration on meaningless part of function space. This is the main reason
why deep model works and will be elaborated with an example in the next
section.</li>
  <li>Coordinates are coordinates. There is only one ground truth of
nature. Coordinates are the way we see the ground truth. Different spaces
are actually different with different levels of abstraction. Different
levels of abstraction are achieved using different sets of bases.</li>
  <li>Taking inner product is the process to map point in one space to another
space. This is also the intuition behind Riesz Theorem that any linear
functional could be represented using inner product.</li>
  <li>The intuition of taking inner product is also computing correlation between
the basis and the signal. The more they are similar, the bigger the
coordinate(in CNN terminology system, the bigger the response). </li>
  <li>Bases are connected with hidden factors referred by Bengio.</li>
  <li>Spatial information is kept by the intuition of cascading local receptive
fields.</li>
</ol>

<hr />

<h2 id="analytically-why-more-depth-is-more-powerful">Analytically Why More Depth Is More Powerful</h2>

<p>Imagine a square whose components are two horizontal edges and two vertical
edges. To detect such shape, we could imagine a three layer convolutional
neural network. In the first layer of filters, we have two kernels – one
for horizontal edges and one for vertical edges. Write them in matrix form,
they are like this:</p>

<p>Horizontal Edge Kernel</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
0 & 0 & 0 \\
1 & 1 & 1 \\
0 & 0 & 0
\end{bmatrix}
 %]]&gt;</script>

<p>and Vertical Edge Kernel</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0
\end{bmatrix}
 %]]&gt;</script>

<p>The square we are going to detect is like this:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
1   & 1 & 1 & 1 &  1 \\
1   & 0 & 0 & 0 &  1 \\
1   & 0 & 0 & 0 &  1 \\
1   & 0 & 0 & 0 &  1 \\
1   & 1 & 1 & 1 &  1 
\end{bmatrix}
 %]]&gt;</script>

<p>As we moving the two kernels, we get response similar like this:</p>

<p>feature map for horizontal edge detection kernel:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
x & 1 & x \\
0 & 0 & 0 \\
x & 1 & x
\end{bmatrix}
 %]]&gt;</script>

<p>and feature map for vertical edge detection kernel:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
x & 0 & x \\
1 & 0 & 1 \\
x & 0 & x
\end{bmatrix}
 %]]&gt;</script>

<p>$x$ is the response of angle, which may not be black and white. And since they
are not very influential to our discussion, it is denoted $x$.</p>

<p>In the next convolution layer, the input are two feature maps, then the kernel
for detecting square will be like:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
x & 1 & x \\
0 & 0 & 0 \\
x & 1 & x
\end{bmatrix}
and
\begin{bmatrix}
 x & 0 & x \\
 1 & 0 & 1 \\
 x & 0 & x
\end{bmatrix}
 %]]&gt;</script>

<p>The final inner product will be normalized to one. In this case, we are only
using four kernels.</p>

<p>If we switch to one layer CNN, The kernel for detecting square will be like:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
x   & 1 & 1 & 1 & x \\
1   & 0 & 0 & 0 & 1 \\
1   & 0 & 0 & 0 & 1 \\
1   & 0 & 0 & 0 & 1 \\
x   & 1 & 1 & 1 & x
\end{bmatrix}
 %]]&gt;</script>

<p>The zero and one is expanded because each edges consist of more than one
pixels. Three here is used for illustration.</p>

<p>It seems that we can detect shape only using one kernel. But wait… we are not
only trying to detect only square. We should also try to detect circle,
rectangles etc… So we will have more than more kernels. In this case, the
space of possible number of kernels will exponentially explode given the
possible changes we may swap the zero and one in the pixels of kernels.</p>

<p>But in previous three layer cases, the space of possible changes are shrunk
dramatically. The key here is explained in previous section, we are reusing the
information that only consecutive edges are sensible combination of pixels and
the number of possibilities of changing locations of edges is much smaller than
changing values of pixels.</p>

]]></content>
  </entry>
  
</feed>
