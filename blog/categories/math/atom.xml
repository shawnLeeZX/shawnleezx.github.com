<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Math | SHAWN LEE]]></title>
  <link href="http://shawnLeeZX.github.io/blog/categories/math/atom.xml" rel="self"/>
  <link href="http://shawnLeeZX.github.io/"/>
  <updated>2015-11-02T11:13:52+08:00</updated>
  <id>http://shawnLeeZX.github.io/</id>
  <author>
    <name><![CDATA[Shawn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[On Linear Programming, Duality of LP and Operator, a Functional Analysis Point of view]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view/"/>
    <updated>2015-10-18T12:39:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view</id>
    <content type="html"><![CDATA[<p>Yet another note on Math, about visualization of Linear Programming, some
thoughts on Operator and Adjoin.</p>

<!-- more -->

<h2 id="dual-map-and-adjoin-operator">Dual Map and Adjoin Operator</h2>

<p>Denote the matrix of a linear mapping $T$ corresponds to natural bases as $A$,
then $A^{T}$ is the matrix corresponds to the dual map of $T$. More
specifically, if the bases of the two space, the original space and its dual,
are orthogonal respectively, $A^{T}$ is the adjoin operator of $A$. Now suppose
$A \in R^{M \times N}$.</p>

<p>For a linear programming problem, the constraints are $Ax \leq b$, intuitively,
we understand it as a polyhedron, the intersection of finite number of
halfspaces.</p>

<p>We could also understand it from linear mapping and dual bases point of view.</p>

<p>$A$ maps points from $R^{N}$ to $R^{M}$. A column of $A$ corresponds to the
coordinates of the an natural basis of $R^{N}$ in the new $R^{M}$, meanwhile,
a column of $A^{T}$, which is a row of $A$, corresponds to the coordinates of a
dual basis of a natural basis of $R^{M}$ in $R^{N}$ if it is transformed by
$A^{T}$.</p>

<p>So a row $\vec{a_{i}}$ of $A$ is a $A^{T}$ transformed dual basis that
corresponds to one natural basis in $R^{M}$, which corresponds to a hyperplane
in $R^{N}$, also an element of the dual space of $R^{N}$.</p>

<h2 id="on-the-feasible-region-of-linear-programming">On the Feasible Region of Linear Programming</h2>

<p>Now we try to see whether we could get a picture from the above concepts.</p>

<h3 id="a-previous-possibly-wrong-attempt">A Previous Possibly Wrong Attempt</h3>

<p>This is the first visualization I have tried. It turns out not that right after
I discussed the idea with my optimization course teacher.</p>

<p>From this perspective, the polyhedron in $R^{N}$ is a squashed(or enlarged)
quadrant of $R^{M}$, which is the intersection of hyperplanes whose normals are
natural bases of $R^{M}$ — the hyperplanes only need to pass a constant
value, probably corresponding dimension of $b$(have not thought very clearly),
do not necessarily pass origin point.</p>

<p>If $M &gt; N$, $R^{M}$ gets squashed, and there are much freedom in the way how
the space gets squashed, which is analog with we have more equations than
variables, so it is easier to get solutions. Otherwise, $R^{M}$ gets enlarged,
however, no matter how the enlarging is done, it still remains as a low
dimensional space in the high dimensional space, thus the freedom is limited,
which is analog with more variables than equations, so it is harder or
impossible to get solutions.</p>

<h3 id="reason-in-the-standard-form">Reason In the Standard Form</h3>

<p>If we convert the problem to the standard form of linear programming, we could
see that the dimension or the original space is usually larger than the dual
space, otherwise, the feasible region could be easily empty. In this case, the
argument, that the dual space gets squashed, does not hold. But a different
picture turns out immediately.</p>

<p>Intuitively, which means it is not necessarily right, as long the objective
function does not parallel with one of the constraints, the solution to LP is a
basic solution, otherwise, we could move the point along one of the constraint
to change the objective value. Now we only discuss basic solutions.</p>

<p>For every basic solution, we have $N$ constraints being active. Note that $N$
is the dimension of the dual space. Thus, each set of $N$ active constraints
corresponds to a version of dual space that gets mapped from the standard basis
of dual space by those columns of $A^{T}$. More specifically, each active
hyperplane in the $R^{M}$ is a basis in the dual space $R^{N}$.</p>

<h2 id="on-duality-of-linear-programming">On Duality of Linear Programming</h2>

<p>This leads to an intuition about how on earth the dual problem of LP is
conceived in the first place, and makes its extension to conic programming
clear.</p>

<p>From my understanding, the dual in Optimization is to find a lower bound of the
distance in the original space in its dual space, being it the shortest
distance from a point to a subspace, from a point to a closed convex set or a
convex epigraph to a concave epigraph of functions. Those three examples, which
is three types of dual in infinite dimensional optimization try to find a
linear approximate lower bound for a normally non-linear objective
function. Their dual problems are usually some kind of LP, or other forms could
be easily solved. But for LP, all parts in the problem is already linear. From
this line of thinking, the dual of LP is to find a lower bound in the dual
space of $Ax$. How could this be? The key lies in the adjoin operator.</p>

<p>The standard form of LP is</p>

<script type="math/tex; mode=display">
\max\ c^{T}\\
Ax = b\\
x \geq 0
</script>

<p>We have an adjoin relation</p>

<script type="math/tex; mode=display">
\langle Ax, y\rangle = \langle x, A^{T}y \rangle\\
\langle b, y\rangle = \langle x, A^{T}y\rangle\\ 
\langle b, y\rangle \leq \langle x, c\rangle
</script>

<p>The last inequality holds because we have $x \geq 0$.</p>

<p>Conic programming just extends the meaning of $\geq$ and inner product.</p>

<h2 id="the-algebraic-nature-of-operator">The Algebraic Nature of Operator</h2>

<p>All of the thinking about originates from the way I learn Math. I want to get a
picture from the symbols and relationships. Up to now, all concepts I know
could be put into a 2D or 3D system with or without coordinates in my mind,
being it topology space, metric space, Banach space, inner product space,
Hilbert space, dual space, affine set, convex set, cone, measure, probability
measure and so on. Dual in optimization is a new concept, and I want to put it
somewhere in the system. In about 4 days, I almost finished the book
<em>Optimization by Vector Space Method</em>, which is about infinite dimensional
optimization, and learned the three kinds of duals mentioned before. I indeed
came up with similar visualization tricks mentioned in the book before I read
them. But in the section, some words rang a bell, after a few days I read them
and pondered on the meaning of adjoin, the relation between the $Ax$ and $X$.</p>

<p>At the beginning of the Chapter Six</p>

<blockquote>
  <p>Because it is difficult to obtain a simple geometric representation of an
arbitrary linear operator, the material in this chapter tends to be somewhat
more algebraic in character than that of other chapters. Effort is made,
however, to extend some of the geometric ideas used for the study of linear
functionals to general linear operators and also to interpret adjoints in
terms of relations among hyperplanes.</p>
</blockquote>

<p>The point of an operator $A$ is to transform a point in $X$ to a new space for
some purpose. It is not clear to see why it helps in the general form, but
think Fourier Transform, which is to transform a point in the signal space to
the frequency space. Boom, science advanced. In retrospect, do we need to keep
a picture in the natural basis of signal space when considering the frequency
space? No. In some research paper I read, for instance, scattering transform,
the whole is to make sure the new space has good properties.</p>

<p>So for operators, the most important is to enforce regularities to make sure
there is good properties in the new space, which is algebraic in nature instead
of geometrical.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Connection between PCA and Fourier Transform]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/06/05/connection-between-pca/"/>
    <updated>2015-06-05T21:10:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/06/05/connection-between-pca</id>
    <content type="html"><![CDATA[<!-- more -->

<p>When we are running PCA on images, the first principle component we get is
normally looks like the DC term, which is the first Fourier bases of
images. After some analysis, I found that this makes a lot of sense.</p>

<p>We could unify PCA and Fourier Transform from filter bank or linear operator
point of view. Actually those two point of views are just different way to say
the same thing in signal processing and mathematics.</p>

<p>From filter bank point of view, the process of getting the PCA coefficients and
Fourier coefficients could be regarded as passing the original signal to a
Linear Time Invariant system. Each Fourier basis or eigenvector is a
filter. All of them make up the filter bank.</p>

<p>From linear operator point of view, such process could be taken as an operator
to project the point in one space to another space.</p>

<p>It turns out that Fourier basis $e^{i\omega t}$ and eigenvector $v$ of covariance
matrix are both eigenvector(this eigenvector is not specific to the eigenvector
computed from covariance matrix of the data) of the LTI system.</p>

<p>The Fourier basis eigenvectors could be taken as the eigenvector of covariance
of all possible images that could be formed by spatial complex exponential
signal. So it is fixed. As for the data driven eigenvector $v$, it is only the
eigenvector of that amount of data. They are both orthogonal with other
eigenvectors within their sets. If data driven eigenvector get normalized, it
has unit norm as well.</p>

<p>This could explain why the first principal components of covariance looks a lot
similar do the first Fourier basis. Because DC term of images affect all
pixels, naming all dimensions. So its variance is the largest, which makes it
similar with first fourier basis. I think similar argument could be made to
other principal components and Fourier bases as well. But they may be less
obvious then the first one.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Concept Explanation on Composition of Matrice Representing Linear Transform]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/05/22/a-concept-explanation-on-composition-of-matrice-representing-linear-transform/"/>
    <updated>2015-05-22T22:18:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/05/22/a-concept-explanation-on-composition-of-matrice-representing-linear-transform</id>
    <content type="html"><![CDATA[<!-- more -->

<p>It is a bit hard to internalize the fact that the composition of finite
dimensional linear transformation is the multiplication of the matrices
representing them. I figured out a concept explanation.</p>

<p>Denoting the first transformation matrix as $A$ and the second as $B$,
$\vec{a_{i}}$ as the row of $A$ and $\vec{b_{j}}$ as the row of $B$, the to
be transformed vector as $x$, basically, each $\vec{a}_{i}$ determines how
each coordinate $x_{i}$ of $\vec{x}$ will contribute to the result
coordinates.</p>

<p>Then if you do another linear transform on $\vec{Ax}$, each $\vec{b}_{j}$ determines how
each coordinate of $\vec{Ax}$ will contribute to the final vector. Since the transform is
linear, we could first consider how $\vec{a_{i}}$ contributes to the final
vector, its final result with $x$ is just a multiple of $x_{i}$. Each
component of $\vec{a_{i}}$ actually could be taken as a to be transformed
vector of $B$. Each row of $AB$ could be viewed in the following way.</p>

<p>$\vec{a_{i}}$ is the new coordinate of standard basis $\vec{e_{i}}$ after
applying $A$. Starting from there, the second new coordinates of those first
new coordinates are obtained by applying $B$. So if we apply $AB$ on standard
basis $\vec{e_{i}}$, the result is going to be the second new
coordinates. Actually, such coordinates are:</p>

<script type="math/tex; mode=display">
\sum\vec{b_{j}}(\vec{a_{i}})_{j}
</script>

<p>which is just $\vec{(AB)_{i}}$.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What A Mathematical Object Random Variable Really Is]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/05/14/what-mathematical-object-random-really-is/"/>
    <updated>2015-05-14T11:14:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/05/14/what-mathematical-object-random-really-is</id>
    <content type="html"><![CDATA[<p>An example will be noted here to illustrate what a random variable really is.</p>

<p>Sadly, things will be clearer with graph but for now I do not have a solution
to make good illustration really quickly enough — normally I write a post in
about half an hour.</p>

<!-- more -->

<p>The problem setting is that if we toss a coin twice, there are four possible
outcomes that we could get — $(hh, ht, th, tt)$, where $h$ stands for head
while $t$ stands for tail. If we denote the results as tuples, we could
represent the experiment results as $((1, 1), (1, 0), (0, 1), (0, 0))$. If we
take each element of the tuple with two elements as dimension of a vector
space, each tuple could be regarded as an element in a vector space. we could
represent our experiment result in a 2D plane.</p>

<p>Consider that a random variable $X$ which is used to denote the number of heads
in this experiment. Its value would be 0, 1 or 2.</p>

<p>Now we make the statement that random variable a mapping, which maps the
original event space $\Omega$(the four tuples here) to a subset$T’$(a set of
${0, 1, 2}$ here) of a set $T$(the positive integer set $N^{+}$). Let’s see
what this actually means.</p>

<p>In the first level, the understanding could be pretty straight forward. $X$
in some sense agglomerate the event space in the most preliminary level – the
atomic exclusive level to a coarser level — the number of heads. So $(1, 0)$
and $(0, 1)$ all collapse to the point ${1}$ in the set $T$. Correspondingly,
the probability measure assigned on each event is changed. Mathematically,
given a set $A$ in $T$, its probability now is </p>

<script type="math/tex; mode=display">
P^{X}(A) = P(\{\omega: X(\omega) \in A \}) = P(X^{-1}(A)) = P(X \in A)
</script>

<p>However, there is a way to think in the framework of functional analysis. Each
mapping, precisely, function, in a vector space(Hilbert space could be a
properer setting) could be regarded as projecting points in the vector space to
the basis determined by the adjoint operator of this function.</p>

<p>In previous example, a two dimensional space is reduced into one dimensional
space, whose basis is the adjoint operator of the function $X$. And the
probability measure of a point in this new one dimensional space is the sum of
all points in the two dimensional points that will be projected at that
point. Now, let’s see what those statements mean.</p>

<p>If we draw a line crossing origin with slope one, all points with the same
number of heads will be projected at the same point in this line. The point
with two heads, which is $(1, 1)$, will be projected at point $(1, 1)$. The
points with one head, which is $(1, 0) and (0, 1)$, will be projected at
$(\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2})$. Finally, the point with no head
will be projected at $(0, 0)$. Now we drop the second dimension, each point in
this line will semantically mean the number of heads, where the random variable
will vary. Their probability measure will be summed while points are being
projecting.</p>

<p>The counting of heads is also represented by the function $(x + y)$(the value
in each dimension of previous two dimensional space actually means the number
of heads, so their sum will the number of heads), which is the adjoint operator
of basis $(1, 1)$. This form is the representation of the basis of our new
space in our old two dimensional space.</p>

<p>One last note. When we are reducing dimensionality, we are also discarding the
information of how many number of tail we get.</p>

<p>In conclusion, what random variable really is?</p>

<p>Random variable first maps original event space onto another event space, then
defines a new probability distribution on the new space according its relation
to the old event space. The intuitive meaning of the random variable is how it
deals with original event space, which is counting the number of heads
here. But essentially, it defines a distribution or in other words, probability
measure at each point(the number of heads), in the new space.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Connection between Fourier Components in Real and Complex Form]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/04/11/connection-between-fourier-components-in-normal-and-complex-form/"/>
    <updated>2015-04-11T10:59:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/04/11/connection-between-fourier-components-in-normal-and-complex-form</id>
    <content type="html"><![CDATA[<p>This post notes down my current understanding of the intuition behind using
complex exponentials to represent function, stressing on the connection between
the complex form and the normally all real form.</p>

<!-- more -->

<h2 id="brief-note-on-complex-number">Brief Note on Complex Number</h2>

<p>One of the most useful properties of complex number is its ability to unify
algebraic and geometrical operations. More about this could be found on
<a href="http://betterexplained.com/articles/a-visual-intuitive-guide-to-imaginary-numbers/">Better Explained</a>
which is a good elementary explanation of imaginary number.</p>

<p>It was not until seriously studying Fourier Transformation that I figured out
the nature of Complex Number.</p>

<p>Number system is human’s way to model nature. Integer number models
counting. Negative number models things like debt. Zero models null. Fraction
number models division and fraction. Those consist of rational number. But to
really model length, which occurs naturally in Pythagoran theorem, irrational
number has to be introduced. Rational number and irrational number together
make up real number system, where limit of points – may be compared with
limit of length is regarded as a conceptual number, to make the system
complete(complete is a mathematical terminology).</p>

<p>But all those models are for scalar. Nature is not one dimensional. It is
rules of vector that engine this world. This is where complex comes
from. </p>

<p>The most elegant equation in the history is Euler Equation:</p>

<script type="math/tex; mode=display">
e^{ix} = cos(x) + i sin(x)
</script>

<p><img class="center" src="http://upload.wikimedia.org/wikipedia/commons/7/71/Euler%27s_formula.svg" title="Euler Equation Illustration" ></p>

<p>It is saying that every complex exponential is a 2D vector, where $(x, y) =
(cos\phi, sin\phi)$. Instead of just modeling scalar, we model 2D vector using
complex number. So by manipulating algebra of complex, we are doing geometrical
operations. And the meaning of imaginary $i$ is rotating the vector 90 degree
anti-clockwise. That’s the reason why it connects algebra and geometry.</p>

<p>This will be clearer after Fourier related stuff is introduced in the following
sections.</p>

<p>I am still wondering if we could unify more dimensions into number instead of
just two?</p>

<h2 id="fourier-series">Fourier Series</h2>

<p>Fourier Series is about this statement:</p>

<blockquote>
  <p>Every periodic function could be represented using arbitrary number of
ocsillating functions, namely cosines and sines or complex exponentials.</p>
</blockquote>

<p>As noted in the beginning, this post is about intuition behind the relationship
between cosines sines and complex exponentials.</p>

<p>First we see fourier series in its real form decomposition.</p>

<script type="math/tex; mode=display">
f(t) = \sum\limits_{n = -\infty}^{\infty} c_{n}cos(nw_{0}t + \phi)
</script>

<p>We could also write it in form of $sin(t)$ given that it is just a shifted
version of $cos(t)$. But we will see that using $cos(t)$ makes sense in a
moment.</p>

<p>NOW, every $cos(nw_{0}t + \phi)$ is the real part of $e^{i(nw_{0}t + \phi)}$.
Remember that the algebra operations between complex exponentials are
equivalent to vector operations. So instead seeing combination of different
Fourier components as superposition of constantly changing(with regard to $t$)
scalar addition, we see it as vector addition in a 2D dimensional complex
plane. See illustration
<a href="http://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/">here</a>,
in about half the article.</p>

<p>Though they vibrate in circle, which indicates the way the interact with each
other, we only see its effect – cosine(real) part. So each time we measure or
in other word, sample, we get the real part, which is the final function we
want.</p>

<p>By converting back and forth between the real form and complex form, you will
find terms related to frequency $s$ is $cos(st + \phi)$ and
$\frac{1}{2}e^{ist}e^{i\phi} + \frac{1}{2}e^{-ist}e^{-i\phi}$. The negative
frequency part is used to “drag” back or cancel out the imaginary or the part
that is orthogonal to real part. Or in other word, we only want the real part.</p>

<p>Intuitively, it means to make the decomposition conforming with observation, we
use conjugation to drag the vector back to real line. Now it makes sense that
we choose $cos(nw_{0}t + \phi)$ to decompose instead of $sin(nw_{0}t +
\tau)$. Cosine is related to the x-axis by definition which is real line.</p>

<p>We could stop here. The connection between the real form and the complex form
is pretty clear now. But there is more, in a finer level.</p>

<p>Given that each components of the decomposition is orthogonal, now we need only
to consider one of its term $cos(nw_{0}t + \phi)$.</p>

<p>From now on for notation simplicity we denote $nw_{0}$ as $s$. Each component
could also be decomposed into two components, remembering that we see each
component as two dimension vector. The phase $\phi$ determines the vector
initially. We can see this clearer by expanding($1/2$ is dropped for
convenience):</p>

<script type="math/tex; mode=display">
e^{ist}e^{i\phi} + e^{-ist}e^{-i\phi} = e^{ist}(cos(\phi) + i sin(\phi)) +
e^{-ist}(cos(\phi) - i sin(\phi))
</script>

<p>THIS actually means in a finer level, the innate vector property is also
modeled – every component consists of two basic components, sine and cosine
and $\phi$ decides how they combine with each other initially, which determines
how they interact with each other forever.</p>

<p>The decomposition into sine and cosine also makes a lot of sense. cos is
orthogonal to sine and together they can represent any trigonometric function.</p>

<h2 id="note-on-other-derivation">Note On Other Derivation</h2>

<p>There is another way to get real form of fourier series to complex
form. Actually it is the first time I realize the connection between them,
though fourier was learned way earlier(TERRIBLE COLLEGE EDUCATION!!!).</p>

<p>Fourier decomposition is written the following way:</p>

<script type="math/tex; mode=display">
f(t) = \sum\limits_{n = -\infty}^{\infty} c_{n}sin(nw_{0}t + \phi)
</script>

<p>By some derivation, we have:</p>

<script type="math/tex; mode=display">
f(t) = \sum\limits_{n = -\infty}^{\infty} a_{n} cos(nw_{0}t) + b_{n}sin(nw_{0}t)
</script>

<p>The phase is merge into coefficients $a_{n}$ and $b_{n}$.</p>

<p>Given $sin(t) = \frac{1}{2i}(e^{it} - e^{-it})$ and $cos(t) = \frac{1}{2}(e^{it} + e^{-it})$,
substitute above equations in, we also get the complex form.</p>

<p>This approach is harder to understand because it gets the atomic level wrong –
it should be $e^{it}$ not $cos(t) + i sin(t)$. But it does have benefits.</p>

<p>Each function $f$ could be decompose into a combination of even and odd part
by:</p>

<script type="math/tex; mode=display">
f = \frac{1}{2}(f(t) + f(-t)) + \frac{1}{2}(f(t) - f(-t))
</script>

<p>The formed is the even part and the latter is the odd part. Let $f(t) = e^{it}$
we can see that $i sin(t)$ the odd part of $e^{it}$ and $cos(t)$ the even part.</p>

<p>It makes clear that when the function is even, we get zero value in coefficients
of $sin(t)$ while when the function is odd, we get zero value in coefficients
of $cos(t)$: if a function has a odd component $sin(t)$, it never is going to
be even.</p>
]]></content>
  </entry>
  
</feed>
