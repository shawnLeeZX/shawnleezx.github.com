<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Functional_Analysis | SHAWN LEE]]></title>
  <link href="http://shawnLeeZX.github.io/blog/categories/functional-analysis/atom.xml" rel="self"/>
  <link href="http://shawnLeeZX.github.io/"/>
  <updated>2016-01-05T12:03:41+08:00</updated>
  <id>http://shawnLeeZX.github.io/</id>
  <author>
    <name><![CDATA[Shawn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[On Linear Programming, Duality of LP and Operator, a Functional Analysis Point of view]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view/"/>
    <updated>2015-10-18T12:39:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/10/18/linear-programming-from-dual-mapping-point-of-view</id>
    <content type="html"><![CDATA[<p>Yet another note on Math, about visualization of Linear Programming, some
thoughts on Operator and Adjoin.</p>

<!-- more -->

<h2 id="dual-map-and-adjoin-operator">Dual Map and Adjoin Operator</h2>

<p>Denote the matrix of a linear mapping $T$ corresponds to natural bases as $A$,
then $A^{T}$ is the matrix corresponds to the dual map of $T$. More
specifically, if the bases of the two space, the original space and its dual,
are orthogonal respectively, $A^{T}$ is the adjoin operator of $A$. Now suppose
$A \in R^{M \times N}$.</p>

<p>For a linear programming problem, the constraints are $Ax \leq b$, intuitively,
we understand it as a polyhedron, the intersection of finite number of
halfspaces.</p>

<p>We could also understand it from linear mapping and dual bases point of view.</p>

<p>$A$ maps points from $R^{N}$ to $R^{M}$. A column of $A$ corresponds to the
coordinates of the an natural basis of $R^{N}$ in the new $R^{M}$, meanwhile,
a column of $A^{T}$, which is a row of $A$, corresponds to the coordinates of a
dual basis of a natural basis of $R^{M}$ in $R^{N}$ if it is transformed by
$A^{T}$.</p>

<p>So a row $\vec{a_{i}}$ of $A$ is a $A^{T}$ transformed dual basis that
corresponds to one natural basis in $R^{M}$, which corresponds to a hyperplane
in $R^{N}$, also an element of the dual space of $R^{N}$.</p>

<h2 id="on-the-feasible-region-of-linear-programming">On the Feasible Region of Linear Programming</h2>

<p>Now we try to see whether we could get a picture from the above concepts.</p>

<h3 id="a-previous-possibly-wrong-attempt">A Previous Possibly Wrong Attempt</h3>

<p>This is the first visualization I have tried. It turns out not that right after
I discussed the idea with my optimization course teacher.</p>

<p>From this perspective, the polyhedron in $R^{N}$ is a squashed(or enlarged)
quadrant of $R^{M}$, which is the intersection of hyperplanes whose normals are
natural bases of $R^{M}$ â€” the hyperplanes only need to pass a constant
value, probably corresponding dimension of $b$(have not thought very clearly),
do not necessarily pass origin point.</p>

<p>If $M &gt; N$, $R^{M}$ gets squashed, and there are much freedom in the way how
the space gets squashed, which is analog with we have more equations than
variables, so it is easier to get solutions. Otherwise, $R^{M}$ gets enlarged,
however, no matter how the enlarging is done, it still remains as a low
dimensional space in the high dimensional space, thus the freedom is limited,
which is analog with more variables than equations, so it is harder or
impossible to get solutions.</p>

<h3 id="reason-in-the-standard-form">Reason In the Standard Form</h3>

<p>If we convert the problem to the standard form of linear programming, we could
see that the dimension or the original space is usually larger than the dual
space, otherwise, the feasible region could be easily empty. In this case, the
argument, that the dual space gets squashed, does not hold. But a different
picture turns out immediately.</p>

<p>Intuitively, which means it is not necessarily right, as long the objective
function does not parallel with one of the constraints, the solution to LP is a
basic solution, otherwise, we could move the point along one of the constraint
to change the objective value. Now we only discuss basic solutions.</p>

<p>For every basic solution, we have $N$ constraints being active. Note that $N$
is the dimension of the dual space. Thus, each set of $N$ active constraints
corresponds to a version of dual space that gets mapped from the standard basis
of dual space by those columns of $A^{T}$. More specifically, each active
hyperplane in the $R^{M}$ is a basis in the dual space $R^{N}$.</p>

<h2 id="on-duality-of-linear-programming">On Duality of Linear Programming</h2>

<p>This leads to an intuition about how on earth the dual problem of LP is
conceived in the first place, and makes its extension to conic programming
clear.</p>

<p>From my understanding, the dual in Optimization is to find a lower bound of the
distance in the original space in its dual space, being it the shortest
distance from a point to a subspace, from a point to a closed convex set or a
convex epigraph to a concave epigraph of functions. Those three examples, which
is three types of dual in infinite dimensional optimization try to find a
linear approximate lower bound for a normally non-linear objective
function. Their dual problems are usually some kind of LP, or other forms could
be easily solved. But for LP, all parts in the problem is already linear. From
this line of thinking, the dual of LP is to find a lower bound in the dual
space of $Ax$. How could this be? The key lies in the adjoin operator.</p>

<p>The standard form of LP is</p>

<script type="math/tex; mode=display">
\max\ c^{T}\\
Ax = b\\
x \geq 0
</script>

<p>We have an adjoin relation</p>

<script type="math/tex; mode=display">
\langle Ax, y\rangle = \langle x, A^{T}y \rangle\\
\langle b, y\rangle = \langle x, A^{T}y\rangle\\ 
\langle b, y\rangle \leq \langle x, c\rangle
</script>

<p>The last inequality holds because we have $x \geq 0$.</p>

<p>Conic programming just extends the meaning of $\geq$ and inner product.</p>

<h2 id="the-algebraic-nature-of-operator">The Algebraic Nature of Operator</h2>

<p>All of the thinking about originates from the way I learn Math. I want to get a
picture from the symbols and relationships. Up to now, all concepts I know
could be put into a 2D or 3D system with or without coordinates in my mind,
being it topology space, metric space, Banach space, inner product space,
Hilbert space, dual space, affine set, convex set, cone, measure, probability
measure and so on. Dual in optimization is a new concept, and I want to put it
somewhere in the system. In about 4 days, I almost finished the book
<em>Optimization by Vector Space Method</em>, which is about infinite dimensional
optimization, and learned the three kinds of duals mentioned before. I indeed
came up with similar visualization tricks mentioned in the book before I read
them. But in the section, some words rang a bell, after a few days I read them
and pondered on the meaning of adjoin, the relation between the $Ax$ and $X$.</p>

<p>At the beginning of the Chapter Six</p>

<blockquote>
  <p>Because it is difficult to obtain a simple geometric representation of an
arbitrary linear operator, the material in this chapter tends to be somewhat
more algebraic in character than that of other chapters. Effort is made,
however, to extend some of the geometric ideas used for the study of linear
functionals to general linear operators and also to interpret adjoints in
terms of relations among hyperplanes.</p>
</blockquote>

<p>The point of an operator $A$ is to transform a point in $X$ to a new space for
some purpose. It is not clear to see why it helps in the general form, but
think Fourier Transform, which is to transform a point in the signal space to
the frequency space. Boom, science advanced. In retrospect, do we need to keep
a picture in the natural basis of signal space when considering the frequency
space? No. In some research paper I read, for instance, scattering transform,
the whole is to make sure the new space has good properties.</p>

<p>So for operators, the most important is to enforce regularities to make sure
there is good properties in the new space, which is algebraic in nature instead
of geometrical.</p>
]]></content>
  </entry>
  
</feed>
