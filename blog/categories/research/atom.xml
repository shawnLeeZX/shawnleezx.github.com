<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Research | SHAWN LEE]]></title>
  <link href="http://shawnLeeZX.github.io/blog/categories/research/atom.xml" rel="self"/>
  <link href="http://shawnLeeZX.github.io/"/>
  <updated>2016-01-09T10:26:01+08:00</updated>
  <id>http://shawnLeeZX.github.io/</id>
  <author>
    <name><![CDATA[Shawn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[On Frameworks of Deep Learning]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/08/01/on-frameworks-of-deep-learning/"/>
    <updated>2015-08-01T14:30:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/08/01/on-frameworks-of-deep-learning</id>
    <content type="html"><![CDATA[<p>I have spent the last two days trying to figure out what do use as a solid base
for further experiments on deep learning. I decided to settle down to Keras for
the time being. But given that it may not be the optimal choice, I want to note
down what I have tried so that it would be easier to pick up in the future.</p>

<!-- more -->

<p>At first, our lab members are using Caffe, which I have spent some effort
learning it. However, as I got to know more in this field, combining with my
background in Computer Science, Caffe is not really the best platform to do
experiments, at least not for the people who want to understand and experiment
on all the details of various neural network elements and added another layer
of unnecessary engineering burden to the experiments.</p>

<p>Starting from the motivation to actually code NN from scratch, a language as
low level as C++ is not the right choice. Maybe at the time that I am really
equipped and really need large scale experiments, I will get back to Caffe.</p>

<p>Then I came to try Torch. At first I held high expectation given that:</p>

<ol>
  <li>The easy binding between Lua and C.</li>
  <li>The API of torch is rather clean and most of the non-computational intensive
are actually written in Lua, which would save a ton of time.</li>
  <li>The <code>image</code> package of Torch is great given the experience to do
visualization under Python.</li>
  <li>Lua is not a barrier to me, given that I learn a new language rather fast.</li>
</ol>

<p>Then I spent some time to learn the code organization of Torch. Basically,
Torch is a combination of <a href="http://luajit.org/">LuaJIT</a> with Lua packages and C
Libraries. It uses CMake to compile the project. So far so good.</p>

<p>But the real problem comes when I tried to find out a reasonable environment to
write Lua code. SURPRISINGLY, both Emacs and Vim does not have a work
environment for Lua. Emacs does not even have a usage syntax highlighting
for Lua. As for Vim, it does have a usage syntax highlighting, but it stops at
that. All other packages:</p>

<ul>
  <li><a href="http://www.vim.org/scripts/script.php?script_id=3169">luainspect.vim</a>:
Semantic highlighting for Lua in Vim</li>
  <li><a href="http://www.vim.org/scripts/script.php?script_id=4950">Lua Support 2</a> : Lua
IDE. Insert codesnippets, run, compile, and check the code and look up help.</li>
  <li><a href="http://www.vim.org/scripts/script.php?script_id=3625">lua.vim</a> : Lua file
type plug-in for the Vim text editor</li>
  <li><a href="http://www.vim.org/scripts/script.php?script_id=3331">lua_omni</a> : omni
completion for Lua plus few extras</li>
</ul>

<p>are buggy in some extent and I really do not have time to fix them.</p>

<p>Then I tried to find some IDE recommended by the Torch documentation. There are
Eclipse with LDT, zbs-torch and some others, which all sucks. Maybe I have been
using Emacs and Vim for too long…</p>

<p>After about one day’s struggling with all different IDEs, I decided to try new
frameworks based on Theano, even they are all in their infancy.</p>

<p><a href="https://blocks.readthedocs.org/en/latest/">Pylearn2</a> and
<a href="https://blocks.readthedocs.org/en/latest/">Blocks &amp; Fuel</a> feels a bit
convoluted in design. Opendeep, Lasagne, Keras look alike. Given Keras offers
to enforce constraints on parameters, I decided to try Keras first.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Progress Buffer for Big Picture Building]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/05/07/progress-buffer-for-big-picture-building/"/>
    <updated>2015-05-07T08:37:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/05/07/progress-buffer-for-big-picture-building</id>
    <content type="html"><![CDATA[<p>I think I will fully devote to a project from now on. To be able to back on the
work to build an overall big picture of perception problem of artificial
intelligence in the future, I try to summarize the progress here.</p>

<!-- more -->

<h2 id="wavelet">Wavelet</h2>

<h3 id="transition-from-fourier-to-wavelet">Transition from Fourier to Wavelet</h3>

<p>The learning of one dimensional Fourier transform part of Fourier transform is
finished. The key of transition from Fourier analysis to wavelet analysis in
transient signal analysis is learned, which will be described in the following:</p>

<p>Fourier transform is actually a scan over all frequency spectrum on the
signal. For a time-limited signal, or mathematically non-periodic signal, the
value of the signal outside the time-limited interval is zero not because it is
indeed zero in nature but we do not care about its value. So if we use all
spectrum of Fourier transform to get its new representation in frequency
domain, the coefficients whose period are far longer than the time span or far
shorter than the time span will be highly inaccurate — only when the integral
or inner product is taken a integer multiplication of period length, the
coefficients we get is accurate. So in windowed Fourier transform, the
frequency whose period is far longer than the window is synthesized using
different time notes of the signal while the one whose period is far shorter
than the window is synthesized by different high frequency in the same time
note.</p>

<p>Discrete Fourier transform is an approximate of continuous Fourier transform,
which is directly called Fourier transform in previous paragraph. The
approximate is made actual taking the time span of the time-limited signal as
such signal’s period. Similar approximation is also made in the frequency
domain. So we get a periodic frequency span. This is why we have to ensure the
sampling rate is high enough to guarantee that after the periodization of
frequency part, different periods do not overlap with each other.</p>

<p>So to overcome this problem is the key reason to introduce wavelet analysis,
whose bases’ time span are dynamically determined to suit the corresponding
frequency they want to decouple, or in another word, to detect.</p>

<h3 id="goals-next-step">Goals Next Step</h3>

<p>How exactly this idea is implemented thirty years ago should be further
investigated <strong>on the purpose of this research goal</strong>:</p>

<blockquote>
  <p>How could Fourier and wavelet analysis bridges the gap between the physical
entity and things that may have semantic meaning?</p>
</blockquote>

<p>What the sensors of human beings are receiving are actually just physical
signals — the different strength of illumination of light, different
amplitude and frequency of air vibration. The problem solved by scientists in
18th and 19th are most linear problem that deals with the nature instead of how
human deals with nature. The light and air pressure vibration are both
superposition of various basic building blocks of nature’s atom —
electromagnet wave and mechanical vibration wave. Frequency, amplitude and
phase of them are the first layer nature of them. For human beings, the learned
nature of them is actually detecting patterns. The gap between those two would
be very important to understand the perception problem of human race.</p>

<p>To address this gap, wavelet analysis and its high dimensional form could be a
promising direction. To know exactly where this direction could lead, the
following questions and guess should be addressed:</p>

<ol>
  <li>Wavelet that does not use complex exponential directly may be understood as
a layer that stacked on the complex exponential, which is the frequency
nature of the signal – the atom of signal. This may be the reason why
theorem proving of wavelet I have encountered now all depends on Fourier
transform.</li>
  <li>How different kinds of wavelets are designed? One key here is that we want
the optimal time and frequency localization for one particular wavelet
basis. But there must be more in this. Some work have mentioned the
vanishing moment of wavelet. Those design may bring to some very deep
physics facts that is very illuminating.</li>
  <li>One dimensional analysis is actually the special case of higher dimensional
analysis. Everything is a vector in the real world. After getting
familiarity with one dimensional theory, the leap from one dimensional to
higher dimensional could lead to the direct intuition to the real world and
the solving of problem that involving the basic properties of higher
dimensional space, such as 2D and 3D rotation, scaling of signals.</li>
</ol>

<h3 id="execution-plan">Execution Plan</h3>

<p>Fortunately, the survey about which books to read has just been made.</p>

<ul>
  <li><em>Insight Into Wavelets, from Theory to Practice</em> is relatively detailed
enough and contains pertinent amount of practice. It seems to cover relevant
Math, such as function space, nested space, enough intuition and historical
development, the design of wavelet and enough examples. This book may be the
main thread to follow when I back to learn it again.</li>
  <li><em>Ten lectures on wavelets</em> is another candidate main thread to follow. Its
level of detail and conversation style will be further checked when I back to
learn it again.</li>
  <li><em>A Friendly Guide to Wavelets</em> should be used in complement with the first
book. This book is too contingent with examples and proof detail, but its
explanation on intuition is good.</li>
  <li><em>A First Course in Wavelets with Fourier Analysis</em> has not been investigated
much. Its try to deal with Fourier and wavelet at the time could make it
miserable, but referring to it from time to time may be helpful to see their
connection after having learned Fourier and wavelet enough.</li>
  <li><em>The World According to Wavelets The Story of a Mathematical Technique in the
Making</em> is all about history and intuition. The second half that deals with
math has not been read yet. After having learned wavelet, this small book
could be helpful on wavelet’s connection with other discipline and historical
development.</li>
  <li><em>A wavelet tour of signal processing</em> is a reference book. It deals with the
big perspective on the transition from wavelet method to dictionary
learning. This is the book to read after I have learned wavelet well.</li>
  <li><em>Conceptual Wavelets in Digital Signal Processing</em>, <em>A primer on wavelets and
their scientific applications</em>, <em>Ripples In Mathematics</em> and <em>The Illustrated
Wavelet Transform Handbook</em> are more about mechanics of wavelet. Those
mechanics could be helpful after the overall picture of wavelet is gotten.</li>
  <li><em>Wavelets and Subband Coding</em>, <em>Multirate Systems And Filter Banks</em> and
<em>Wavelets and Filter Banks</em> are about more the discrete version of
*wavelet. Those will be helpful when I am working on implementation.</li>
</ul>

<h2 id="bayesian-statistics">Bayesian Statistics</h2>

<h3 id="motivation">Motivation</h3>

<p>There has been twenty years of work on computer vision and machine learning
using Bayesian methods. The initial taking off initialized by Hinton also
origins from statistical method — Deep Belief Net. There are several reasons
to understand Bayesian related methods:</p>

<ol>
  <li>To acquire the ability to understand previous important work and work on
deep learning involving Bayesian methods.</li>
  <li>Statistics is important for biologically inspired work. Neurons in brain
never get activated alone. It is their statistics pattern that matters.</li>
  <li>It is another way to think about problem. If we view the branch of applied
mathematics as modeling the world from a low level, relatively deterministic
perspective, probability viewed it from a high level, semantic way. It is
important to have probability to deal with high level part of deep learning
system.</li>
</ol>

<h3 id="goal-next-step">Goal Next Step</h3>

<p>The first step goal is to understand the fundamental intuition behind
probability and statistics, just like understanding the construction of number
system while studying functional analysis, then how exactly Bayesian methods
depart from traditional frequentism. Computational perspective of Bayesian
method such as sampling could wait.</p>

<h3 id="execution-plan-1">Execution Plan</h3>

<p>The main plan to follow is the thread on Bayesian statistics on
Metaacademy. Books are also important for reference and systemtization of
learning.</p>

<h4 id="basic-probability-and-statistics">Basic Probability and Statistics</h4>
<ul>
  <li><em>An Introduction To Probability Theory And Its Applications</em> is the classic
book on probability. It gets the intuition and motivation right. If knowledge
which needs deeper learning arises, this is the book to consult.</li>
  <li><em>Probability Jim Pitman</em> is an undergraduate textbook on probability.</li>
  <li><em>Statistical Inference</em> is the book to read after having a good understanding
of probability.</li>
  <li><em>Mathematical Statistics and Data Analysis</em> is a good reference for
statistics. It is relatively simpler than <em>Statistical Inference</em>, which it a
good fallback candidate if I have a hard time understanding the book.</li>
  <li><em>Bayesian Data Analysis</em> After the basic foundation of probability and
statistics is laid. This is the book to read to know Bayesian methods.</li>
</ul>

<h4 id="multivariate-statistics">Multivariate statistics</h4>
<ul>
  <li><em>Modern Multivariate Statistical Techniques</em> and <em>An Introduction to
Multivariate Statistical Analysis</em> is the book to consult when jumping into
real world data.</li>
</ul>

<h4 id="modern-probability-theory">Modern Probability Theory</h4>
<ul>
  <li><em>Probability and Random Processes Grimmett</em> tries to cover stuffs that needs
measure theory in using language that without measure theory. It could be a
bridge to the gap of modern probability and classic probability.</li>
  <li><em>Probability Essentials</em> describes the thread of modern probability tersely.</li>
  <li><em>Probability, Theory and Examples</em> is like Mallat’s wavelet tour. It is a
book to read after you have relative good understanding of modern probability
theory. It could offer you a bigger picture.</li>
  <li><em>Foundation of Modern Probability</em> is the most advanced book which may only
be needed by PhD in probability.</li>
</ul>

<h4 id="real-analysis">Real Analysis</h4>
<ul>
  <li><em>Principles of Mathematical Analysis</em> is the book to consult in undergraduate
level.</li>
  <li><em>James Munkres Topology</em> is the book to consult about topology.</li>
  <li><em>Folland. Real Analysis Modern Techniques and their Application</em> is the book
to learn real analysis seriously.</li>
</ul>

<h2 id="link-between-communities">Link Between Communities</h2>

<p>A major breakthrough could be made is that the work on sparse and redundant
dictionary learning in signal processing community, traditional Bayesian
statistics machine learning community, neuroscience community and the community
working on experimental deep learning could be linked together.</p>

<p>What CNN tries to learn is basically a sparse and redundant dictionary that is
used to capture semantic patterns of sensed signal. Signal processing community
could offer theoretical guidance. Neurosciece could offer biological
experimental facts and deep learning community could offer simulation
experiments facts and lastly, Bayesian statistics community could offer ways to
deal with the high level relationship between atom in the dictionary.</p>

<h2 id="ending">Ending</h2>

<p>I think this is all of it. Hope I will be back soon.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Preliminary Summary On What CNN Is Doing Mathematically]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2015/04/07/a-preliminary-summary-on-what-cnn-is-doing-mathematically/"/>
    <updated>2015-04-07T19:05:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2015/04/07/a-preliminary-summary-on-what-cnn-is-doing-mathematically</id>
    <content type="html"><![CDATA[<p>There is too much to stuff all those notes into one weekly summary, so I decide
to decouple it into this post. It is about a description of the process what
Convolutional Neural Network(CNN) is doing mathematically.</p>

<!-- more -->

<p>Last week I have written an informal one. The informality was resulted from an
unclear understanding of space, mapping and the way CNN works. Things get much
clearer now.</p>

<p>The task CNN is trying to achieve is the same with all the other machine
learning algorithms – finding a mapping that will transform the preliminary
input to one kind of output that makes the task easier to solve or directly
solve the task. The formed one is called feature learning and the latter is
called classification or anything else that is the target.</p>

<p>The problem is how exactly such mapping is found? Or more specifically, why CNN
is more powerful than other algorithms?</p>

<h2 id="how-mapping-is-found-mathematically">How Mapping Is Found Mathematically</h2>

<p>First we address the first problem, though those two are actually the same.</p>

<p>Instead of trying to find a mapping to our problem directly, which are methods
called shallow model today, CNN tries to find a sequence of mappings to
decouple information from its most primitive form. If we rephrase the word
mapping to operation, it may sound more like plain English. Then judging from
the information extracted, it answers whatever relevant questions we are
interested, such as classification of images of different animals.</p>

<p>More mathematically, CNN tries to find the mapping by finding a representation
of the mapping on relevant bases which are found by compositing preliminary
bases. Then in the space represented by the relevant bases, divide the new
space using previous shallow model.</p>

<p>The latter part has not been investigated fully yet. I guess support vector
machine may be the key to this problem.</p>

<p>Further zooming in the process, in each layer, every kernel of CNN will
convolute with the input. In the first layer, the input is the raw signal. In
the following layer, the input is the output of last layer. A kernel here is a
basis. Convoluting with input is actually taking inner product with input.</p>

<p>In the first layer, by computing the inner product of a basis function with a
signal, we get a value. This value is one way to look at the signal, or the
information extracted which may be comprehended by human, like the existence of
a certain of kind edge. By taking inner product of all bases, we get the
signal’s representation in form of coordinates of the set of bases.</p>

<p>From now on, the task is to find combinations of the first layer’s bases that
are more informative. By using more layers, we hope to find relevant
combinations in exponential number of possible ones.</p>

<p>The most preliminary bases actually have mathematical terminology: in the
continuous case, it is called Schauder basis; in the discrete case, it is
called Hamel basis. In this case, the coordinates of the signal is the same
with its ground truth value.</p>

<p>Feature mappings from previous layer to current layer are the key to find a
more complex bases. It brings the search in the space of preliminary bases to a
more advanced space of more advanced bases. A best example is an input in form
of a three channel image will have three feature mappings, one corresponding to
one color. If we just concatenate three pixels of different color, we have no
idea it corresponds to different frequencies of light. But if we take it from
three channels, we are using the high-level information of colors. Another
example is provided in the next section.</p>

<p>Now the process up to the first part of the process is done. The second part
will not be described given that I have not understood it fully yet.</p>

<p>The followings are some notes on properties of such process:</p>

<ol>
  <li>The key of depth is reusing information hierarchically and preventing
exploration on meaningless part of function space. This is the main reason
why deep model works and will be elaborated with an example in the next
section.</li>
  <li>Coordinates are coordinates. There is only one ground truth of
nature. Coordinates are the way we see the ground truth. Different spaces
are actually different with different levels of abstraction. Different
levels of abstraction are achieved using different sets of bases.</li>
  <li>Taking inner product is the process to map point in one space to another
space. This is also the intuition behind Riesz Theorem that any linear
functional could be represented using inner product.</li>
  <li>The intuition of taking inner product is also computing correlation between
the basis and the signal. The more they are similar, the bigger the
coordinate(in CNN terminology system, the bigger the response). </li>
  <li>Bases are connected with hidden factors referred by Bengio.</li>
  <li>Spatial information is kept by the intuition of cascading local receptive
fields.</li>
</ol>

<hr />

<h2 id="analytically-why-more-depth-is-more-powerful">Analytically Why More Depth Is More Powerful</h2>

<p>Imagine a square whose components are two horizontal edges and two vertical
edges. To detect such shape, we could imagine a three layer convolutional
neural network. In the first layer of filters, we have two kernels – one
for horizontal edges and one for vertical edges. Write them in matrix form,
they are like this:</p>

<p>Horizontal Edge Kernel</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
0 & 0 & 0 \\
1 & 1 & 1 \\
0 & 0 & 0
\end{bmatrix}
 %]]&gt;</script>

<p>and Vertical Edge Kernel</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
0 & 1 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0
\end{bmatrix}
 %]]&gt;</script>

<p>The square we are going to detect is like this:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
1   & 1 & 1 & 1 &  1 \\
1   & 0 & 0 & 0 &  1 \\
1   & 0 & 0 & 0 &  1 \\
1   & 0 & 0 & 0 &  1 \\
1   & 1 & 1 & 1 &  1 
\end{bmatrix}
 %]]&gt;</script>

<p>As we moving the two kernels, we get response similar like this:</p>

<p>feature map for horizontal edge detection kernel:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
x & 1 & x \\
0 & 0 & 0 \\
x & 1 & x
\end{bmatrix}
 %]]&gt;</script>

<p>and feature map for vertical edge detection kernel:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
x & 0 & x \\
1 & 0 & 1 \\
x & 0 & x
\end{bmatrix}
 %]]&gt;</script>

<p>$x$ is the response of angle, which may not be black and white. And since they
are not very influential to our discussion, it is denoted $x$.</p>

<p>In the next convolution layer, the input are two feature maps, then the kernel
for detecting square will be like:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
x & 1 & x \\
0 & 0 & 0 \\
x & 1 & x
\end{bmatrix}
and
\begin{bmatrix}
 x & 0 & x \\
 1 & 0 & 1 \\
 x & 0 & x
\end{bmatrix}
 %]]&gt;</script>

<p>The final inner product will be normalized to one. In this case, we are only
using four kernels.</p>

<p>If we switch to one layer CNN, The kernel for detecting square will be like:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{bmatrix}
x   & 1 & 1 & 1 & x \\
1   & 0 & 0 & 0 & 1 \\
1   & 0 & 0 & 0 & 1 \\
1   & 0 & 0 & 0 & 1 \\
x   & 1 & 1 & 1 & x
\end{bmatrix}
 %]]&gt;</script>

<p>The zero and one is expanded because each edges consist of more than one
pixels. Three here is used for illustration.</p>

<p>It seems that we can detect shape only using one kernel. But wait… we are not
only trying to detect only square. We should also try to detect circle,
rectangles etc… So we will have more than more kernels. In this case, the
space of possible number of kernels will exponentially explode given the
possible changes we may swap the zero and one in the pixels of kernels.</p>

<p>But in previous three layer cases, the space of possible changes are shrunk
dramatically. The key here is explained in previous section, we are reusing the
information that only consecutive edges are sensible combination of pixels and
the number of possibilities of changing locations of edges is much smaller than
changing values of pixels.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vision Using Wavelet -- Three]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2014/12/02/vision-using-wavelet-three/"/>
    <updated>2014-12-02T17:16:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2014/12/02/vision-using-wavelet-three</id>
    <content type="html"><![CDATA[<h2 id="wavelet-in-image">Wavelet in Image</h2>

<p>The next question is how could we mathematically model the process?
How could we mathematically define “semantic”?</p>

<p>In an explicit sense, “semantic” is taught. We see an axe as axe is
because all people speaking English call it axe. It would be 斧头 in
Chinese. Semantic expressed is nothing but common agreement. Semantic
not expressed is what the object looks like, what the it could do and
so on so forth. This is the highest level of knowledge. Decomposing it
further, an axe may made up of one head and one body. The body may
just be a stick while the head may be just a special shape iron. So
one semantic is composed of more sub semantics. Down to this level,
all are still learned. How could we keep analyzing?</p>

<p>We may start with things that are simple and remember the ultimate
task is to extract semantic information from raw image.</p>

<p>Again, we return back audio signal, or one dimensional signal –
neither of them are perfectly accurate. We consider problem in
seismology. Different type of rock layers reflect to the detecting
signal in different frequency pattern in the short range of time the
signal encountering the layer. So to find rock layers that contain
oil, the task is to find patterns in the reflected signals. Such
patter represents semantic meaning that “I am the oil layer you are
looking for.” Such semantic is learned. Maybe different encountering
directions will create different signals, but they all should convey
the same semantic. In this case, semantic is gradually lowed down to
physical properties,meaning there are some deformation in the
signal. The different direction here is analogue with seeing one
object in different faces. In our vision, we do not learn different
faces of object, but by recognizing similarity in their physical
shape. To sum up, the task is to find some small patterns in a bunch
of noisy signals reflected back by the rock layer, and such signals
should be of some invariant properties. If we could find the invariant
right, based on such invariance, we can learn patterns that we can give
meaning to them.</p>

<p>Now, let’s return back to the image problem. Images are made up of
pixels. Pixels makes surfaces and edges. Surfaces and edges make
objects. So the first step is how to identify surfaces and edges in
the image. This bridges the gap between audio signal and digital
image! High frequency patters in audio signal is very representative
like the discovering oil example above. Analogue of high frequency
signal in image is edges, which represents rapid change in the gray
level of image. Consequently, low frequency signal is analogue with
surface. To identify surfaces and edges, we identify high and low
frequency components of the image. This is what wavelet does.</p>

<p>I will do more experiments to see what this leads.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vision Using Wavelet -- Two]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2014/12/02/vision-using-wavelet-two/"/>
    <updated>2014-12-02T16:45:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2014/12/02/vision-using-wavelet-two</id>
    <content type="html"><![CDATA[<h2 id="what-does-vision-system-do-in-an-abstract-sense">What does vision system do in an abstract sense?</h2>

<p>Now we have analyzed what information we could utilize and what are
their physical meaning of digital image. Let us return back to the
vision problem. How could we imitate human vision? This is the
ultimate purpose of computer vision. OK, maybe not ultimate, since we
could surpass human being but we are way far from there.</p>

<p>The first step to solve a problem, which is imitating human vision in
this case is to ask the right preliminary question. What question
should we ask? Since we are trying to imitate human vision, what
specific features or functions what we are trying imitate? I guess the
question is, a lot. But among them, the most fundamental one should be
how do human distinguishes different objects, which is the basis how
human accumulates knowledge of the world. In computer vision’s
terminology, how does human do objects recognition?</p>

<p>The whole process may be simplified as human are extracting semantic
information from the image forming on human’s retina. Though such
process seems simple for humans, it is a rather complex procedure that
takes thousands of talents on the problem for tens of years without
really understanding it. Now we have a high level description of the
task – extracting semantic information from raw digital image. So
how could we define “semantic”?</p>

<p>When we say an axe, it is actually made up of infinite amount of
sources that reflects light. In the digital image, it consists of a
region of different gray scale. So when we see the points that make
the axe, our vision system extract the axe from the background and
tells us there is an object called axe there. Such process is called
extracting semantic information from raw image.</p>

<p>This should be our vision system is constantly processing as long as
we are opening our eyes.</p>
]]></content>
  </entry>
  
</feed>
