<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: PGM | SHAWN LEE]]></title>
  <link href="http://shawnLeeZX.github.io/blog/categories/pgm/atom.xml" rel="self"/>
  <link href="http://shawnLeeZX.github.io/"/>
  <updated>2016-06-14T21:37:49+08:00</updated>
  <id>http://shawnLeeZX.github.io/</id>
  <author>
    <name><![CDATA[Shawn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Probabilistic Graphical Model Reading Note -- Chapter One]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2013/12/21/probabilistic-graphical-model-reading-note-chapter-one/"/>
    <updated>2013-12-21T11:58:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2013/12/21/probabilistic-graphical-model-reading-note-chapter-one</id>
    <content type="html"><![CDATA[<p>This is the reading notes taken while I am reading the book <em>Probabilistic
Graphical Model</em>. The start point of reading this book is I have a sense of
how the methods in supervised learning, such as least square, logistic
regression, svm, neural network, in unsupervised learning, such as clustering,
and neural network again. However, I do not know much about the role
probabilistic graphical model plays in the ML field.</p>

<p>So I try to get that solved by reading this book.</p>

<!-- more -->

<h2 id="introduction">Introduction</h2>

<h3 id="the-declarative-representation-approach">The Declarative Representation Approach</h3>

<p>One can always write a special-purpose computer program for every domain one
encounters and every type of questiion that one may wish to answer, however,
when the context of the question changes, the program’s capability will degrade
significantly.</p>

<p>The book takes an approach based on declarative representation. The key
property of a declarative representation is the separation of knowledge and
reasoning. The representation has its own clear semantics, separate from the
algorithms that one can apply to it. Thus, we can develop a general suite of
algorithms that apply any model within a broad class, whether in the domina of
medical diagonosis or speech recognition. Conversely, we can improve our model
for a specific application domain without having to modify our reasoning
algorithms constantly.</p>

<h3 id="the-inevitable-uncertainty-of-the-world">The inevitable Uncertainty of the World</h3>

<p>Uncertainty arises because of limitations in our ability to oberserve the
world, limitations in our ability to model it, and possibly even because of
innate nodeterminism.</p>

<h3 id="math-foundation-calculus-of-probability-theory">Math Foundation: Calculus of Probability Theory</h3>

<p>The calculus of probability theory provides us with a formal framework for
considering multiple possible outcomes and their likelihood. It defines a set
of mutually exclusive and exhaustive possibilities, and associates each of them
with a probability – a number between 0 and 1, so that it sums to one.</p>

<h3 id="approximation-is-inevitable">Approximation is inevitable</h3>

<p>Just the title.</p>

<h2 id="structured-probability-models">Structured Probability Models.</h2>

<p>A violent joint probability distribution is too violent and also ignores the
inner connection between different Random Viables. The author put forward a
graph representation to capture the inner connections between Random Viables.</p>

<p>Graphical representation can be viewed in two perspectives(but they are, in a
deep sense, equivalent):</p>

<ol>
  <li>the graph as a representation of a set of independencies.</li>
  <li>the graph as a skeleton for factorizing a distribution.</li>
</ol>

<p>To make the Graphicla Model relevant to real world, there components are
important:</p>

<ol>
  <li>The representation captures one kind of real-world distribution – variables
tend to interact directly only with very few others, and make it can be
written down tractably.</li>
  <li>It can be effectively inferenced.</li>
  <li>It can be effectively constructed, in both sense of taking in the knowledge
of human experts and automatic learning.</li>
</ol>

<p>This is summary given by the author:</p>

<blockquote>
  <p>These three components – representation, inference, and learning – are
critical components in constructing an intelligence system. We need a
declarative representation that is a reasonable encoding of our world model. We
need to be able to use this representation effectively to answer a broad range
of questions that are of interest. And we need to be able to acquire this
distribution, combining expert knowledge and accumulated data. Probabilistic
graphical models are one of a small handful of frameworks that support all
three capabilities for a broad range of problems.</p>
</blockquote>

<h2 id="historical-note">Historical Note</h2>

<p>This section definitely worth reading. Here I just takes some notes on why
probability graphical model was rejected at first and rose again.</p>

<p>Despite the success of PGM, the approach fell into disfavor in the AI
community, owing to combination of several factors. One was the belief,
prevalent at the time, that artificial intelligence should be based on similar
methods to human intelligence, combined with a trong impression that people do
not manipulate numbers when reasoning. A second issue was the belief that the
strong independence assumjptions made in the existing expert systmes were
fundamental to the approach. Thus, the lack of a flexible scalable mechanism to
represent interactions between variables in a distirbution was a key factor in
the rejection of the probabilistic framework.</p>

<p>The rejection of probabilistic methods was accompanies by the invention of a
range of alternative formatlisms for reasoning under uncertainty, and the
construction of expert systems based on these formalisms(notably Prospector by
Duda, Gaschnig, and Hart 1979 and Mycin by Buchanan and Shortliffe 1984). Most
of these formalisms used the production rule framework, where each rule is
augemented with some number(s) defining a measure of “confidence” in its
validity. These frameworks largely lacked formal semantics, and many exhibited
significant problems in key reasoning patterns. Other frameworks for handling
uncertainty proposed at the time included fuzzy logic, possibility theory, and
Dempster-Shafer belief functions. For a discusstion of some of these
alternative frameworks see Shafer and Pearl(1990); Horvita at al.(1988);
Halpern(2003).</p>

<p>The widespread acceptance of probabilistic methods began in the late 1980s,
driven forward by two major factors. The first was a series of seminal
theoretical developments. The most influential among these was the development
of the Bayesian network framework by Judea Peal and his colleagues in a series
of paper that culminated in Pearl’s highly influential textbook <em>Probabilistic
Reasoning in Intellligent Systems</em>. In parallel, the key paper by S.L.
Lauritzen and D.J. Spiegelhaltcr 1988 set forth the foundations for efficient
resoning using probabilistic graphical models. The second major factor was the
construction of large-scale highly successfull expert systems based on this
framework that avoided the unrealistically strong assumptions made by early
probabilistic expert systems.</p>
]]></content>
  </entry>
  
</feed>
