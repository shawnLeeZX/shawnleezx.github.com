<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: CV | SHAWN LEE]]></title>
  <link href="http://shawnLeeZX.github.io/blog/categories/cv/atom.xml" rel="self"/>
  <link href="http://shawnLeeZX.github.io/"/>
  <updated>2015-11-30T11:44:37+08:00</updated>
  <id>http://shawnLeeZX.github.io/</id>
  <author>
    <name><![CDATA[Shawn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Vision Using Wavelet -- Three]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2014/12/02/vision-using-wavelet-three/"/>
    <updated>2014-12-02T17:16:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2014/12/02/vision-using-wavelet-three</id>
    <content type="html"><![CDATA[<h2 id="wavelet-in-image">Wavelet in Image</h2>

<p>The next question is how could we mathematically model the process?
How could we mathematically define “semantic”?</p>

<p>In an explicit sense, “semantic” is taught. We see an axe as axe is
because all people speaking English call it axe. It would be 斧头 in
Chinese. Semantic expressed is nothing but common agreement. Semantic
not expressed is what the object looks like, what the it could do and
so on so forth. This is the highest level of knowledge. Decomposing it
further, an axe may made up of one head and one body. The body may
just be a stick while the head may be just a special shape iron. So
one semantic is composed of more sub semantics. Down to this level,
all are still learned. How could we keep analyzing?</p>

<p>We may start with things that are simple and remember the ultimate
task is to extract semantic information from raw image.</p>

<p>Again, we return back audio signal, or one dimensional signal –
neither of them are perfectly accurate. We consider problem in
seismology. Different type of rock layers reflect to the detecting
signal in different frequency pattern in the short range of time the
signal encountering the layer. So to find rock layers that contain
oil, the task is to find patterns in the reflected signals. Such
patter represents semantic meaning that “I am the oil layer you are
looking for.” Such semantic is learned. Maybe different encountering
directions will create different signals, but they all should convey
the same semantic. In this case, semantic is gradually lowed down to
physical properties,meaning there are some deformation in the
signal. The different direction here is analogue with seeing one
object in different faces. In our vision, we do not learn different
faces of object, but by recognizing similarity in their physical
shape. To sum up, the task is to find some small patterns in a bunch
of noisy signals reflected back by the rock layer, and such signals
should be of some invariant properties. If we could find the invariant
right, based on such invariance, we can learn patterns that we can give
meaning to them.</p>

<p>Now, let’s return back to the image problem. Images are made up of
pixels. Pixels makes surfaces and edges. Surfaces and edges make
objects. So the first step is how to identify surfaces and edges in
the image. This bridges the gap between audio signal and digital
image! High frequency patters in audio signal is very representative
like the discovering oil example above. Analogue of high frequency
signal in image is edges, which represents rapid change in the gray
level of image. Consequently, low frequency signal is analogue with
surface. To identify surfaces and edges, we identify high and low
frequency components of the image. This is what wavelet does.</p>

<p>I will do more experiments to see what this leads.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vision Using Wavelet -- Two]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2014/12/02/vision-using-wavelet-two/"/>
    <updated>2014-12-02T16:45:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2014/12/02/vision-using-wavelet-two</id>
    <content type="html"><![CDATA[<h2 id="what-does-vision-system-do-in-an-abstract-sense">What does vision system do in an abstract sense?</h2>

<p>Now we have analyzed what information we could utilize and what are
their physical meaning of digital image. Let us return back to the
vision problem. How could we imitate human vision? This is the
ultimate purpose of computer vision. OK, maybe not ultimate, since we
could surpass human being but we are way far from there.</p>

<p>The first step to solve a problem, which is imitating human vision in
this case is to ask the right preliminary question. What question
should we ask? Since we are trying to imitate human vision, what
specific features or functions what we are trying imitate? I guess the
question is, a lot. But among them, the most fundamental one should be
how do human distinguishes different objects, which is the basis how
human accumulates knowledge of the world. In computer vision’s
terminology, how does human do objects recognition?</p>

<p>The whole process may be simplified as human are extracting semantic
information from the image forming on human’s retina. Though such
process seems simple for humans, it is a rather complex procedure that
takes thousands of talents on the problem for tens of years without
really understanding it. Now we have a high level description of the
task – extracting semantic information from raw digital image. So
how could we define “semantic”?</p>

<p>When we say an axe, it is actually made up of infinite amount of
sources that reflects light. In the digital image, it consists of a
region of different gray scale. So when we see the points that make
the axe, our vision system extract the axe from the background and
tells us there is an object called axe there. Such process is called
extracting semantic information from raw image.</p>

<p>This should be our vision system is constantly processing as long as
we are opening our eyes.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vision Using Wavelet Note -- One]]></title>
    <link href="http://shawnLeeZX.github.io/blog/2014/12/02/vision-using-wavelet-one/"/>
    <updated>2014-12-02T15:57:00+08:00</updated>
    <id>http://shawnLeeZX.github.io/blog/2014/12/02/vision-using-wavelet-one</id>
    <content type="html"><![CDATA[<p>Last week, all intuition of scattering transformation begins to make
sense. To make this easier to refer to in the future, I decide to
decouple relevant contents into a separate series of blogs.</p>

<p>This post aims to understand what information we have gathered and
could be utilized and what are their physical meaning when we are
dealing with digital image.</p>

<!-- more -->

<p>Let’s first start things simple. The first thing computer vision
technique needs to perform well should be static input, which should
be digital images. But what is the nature of digital image?</p>

<p>The nature of audio data is the vibration wave in the air, which is
transmitted by the changing air pressure. Taking into account how to
gather the data, it is OK to replace “transmitted” with “generated”,
since we could regard intermediate transmission point as the source if
we have no idea what is the true origin of the wave.  Every wave
consists of three main elements, amplitude, phase and frequency.  If
it is only for musical purpose, its phase is not important, since
human are only sensitive to the frequency and amplitude of the music
listened to. But if it is language, the phase matters. The
superposition between different waves create peaks in the mixed waves
which represents words. As an evidence, if you play a music from end
to start, it is still an enjoyable music, but for a speech, it will
not be recognizable. And such superposition of different phases of
wave may not be very relevant to the frequency. The hypothesis is
based on the fact that we can understand people who say hello both in
the low tone or high pitch tone. It is the time dynamics in the waves
that are in close frequency that makes such superposition on phases.</p>

<p>What is the nature of image? The light that formulates an image are
electromagnet waves. But human does not interpret the waves
directly. The waves or maybe photons hit on the retina of human eye,
and it is the strength in different wavelength(color) and their
superposition of phases and amplitude that tells the human
information. Naturally, we may ask what kind of information is being
gathered by the advanced vision system of human?</p>

<p>First let us rule out the information that is not useful. Since we are
considering static input, one pixel in the digital image is the
accumulation of light in certain amount of time. Thus the phases of
different waves will be averaged out and make only their amplitude
matter. In another word, the time dynamics is not utilized in static
images. If it is video, it may be a different story. But I do no
consider such case for the time being. Therefore, we rule out the
phase information of different waves in the electromagnet waves
creating the digital image.</p>

<p>As for the wave lengths and amplitude, different wave length means
different colors for human beings. This could be analogous with human
being interprets high frequency vibration in the air as high pitch
tone and low frequency as low pitch tone. Another interesting analogue
may be made is that just as for music humans are only sensitive to the
frequency and amplitude, humans only take into account the information
of amplitude and frequency. Amplitude corresponds to the gray scale of
the image while frequency corresponds to the color. The information
color conveys are enormous. It is through color that we know trees
with the green leaves are different species from the one with red
leaves. However, it should be a higher level of information we human
are taking advantage of. Imagine that we put an green axe on a table
whose color in light green. It is the different of intensity of light
that makes us distinguish the axe from the table, which means the
amplitude of light we perceive. So to do task like object recognition,
amplitude is enough for extracting the information that is essential
we needed in the first stage. Color is only used for further enhancing
the ability of the method. To sum up, gray scale image is enough.</p>
]]></content>
  </entry>
  
</feed>
